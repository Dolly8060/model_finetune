[
  {
    "instruction": "介绍计算机视觉，但不要使用'特别'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "特别"
      ]
    ],
    "topic": "计算机视觉",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "计算机视觉是人工智能的重要分支，研究如何让计算机从图像和视频中获取信息、理解场景并做出决策。它结合了图像处理、机器学习、深度学习和几何建模等方法，通过对像素数据的分析，实现目标检测、图像分类、语义分割、姿态估计与跟踪等能力。\n\n随着算力提升和大规模数据的出现，基于卷积神经网络和视觉 Transformer 的模型在多种任务上取得了显著进展。计算机视觉已广泛应用于自动驾驶、医疗影像、工业质检、安防监控、零售分析和内容创作等领域。同时，该领域仍面临数据偏差、泛化能力、可解释性与隐私保护等挑战，研究者正持续探索更高效、可靠且负责任的解决方案。"
  },
  {
    "instruction": "描述迁移学习的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "极其"
      ]
    ],
    "topic": "迁移学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "迁移学习的主要特点包括：\n\n- 利用已有模型、参数或表示，将源任务中的知识应用到目标任务。  \n- 适用于目标任务数据量有限的场景，降低对大规模标注数据的依赖。  \n- 缩短模型训练周期，降低计算与开发成本。  \n- 有助于模型在新任务上的泛化能力表现。  \n- 可发生在不同任务、不同数据分布或不同领域之间。  \n- 效果依赖于源域与目标域之间的相关性。  \n- 可能出现负迁移现象，即迁移的知识对目标任务产生干扰。  \n- 常见实现方式包括特征迁移、参数迁移和模型微调。"
  },
  {
    "instruction": "请用不少于300个字描述推荐系统。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        300
      ]
    ],
    "topic": "推荐系统",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "推荐系统是一类利用数据分析与算法模型，为用户自动筛选和推荐信息、商品或服务的重要技术，已经广泛应用于电子商务、社交媒体、新闻资讯、视频平台和音乐平台等场景。其核心目标是在信息过载的环境中，帮助用户更高效地发现自己可能感兴趣的内容，同时提升平台的用户体验和商业价值。\n\n从技术角度看，推荐系统通常以用户数据和物品数据为基础，通过对用户历史行为的分析来推断其兴趣偏好。常见的数据包括浏览记录、点击行为、购买历史、评分反馈以及停留时间等。系统会对这些数据进行清洗、建模和特征提取，从而构建用户画像和物品画像，为后续推荐提供依据。\n\n在算法层面，推荐系统经历了从简单到复杂的发展过程。早期常用的方法是基于内容的推荐和协同过滤。基于内容的推荐关注物品本身的属性，向用户推荐与其过去喜欢内容相似的项目；协同过滤则利用用户之间或物品之间的相似性，通过“相似用户喜欢什么”或“相似物品被谁喜欢过”来进行推荐。随着数据规模和计算能力的提升，深度学习、强化学习和图神经网络等方法也被广泛引入，使推荐结果更加精准和多样化。\n\n在实际应用中，推荐系统还需要通过评价指标来衡量效果，例如准确率、召回率、点击率和转化率等。同时，也面临诸多挑战，如冷启动问题、数据稀疏问题、兴趣漂移以及信息茧房和算法偏见等。为此，研究者和工程师不断探索多样化推荐、可解释推荐和公平性设计，以平衡用户体验、社会责任和商业目标。\n\n总体而言，推荐系统已经成为现代信息社会的重要基础技术之一，不仅改变了人们获取信息的方式，也深刻影响着内容生产、商业模式和用户行为，其发展仍在持续演进之中。"
  },
  {
    "instruction": "写一段关于自监督学习的文字，必须包含以下关键词：训练, 特征提取, 推理。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "模型",
          4
        ]
      ]
    ],
    "topic": "自监督学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "自监督学习是一种利用数据自身结构来构建学习信号的方法，在无需大量人工标注的情况下，通过设计预任务完成模型的训练，从而获得具有泛化能力的表示；在这一过程中，模型能够自动进行有效的特征提取，为下游任务提供高质量输入；当模型部署到实际应用中时，这些通过自监督方式学到的表示可以支持高效而稳定的推理，提升整体性能与鲁棒性。"
  },
  {
    "instruction": "写一段关于大语言模型的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "非常"
      ]
    ],
    "topic": "大语言模型",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "语言模型基于文本数据进行训练，通过概率分布预测词元。模型使用参数、向量表示和计算图来处理输入与输出，在训练阶段采用损失函数和优化方法。在应用中，模型用于对话、检索、代码生成和分析任务，同时需要数据治理、评测流程和人类反馈来进行部署与维护。"
  },
  {
    "instruction": "描述自然语言处理的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "特别"
      ]
    ],
    "topic": "自然语言处理",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "自然语言处理的主要特点包括：\n\n1. **语义理解与歧义处理**：能够分析词语和句子的多重含义，并结合语境进行判断。  \n2. **上下文建模能力**：通过对上下文信息的建模，实现连贯的文本理解与生成。  \n3. **数据驱动方法**：依赖语料数据和统计学习模型进行训练与推断。  \n4. **语言多样性支持**：可处理不同语言、方言及表达风格。  \n5. **人机交互导向**：服务于对话系统、问答系统等应用场景，促进自然交流。  \n6. **跨学科融合**：结合语言学、计算机科学与人工智能相关方法。"
  },
  {
    "instruction": "在描述机器学习时，请确保'梯度下降'这个词出现至少3次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "梯度下降",
          3
        ]
      ]
    ],
    "topic": "机器学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "机器学习是一门研究如何让计算机从数据中自动学习规律并做出预测或决策的学科，其中模型训练是核心过程。在训练过程中，**梯度下降**被广泛用于最小化损失函数，从而不断改进模型参数。常见的做法是通过反向传播计算梯度，然后利用**梯度下降**逐步更新权重，使模型性能提升。无论是线性回归、逻辑回归还是深度神经网络，**梯度下降**都是基础且关键的优化方法之一，实际应用中还会根据数据规模和需求选择不同形式的**梯度下降**。"
  },
  {
    "instruction": "写一段关于深度学习的文字，字数要求在250到450之间。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        250
      ],
      [
        "max_length",
        450
      ]
    ],
    "topic": "深度学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "深度学习是人工智能领域的重要分支，它通过构建多层神经网络来模拟人脑对信息的处理方式。与传统机器学习依赖人工特征不同，深度学习能够从大量数据中自动学习特征表示，因此在语音识别、图像识别、自然语言处理等任务中取得了突破性进展。近年来，随着计算能力的提升和大数据的积累，卷积神经网络、循环神经网络以及Transformer等模型不断发展，使模型在准确率和泛化能力上显著提高。然而，深度学习也面临着数据需求高、模型可解释性不足和能耗较大的问题。未来，如何在"
  },
  {
    "instruction": "用恰好100个字概括生成对抗网络的核心要点。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "exact_length",
        100
      ]
    ],
    "topic": "生成对抗网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "生成对抗网络由生成器与判别器组成通过博弈学习分布生成器生成逼真样本判别器区分真伪双方交替优化推动模型逼近真实数据在图像合成表示学习等领域效果显著训练不稳定需平衡收敛多样性与模式崩塌风险研究持续推进发展"
  },
  {
    "instruction": "用恰好100个字概括分布式训练的核心要点。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "exact_length",
        100
      ]
    ],
    "topic": "分布式训练",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "分布式训练通过并行计算与通信协同提升效率需要数据划分模型切分梯度同步参数更新通信策略容错机制与资源调度保证收敛稳定与可扩展性同时优化带宽延迟平衡计算负载降低成本提升可靠性与可复现性支持大规模训练落地应"
  },
  {
    "instruction": "描述自然语言处理，开头必须是'随着技术发展'。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "自然语言处理",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "随着技术发展，自然语言处理（NLP）成为人工智能领域的重要分支，旨在让计算机理解、分析并生成人类语言。它结合语言学、统计学和机器学习等方法，能够实现文本分类、情感分析、机器翻译、信息抽取和对话系统等应用。通过对大量语料的学习，模型逐步掌握语言结构与语义关系，使人机交互更加自然高效，并在搜索引擎、智能客服、内容创作等场景中发挥重要作用。"
  },
  {
    "instruction": "描述自监督学习，开头必须是'众所周知'。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "自监督学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "众所周知，自监督学习是一种介于监督学习与无监督学习之间的范式，它不依赖人工标注的标签，而是从数据本身构造“伪标签”或预训练任务（如对比学习、掩码预测、时序重排等），让模型在大规模未标注数据上学习通用表征；随后，这些表征可以通过少量标注数据进行微调，从而在下游任务中取得更好的性能与泛化能力，尤其在数据标注成本高或领域变化大的场景中优势明显。"
  },
  {
    "instruction": "描述模型量化，开头必须是'在当今时代'。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "模型量化",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "在当今时代，模型量化是一种将深度学习模型中的高精度数值（如32位浮点数）转换为低精度表示（如8位整数）的技术，旨在在尽量保持模型精度的前提下，显著降低模型的存储空间、内存占用和计算开销。通过量化，模型可以更高效地在边缘设备、移动终端和嵌入式系统上运行，同时减少能耗并提升推理速度。常见的量化方法包括训练后量化和量化感知训练，它们在实现复杂度、精度损失和部署灵活性方面各有权衡，因此在实际应用中需要根据场景需求进行选择。"
  },
  {
    "instruction": "用恰好50个字概括模型量化的核心要点。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "exact_length",
        50
      ]
    ],
    "topic": "模型量化",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "模型量化通过降低数值精度压缩参数与计算加速推理节省存储功耗平衡精度效率并依赖训练策略与硬件支持能力强"
  },
  {
    "instruction": "写一段关于目标检测的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "非常"
      ]
    ],
    "topic": "目标检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "目标检测是计算机视觉中的一种任务，系统从图像或视频中定位目标并给出类别与位置。该过程接收输入数据，经过特征提取、模型推理和结果输出，产生边界框和标签。训练阶段使用标注数据进行参数更新，推理阶段对数据进行预测。应用场景包括监控、交通、工业检测和机器人。"
  },
  {
    "instruction": "介绍机器翻译，但不要使用'很'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "很"
      ]
    ],
    "topic": "机器翻译",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "机器翻译是利用计算机技术将一种语言自动转换为另一种语言的过程。它融合了语言学、统计学与人工智能，通过分析语料、学习词汇与句法关系，生成目标语言文本。早期方法以规则和统计模型为主，当前主流采用神经网络，能够在上下文理解、语义一致性与流畅度方面取得提升。机器翻译已广泛应用于跨语言交流、信息获取、国际商务与教育等场景，同时仍面临歧义处理、文化差异与专业领域适配等挑战。"
  },
  {
    "instruction": "用简洁的技术文档风格说明边缘计算。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "边缘计算",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**边缘计算（Edge Computing）技术说明**\n\n**概述**  \n边缘计算是一种将数据处理与计算能力下沉到数据产生源头（网络边缘）的计算范式，用于降低时延、节省带宽并提升实时性与可靠性。\n\n**架构**  \n- **终端设备**：传感器、摄像头、IoT 设备  \n- **边缘节点**：网关、边缘服务器，执行本地计算与缓存  \n- **云中心**：集中管理、模型训练、长期存储\n\n**关键特性**  \n- 低时延：就近处理，减少回传  \n- 带宽优化：仅上传必要数据  \n- 高可靠性：弱网/离线可运行  \n- 数据隐私：本地处理敏感数据\n\n**典型场景**  \n- 工业自动化与设备监控  \n- 智能交通与车联网  \n- 视频分析与安防  \n- 智慧城市与零售\n\n**挑战**  \n- 设备异构与运维复杂  \n- 安全与访问控制  \n- 资源受限与调度优化"
  },
  {
    "instruction": "请用JSON格式输出推荐系统的关键信息。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "推荐系统",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "{\n  \"系统名称\": \"推荐系统\",\n  \"目标\": \"根据用户行为和偏好，为用户提供个性化内容或产品推荐\",\n  \"核心组件\": [\n    \"用户画像\",\n    \"物品画像\",\n    \"特征工程\",\n    \"召回模块\",\n    \"排序模块\",\n    \"反馈与更新机制\"\n  ],\n  \"常用算法\": [\n    \"协同过滤\",\n    \"基于内容的推荐\",\n    \"矩阵分解\",\n    \"深度学习推荐模型\",\n    \"强化学习\"\n  ],\n  \"数据来源\": [\n    \"用户行为数据\",\n    \"用户属性数据\",\n    \"物品内容数据\",\n    \"上下文数据\"\n  ],\n  \"评估指标\": [\n    \"点击率（CTR）\",\n    \"转化率（CVR）\",\n    \"召回率（Recall）\",\n    \"准确率（Precision）\",\n    \"覆盖率\"\n  ],\n  \"应用场景\": [\n    \"电商平台\",\n    \"视频与音乐平台\",\n    \"新闻资讯\",\n    \"社交网络\"\n  ],\n  \"主要挑战\": [\n    \"冷启动问题\",\n    \"数据稀疏性\",\n    \"实时性要求\",\n    \"推荐多样性与公平性\"\n  ]\n}"
  },
  {
    "instruction": "用简洁的技术文档风格说明情感分析。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "情感分析",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**情感分析（Sentiment Analysis）**\n\n**概述**  \n情感分析是自然语言处理（NLP）中的一项技术，用于自动识别文本中表达的情绪极性或情感状态。\n\n**输入**  \n- 文本数据（如评论、社交媒体、客服对话）\n\n**输出**  \n- 情感极性：正面 / 负面 / 中性  \n- 情感强度（可选）：概率或评分  \n- 细粒度标签（可选）：如愤怒、喜悦、失望等\n\n**方法**  \n- 基于规则：情感词典与规则匹配  \n- 传统机器学习：TF-IDF + 分类器（SVM、LR）  \n- 深度学习：CNN/RNN/Transformer（如BERT）\n\n**评估指标**  \n- 准确率、精确率、召回率、F1\n\n**应用场景**  \n- 用户反馈分析  \n- 品牌舆情监测  \n- 客服质检与满意度评估\n\n**局限性**  \n- 讽刺与隐喻识别困难  \n- 领域迁移成本高  \n- 多语言与混合文本挑战"
  },
  {
    "instruction": "以幽默风趣的方式介绍分布式训练。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "分布式训练",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "想象一下你要教一只“超级学霸”模型学习，但书太多、题太难，你一个人根本改不过来作业。这时候你灵机一动：**叫上全班同学一起改！**——这就是**分布式训练**的核心思想。\n\n在传统训练里，模型像个单打独斗的学霸，一台机器从早学到晚，风雨无阻；而分布式训练呢，更像是**学习型“打工人联盟”**：  \n- 有的同学负责算数学（GPU/CPU 计算），  \n- 有的同学负责记笔记（参数存储），  \n- 还有的同学专门负责传话（网络通信）。\n\n大家分工协作，把一大堆数据切成小块，同时开干。学完之后再把成果一合并：“好家伙，知识点全会了，还省时间！”\n\n当然，分布式训练也有小脾气：  \n- 人多了，**沟通成本**会上来（“你算到哪一页了？”）。  \n- 步调不一致，还得有人喊口号（同步 vs 异步）。  \n\n但总体来说，它的座右铭只有一句话：  \n**“人多力量大，机器也一样。”**  \n于是，原本要训练几周的模型，可能几天甚至几小时就能“毕业”了。"
  },
  {
    "instruction": "用恰好120个字概括情感分析的核心要点。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "exact_length",
        120
      ]
    ],
    "topic": "情感分析",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "情感分析旨在识别文本中的情绪态度与倾向通过情绪词特征语境建模与机器学习方法判断极性强度来源与变化支持舆情监测用户洞察决策优化并关注领域差异数据质量与可解释性实时多模态跨语言鲁棒性评估伦理偏见与隐私保护应用场景持续迭代提升准确可靠通用价值目标"
  },
  {
    "instruction": "用简洁的技术文档风格说明语音识别。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "语音识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**语音识别（ASR）技术说明**\n\n**1. 概述**  \n语音识别（Automatic Speech Recognition, ASR）是将人类语音信号自动转换为可读文本的技术，属于语音与自然语言处理领域。\n\n**2. 工作流程**  \n1. 语音采集：通过麦克风获取音频信号。  \n2. 预处理：降噪、分帧、特征提取（如 MFCC、Log-Mel）。  \n3. 声学建模：将语音特征映射为音素或子词概率。  \n4. 语言建模：约束词序，提升识别合理性。  \n5. 解码输出：生成最终文本结果。\n\n**3. 关键技术**  \n- 深度神经网络（DNN、CNN、RNN、Transformer）  \n- 端到端模型（CTC、Attention、Transducer）  \n- 语言模型（n-gram、神经语言模型）\n\n**4. 评估指标**  \n- 词错误率（WER）  \n- 实时率（RTF）  \n- 识别准确率与延迟\n\n**5. 应用场景**  \n语音输入法、语音助手、会议转写、客服系统、智能车载系统。"
  },
  {
    "instruction": "描述注意力机制，开头必须是'众所周知'。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "注意力机制",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "众所周知，注意力机制是一种在处理信息时动态分配计算资源的方法，其核心思想是根据当前任务的重要性，对输入的不同部分赋予不同的权重。在神经网络中，注意力机制通过计算查询（Query）、键（Key）和值（Value）之间的相关性，使模型能够聚焦于与当前输出最相关的信息，从而提高表示能力和建模效果。它被广泛应用于自然语言处理、计算机视觉等领域，不仅提升了模型性能，也增强了结果的可解释性。"
  },
  {
    "instruction": "描述生成对抗网络，开头必须是'众所周知'。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "生成对抗网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "众所周知，生成对抗网络（Generative Adversarial Network，GAN）是一种由两个神经网络组成的深度学习模型：生成器和判别器。生成器的目标是从随机噪声中生成尽可能逼真的数据，而判别器则负责判断输入数据是真实样本还是生成器生成的伪样本。二者在训练过程中通过对抗博弈不断提升性能，最终使生成器能够产生以假乱真的结果。GAN在图像生成、风格迁移、数据增强等领域具有广泛应用。"
  },
  {
    "instruction": "写一段关于分布式训练的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "极其"
      ]
    ],
    "topic": "分布式训练",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "分布式训练通过在计算节点集合上运行模型来完成学习过程，节点之间通过网络交换参数和梯度，调度系统负责任务分配和资源管理，训练过程中需要处理通信、容错和一致性问题。"
  },
  {
    "instruction": "用简洁的技术文档风格说明联邦学习。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "联邦学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**联邦学习（Federated Learning）**\n\n**1. 定义**  \n联邦学习是一种分布式机器学习范式，在不集中原始数据的前提下，由多方协同训练共享模型，仅交换模型参数或梯度。\n\n**2. 架构**  \n- **客户端**：持有本地数据，执行本地训练。  \n- **服务器/协调方**：聚合各客户端模型更新，生成全局模型。  \n- **通信机制**：安全传输参数更新（可结合加密/安全聚合）。\n\n**3. 工作流程**  \n1) 服务器初始化全局模型并下发；  \n2) 客户端基于本地数据训练模型；  \n3) 客户端上传参数更新；  \n4) 服务器聚合更新（如加权平均）；  \n5) 重复迭代直至收敛。\n\n**4. 优点**  \n- 数据隐私保护  \n- 合规性强（数据不出域）  \n- 支持跨组织协作\n\n**5. 挑战**  \n- 数据异质性（Non-IID）  \n- 通信开销与稳定性  \n- 安全与对抗攻击  \n- 客户端参与不均\n\n**6. 典型应用**  \n- 移动设备个性化模型  \n- 医疗与金融跨机构建模  \n- 物联网场景"
  },
  {
    "instruction": "请用不少于300个字描述机器学习。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        300
      ]
    ],
    "topic": "机器学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "机器学习是人工智能领域中的一个重要分支，它研究如何让计算机在不被明确编程的情况下，通过数据和经验自动改进性能。传统程序通常依赖人工编写的规则，而机器学习则强调从大量数据中发现规律，并利用这些规律对未知情况进行预测或决策。随着计算能力的提升和数据规模的爆炸式增长，机器学习在近几十年中得到了迅速发展。  \n\n从方法上看，机器学习可以分为监督学习、无监督学习和强化学习等类型。监督学习通过带有标签的数据训练模型，常用于分类和回归问题；无监督学习则在没有明确标签的情况下分析数据结构，常见应用包括聚类和降维；强化学习强调智能体在环境中通过试错获得奖励，从而学习最优策略。这些方法各有特点，适用于不同的实际场景。  \n\n在应用层面，机器学习已经深入到人们的日常生活之中。例如，在搜索引擎中，它用于排序结果和理解用户意图；在医疗领域，它可以辅助疾病诊断和药物研发；在金融行业，它被广泛用于风险控制和欺诈检测。此外，语音识别、图像识别、自动驾驶等技术的发展，也都离不开机器学习的支持。  \n\n尽管机器学习带来了巨大的价值，但它也面临数据偏差、模型可解释性不足以及隐私安全等挑战。因此，如何在提升模型性能的同时，确保其可靠性、公平性和安全性，仍然是学术界和产业界需要持续探索的重要问题。"
  },
  {
    "instruction": "写一段关于语音识别的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "极其"
      ]
    ],
    "topic": "语音识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "语音识别技术通过接收语音信号，将信号转为文本，系统依靠模型和算法进行处理与输出，流程包含采集、分段、建模和解码，结果用于输入、检索和控制等场景。"
  },
  {
    "instruction": "在描述联邦学习时，请确保'特征提取'这个词出现至少3次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "特征提取",
          3
        ]
      ]
    ],
    "topic": "联邦学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "联邦学习是一种在不集中原始数据的前提下进行模型训练的分布式机器学习范式。各参与方在本地设备上完成数据预处理与特征提取，并仅将模型参数或梯度上传到服务器进行聚合，从而在保护隐私的同时提升整体模型性能。在这一过程中，本地的特征提取对于捕捉数据差异、提高模型泛化能力至关重要。\n\n在实际应用中，不同节点可能采用不同策略进行特征提取，以适配各自的数据分布和计算资源限制。通过多轮通信与聚合，联邦学习能够在全局模型中融合多源知识，而高质量的特征提取反复影响着训练效率与最终效果。因此，合理设计特征提取流程是联邦学习系统成功落地的关键因素之一。"
  },
  {
    "instruction": "用三段式结构（引言、正文、结论）介绍迁移学习。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "迁移学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**引言**  \n迁移学习是一种机器学习方法，其核心思想是将已经在某一任务或领域中学到的知识，迁移并应用到新的、相关的任务或领域中。通过复用已有经验，迁移学习可以在数据不足或训练成本较高的情况下，显著提升模型的学习效率和性能。\n\n**正文**  \n在传统机器学习中，模型通常假设训练数据与测试数据来自同一分布，而迁移学习打破了这一限制。它常见的形式包括特征迁移、模型参数迁移和关系迁移等。例如，在计算机视觉中，常使用在大规模数据集（如ImageNet）上预训练的模型作为基础，再针对具体任务进行微调。这种做法不仅减少了对大量标注数据的依赖，还能加快模型收敛速度，提高泛化能力。\n\n**结论**  \n总体而言，迁移学习通过跨任务或跨领域共享知识，有效缓解了数据稀缺和训练成本高的问题。随着深度学习的发展，迁移学习已成为自然语言处理、计算机视觉等领域的重要技术，并在实际应用中展现出广阔的前景。"
  },
  {
    "instruction": "描述异常检测的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "非常"
      ]
    ],
    "topic": "异常检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "异常检测具有以下特点：\n\n- 目标是从大量正常数据中识别不符合常规模式的样本。  \n- 异常样本数量有限，分布往往不均衡。  \n- 对数据分布假设依赖程度因方法而异，部分方法不需要先验标签。  \n- 对噪声和数据质量较为敏感，预处理过程具有重要作用。  \n- 应用场景广泛，包括安全监测、设备运维、金融风控等。  \n- 评估结果受业务背景影响，需要结合领域知识进行解释。"
  },
  {
    "instruction": "写一段关于注意力机制的文字，必须包含以下关键词：推理, 神经网络, 参数。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "反向传播",
          2
        ]
      ]
    ],
    "topic": "注意力机制",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "注意力机制是现代神经网络中的重要组成部分，它通过为不同输入分配不同权重，使模型在推理过程中能够聚焦于更关键的信息，从而提升表达能力和效果；在实现上，注意力机制通常通过可学习的参数来计算相关性分数，并在训练中不断优化，这种方式不仅增强了模型对长程依赖的建模能力，也让复杂任务中的信息选择更加高效和灵活。"
  },
  {
    "instruction": "请用列表形式列出RAG检索增强的3个要点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "列表"
      ]
    ],
    "topic": "RAG检索增强",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. **外部知识检索**：在生成回答前从向量数据库或文档库中检索相关信息，补充模型内部知识。  \n2. **降低幻觉、提升准确性**：基于检索到的真实资料进行生成，使回答更有依据、更可靠。  \n3. **知识可更新、可扩展**：无需重新训练模型，只需更新检索数据即可获得最新信息。"
  },
  {
    "instruction": "介绍RAG检索增强，但不要使用'很'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "很"
      ]
    ],
    "topic": "RAG检索增强",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "RAG（Retrieval-Augmented Generation，检索增强生成）是一种将外部检索与生成式模型结合的方法。其流程通常包括：先从向量数据库、文档库或搜索引擎中检索与问题相关的信息，再将检索结果作为上下文输入给大语言模型生成答案。通过引入最新、可控的知识来源，RAG可以降低幻觉风险，提高事实一致性，并支持私有数据的安全利用。\n\n在实践中，RAG的关键要素包括高质量切分与向量化、有效的召回与重排、上下文压缩，以及对提示词的精心设计。它适用于问答系统、企业知识助手、代码与文档检索等场景，能够在保持模型通用能力的同时，提供更贴合业务的数据支持。"
  },
  {
    "instruction": "用恰好120个字概括模型压缩的核心要点。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "exact_length",
        120
      ]
    ],
    "topic": "模型压缩",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "模型压缩旨在在保持性能前提下降低模型规模与计算开销常用方法包括剪枝量化蒸馏低秩分解与参数共享通过减少冗余提升推理速度降低存储能耗并便于部署于资源受限场景兼顾精度稳定性训练成本与硬件适配并通过自动化搜索实现最优权衡满足实际应用需求可维护安全性"
  },
  {
    "instruction": "请用不少于300个字描述联邦学习。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        300
      ]
    ],
    "topic": "联邦学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "联邦学习是一种分布式机器学习范式，其核心思想是在不集中原始数据的前提下完成模型训练。与传统将数据上传到中心服务器再进行建模的方式不同，联邦学习将模型下发到各个数据拥有方或终端设备上，在本地利用私有数据进行训练，只上传模型参数或梯度，从而在“数据不出本地”的情况下实现多方协同学习。这种方式在一定程度上缓解了数据孤岛问题，同时增强了对用户隐私和数据安全的保护。\n\n在典型的联邦学习流程中，中心服务器首先初始化一个全局模型，并将其发送给多个客户端。客户端在本地使用自身数据对模型进行若干轮训练，然后将更新后的参数通过安全通信机制上传至服务器。服务器对来自不同客户端的参数进行聚合，例如采用加权平均的方法，生成新的全局模型，再下发给各客户端，如此反复迭代，直到模型收敛。这一过程通常需要结合安全聚合、差分隐私等技术，以防止通过参数反推出原始数据。\n\n联邦学习具有多方面优势。首先，它符合数据隐私保护法规的要求，特别适用于医疗、金融、政务等对数据敏感度极高的领域。其次，它能够充分利用分散在不同机构或设备中的海量数据，提升模型的泛化能力。此外，在边缘设备上进行计算还能减少原始数据传输带来的带宽压力。然而，联邦学习也面临挑战，例如数据分布不一致、通信成本高、客户端不稳定以及潜在的安全攻击等问题。\n\n总体来看，联邦学习为隐私保护与数据价值挖掘之间提供了一种平衡方案。随着算法、系统架构以及安全技术的不断进步，联邦学习正在从理论研究逐步走向大规模实际应用，并有望成为未来人工智能和数据协作的重要基础技术之一。"
  },
  {
    "instruction": "请用列表形式列出强化学习的6个要点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "列表"
      ]
    ],
    "topic": "强化学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. **智能体与环境交互**：智能体在环境中行动，环境根据动作反馈新的状态和奖励。  \n2. **状态、动作与奖励**：学习过程围绕状态（State）、动作（Action）和奖励（Reward）展开。  \n3. **策略（Policy）**：策略定义了智能体在给定状态下选择动作的规则。  \n4. **价值函数**：用于评估状态或状态-动作对在长期回报上的好坏。  \n5. **探索与利用的权衡**：既要尝试新动作获取信息（探索），也要选择已知最优动作（利用）。  \n6. **基于试错的延迟回报学习**：通过不断试错，在可能延迟的奖励信号下优化长期收益。"
  },
  {
    "instruction": "请用JSON格式输出提示工程的关键信息。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "提示工程",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "```json\n{\n  \"定义\": \"提示工程是设计和优化输入提示以引导大语言模型产生高质量、可控输出的方法。\",\n  \"目标\": [\n    \"提高输出准确性\",\n    \"增强一致性与可控性\",\n    \"减少歧义与幻觉\",\n    \"提升任务完成效率\"\n  ],\n  \"核心原则\": [\n    \"清晰具体\",\n    \"提供上下文\",\n    \"明确约束与格式\",\n    \"逐步引导\",\n    \"可迭代优化\"\n  ],\n  \"常用技巧\": [\n    \"角色设定\",\n    \"少样本示例（Few-shot）\",\n    \"思维链提示（Chain-of-Thought）\",\n    \"分步指令\",\n    \"输出格式约束\"\n  ],\n  \"最佳实践\": [\n    \"使用明确的任务描述\",\n    \"限定输出长度与格式\",\n    \"提供正反示例\",\n    \"逐步测试与调整提示\",\n    \"结合系统与用户指令分层设计\"\n  ],\n  \"常见问题\": [\n    \"提示过于模糊\",\n    \"约束冲突\",\n    \"信息过载\",\n    \"忽视模型局限\"\n  ],\n  \"应用场景\": [\n    \"文本生成\",\n    \"代码编写\",\n    \"数据分析\",\n    \"问答系统\",\n    \"自动化工作流\"\n  ]\n}\n```"
  },
  {
    "instruction": "用通俗易懂的语言向初学者解释分布式训练。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "分布式训练",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "分布式训练，简单说就是**让多台电脑一起训练一个模型**，而不是只用一台电脑。可以把它想象成：一个人搬家很慢，叫上很多朋友一起搬，每个人负责一部分，事情就快多了。训练大模型时，数据多、计算量大，一台机器又慢又容易“累坏”，所以就需要大家一起干活。\n\n它通常有两种常见方式：  \n- **数据并行**：把训练数据分成几份，每台机器都拿同一个模型，各自用不同的数据算一遍，然后把结果汇总更新模型。就像同一道作业，大家各做几页，最后合成一份答案。  \n- **模型并行**：当模型太大，一台机器放不下时，就把模型拆开，不同机器负责不同部分，像流水线一样合作完成计算。\n\n为什么要用分布式训练？主要是**更快**（训练时间大幅缩短）和**能训练更大的模型**。当然，它也更复杂，需要机器之间频繁“沟通”和同步，但好在现在的深度学习框架（如 PyTorch、TensorFlow）已经帮我们把这些复杂细节封装好了，初学者只要理解这个“多人协作训练”的概念就够了。"
  },
  {
    "instruction": "写一段关于文本分类的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "特别"
      ]
    ],
    "topic": "文本分类",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "文本分类是语言处理中的一项任务，目标是根据文本内容分配标签。系统通过数据收集、分词、特征提取、模型训练和预测等步骤完成流程。方法包括规则方法和学习方法，模型接收输入文本并输出类别。结果用于信息检索、内容管理和用户服务。"
  },
  {
    "instruction": "描述模型量化的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "特别"
      ]
    ],
    "topic": "模型量化",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "模型量化的特点包括以下方面：\n\n- 通过将浮点数表示转换为低位宽整数表示，减少模型参数的存储需求。  \n- 在保持模型结构不变的情况下，降低计算过程中的数值精度。  \n- 有助于降低内存访问开销，提升推理阶段的计算效率。  \n- 量化过程可能引入数值误差，对模型精度产生影响。  \n- 可与多种硬件平台适配，便于在资源受限的设备上部署。  \n- 支持多种实现方式，如静态量化、动态量化和量化感知训练。"
  },
  {
    "instruction": "用三段式结构（引言、正文、结论）介绍模型量化。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "模型量化",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**引言**  \n模型量化是一种常见的模型压缩与加速技术，主要用于将高精度的数值表示（如32位浮点数）转换为低精度表示（如8位整数）。随着深度学习模型规模不断增大，模型量化在降低存储开销、减少计算量以及提升推理效率方面发挥着越来越重要的作用，尤其适用于边缘设备和移动端场景。\n\n**正文**  \n模型量化的核心思想是在尽量保持模型精度的前提下，降低参数和计算的数值精度。常见的量化方式包括权重量化、激活量化以及权重与激活同时量化；按照实施阶段，又可分为训练后量化和量化感知训练。训练后量化实现简单、成本低，但精度可能略有下降；量化感知训练则在训练过程中模拟低精度计算，通常能获得更好的精度表现。通过量化，模型不仅体积显著减小，还能更好地利用硬件对整数运算的加速能力。\n\n**结论**  \n总体而言，模型量化是在性能与效率之间取得平衡的重要手段。它为深度学习模型在资源受限环境中的部署提供了可行路径，是模型工程化和实际落地中的关键技术之一。随着硬件与算法的不断发展，模型量化方法也将更加成熟和多样化。"
  },
  {
    "instruction": "写一段关于文本分类的文字，字数要求在150到250之间。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        150
      ],
      [
        "max_length",
        250
      ]
    ],
    "topic": "文本分类",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "文本分类是自然语言处理中的基础任务之一，旨在根据文本内容将其自动归入预定义的类别。它广泛应用于新闻主题识别、情感分析、垃圾邮件过滤和舆情监测等场景。随着机器学习和深度学习的发展，文本分类方法从早期的规则和特征工程，逐步演进到基于词向量、预训练语言模型的端到端学习，大幅提升了准确率与泛化能力，同时也降低了人工成本。"
  },
  {
    "instruction": "写一段关于模型量化的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "极其"
      ]
    ],
    "topic": "模型量化",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "模型量化是将模型中的参数和计算从浮点表示转换为整数表示的过程。该过程在训练后或训练中进行，通过设定尺度和零点来完成映射。量化可以减少存储需求，降低计算量，缩短推理时间，并支持在边缘设备上部署。量化过程需要评估精度变化，并结合校准数据或感知训练来控制误差。"
  },
  {
    "instruction": "请用不少于100个字描述自监督学习。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        100
      ]
    ],
    "topic": "自监督学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "自监督学习是一种介于监督学习与无监督学习之间的机器学习范式。它不依赖人工标注数据，而是从原始数据中自动构造监督信号，例如通过预测被遮挡的部分、恢复被打乱的顺序或对比不同视角的一致性。模型在预训练阶段学习通用表示，再迁移到下游任务中微调，从而显著降低标注成本，提高在小样本场景下的性能，并在视觉、语音和自然语言处理领域取得了广泛成功。"
  },
  {
    "instruction": "以幽默风趣的方式介绍联邦学习。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "联邦学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "想象一下，联邦学习就像一场**“不交作业也能集体进步”的学霸联盟**。\n\n传统机器学习很霸道：“把你的数据都交出来，我来学。”  \n联邦学习却很绅士：“数据你留着，我只要你学到的经验。”  \n于是，大家各自在家偷偷学习（比如手机、医院、银行），只把“学习心得”（模型参数）发给班长。班长负责汇总，顺便说一句：“很好，大家都变聪明了，但我完全不知道你们昨晚刷了什么。”\n\n再打个比方：  \n联邦学习就像**共享菜谱，不共享冰箱**。  \n你可以告诉我“盐放多少、火候几分钟”，但冰箱里那块神秘的腊肉？不好意思，隐私。  \n最终，大家一起把菜谱优化得越来越好，却谁也没翻谁家冰箱。\n\n一句话总结：  \n**联邦学习 = 数据不出门，智慧一起卷。**  \n既保护隐私，又能训练模型，堪称AI界的“文明上网模范生”。"
  },
  {
    "instruction": "写一段关于边缘计算的文字，必须包含以下关键词：损失函数, 特征提取, 神经网络。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "参数",
          2
        ]
      ]
    ],
    "topic": "边缘计算",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "边缘计算通过将数据处理和分析能力下沉到靠近数据源的设备上，显著降低了时延与带宽消耗，在智能感知与实时决策场景中尤为关键。在这一过程中，设备侧常借助神经网络对本地数据进行特征提取，从而减少原始数据上传云端的需求；模型训练或微调时，则通过定义合适的损失函数来衡量预测结果与真实标签之间的差异，以指导参数更新并提升整体性能。"
  },
  {
    "instruction": "写一段关于代码生成的文字，字数要求在200到300之间。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        200
      ],
      [
        "max_length",
        300
      ]
    ],
    "topic": "代码生成",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "代码"
  },
  {
    "instruction": "用恰好100个字概括边缘计算的核心要点。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "exact_length",
        100
      ]
    ],
    "topic": "边缘计算",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "边缘计算将数据处理靠近数据源降低时延节省带宽提升隐私与可靠性通过分布式架构在终端或网关实时分析支持物联网人工智能应用实现高效低成本可扩展的计算服务满足安全管理运维协同跨域部署需求支持弹性调度监控审计化"
  },
  {
    "instruction": "描述注意力机制的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "极其"
      ]
    ],
    "topic": "注意力机制",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "注意力机制具有以下特点：  \n- 通过为不同输入分配权重，引导模型关注与当前任务相关的信息。  \n- 权重由查询、键和值共同计算，能够随输入变化而调整。  \n- 将分散的信息加权汇聚，形成上下文相关的表示。  \n- 权重分布可被观察，用于分析模型在处理信息时的关注位置。  \n- 支持建模序列中相距较远的依赖关系。  \n- 在自注意力结构中，多个位置可以并行计算，适合大规模训练。"
  },
  {
    "instruction": "写一段关于联邦学习的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "很"
      ]
    ],
    "topic": "联邦学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "联邦学习是一种机器学习方法。参与方在设备上训练模型，并通过通信交换参数。服务器汇总参数并更新模型，数据不离开设备。过程用于数据利用和模型训练。"
  },
  {
    "instruction": "用正式的学术语言描述分布式训练。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "分布式训练",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "分布式训练是指在多个计算节点（如多台服务器或多张加速器）上协同完成机器学习模型训练的计算范式，其核心目标是在保证模型收敛性与数值一致性的前提下，提高训练效率与可扩展性。该方法通过将计算与数据负载在节点间进行划分与并行化执行，突破单一计算资源在存储容量、计算能力和时间成本方面的限制。\n\n从实现机制上看，分布式训练通常包括数据并行、模型并行和流水线并行等基本形式，并依赖高效的通信与同步策略（如参数服务器架构或基于 All-Reduce 的去中心化通信）来协调各节点间的梯度或参数更新。在实际系统中，还需综合考虑通信开销、负载均衡、一致性模型以及容错机制等关键问题，以在大规模计算环境下实现稳定、可扩展且高效的模型训练过程。"
  },
  {
    "instruction": "用恰好50个字概括分布式训练的核心要点。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "exact_length",
        50
      ]
    ],
    "topic": "分布式训练",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "分布式训练通过数据并行模型并行提升效率依赖通信同步容错与资源调度实现可扩展稳定高效训练管理成本与性能"
  },
  {
    "instruction": "请写5段关于注意力机制的内容，每段以数字编号开头。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "注意力机制",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. 注意力机制最早源于人类认知过程中的选择性注意思想，在机器学习中用于让模型聚焦于与当前任务最相关的信息，从而提升表达能力和预测效果。\n\n2. 在神经网络中，注意力机制通过为不同输入分配不同权重，使模型能够动态地判断哪些特征或位置更重要，而不是对所有信息一视同仁地处理。\n\n3. 自注意力机制是注意力机制的重要形式，它在同一序列内部建模元素之间的依赖关系，典型代表是 Transformer 架构，在自然语言处理领域取得了突破性成果。\n\n4. 注意力机制已被广泛应用于机器翻译、文本摘要、语音识别和计算机视觉等任务中，能够显著提升模型对长距离依赖和复杂结构的建模能力。\n\n5. 尽管注意力机制带来了性能提升，但也存在计算复杂度较高等挑战，因此研究者不断提出稀疏注意力、线性注意力等改进方法，以提高效率和可扩展性。"
  },
  {
    "instruction": "用三段式结构（引言、正文、结论）介绍边缘计算。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "边缘计算",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**引言**  \n随着物联网、人工智能和5G等技术的快速发展，海量数据在网络边缘不断产生，传统依赖云计算的集中式处理模式逐渐暴露出时延高、带宽压力大等问题。在这样的背景下，边缘计算作为一种新的计算范式应运而生，受到广泛关注。\n\n**正文**  \n边缘计算是指将数据的计算、存储和分析能力从远端云数据中心下沉到靠近数据源的网络边缘节点，如传感器、网关或本地服务器。通过在数据产生的附近进行处理，边缘计算能够显著降低网络时延，减少数据传输量，提高系统的实时性和可靠性。这一技术在智能制造、自动驾驶、智慧城市和工业互联网等场景中具有重要应用价值。\n\n**结论**  \n总体来看，边缘计算是对云计算的重要补充，而非替代。它通过“就近计算”的方式解决了实时性和效率问题，为新一代智能应用提供了关键支撑。随着相关技术和标准的不断成熟，边缘计算将在未来的信息技术体系中发挥越来越重要的作用。"
  },
  {
    "instruction": "写一篇关于迁移学习的短文，必须以问句结尾。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "迁移学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "迁移学习是一种将已有任务中学到的知识迁移到新任务上的机器学习方法，旨在减少对大量标注数据的依赖并提升模型训练效率。通过复用预训练模型的特征表示，迁移学习在计算机视觉、自然语言处理等领域取得了显著成效，同时也面临领域差异和负迁移等挑战。在数据稀缺或计算资源受限的现实场景中，迁移学习是否会成为推动智能应用快速落地的关键路径？"
  },
  {
    "instruction": "以问答形式介绍Transformer架构，包含至少4个问题和回答。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "Transformer架构",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**问答形式介绍 Transformer 架构**\n\n**Q1：什么是 Transformer 架构？**  \nA1：Transformer 是一种用于序列建模的深度学习架构，最早由 Vaswani 等人在 2017 年提出。它主要用于自然语言处理任务，如机器翻译、文本生成等，其核心特点是完全基于注意力机制，而不依赖循环神经网络或卷积神经网络。\n\n**Q2：Transformer 的核心思想是什么？**  \nA2：Transformer 的核心思想是“自注意力机制（Self-Attention）”，它允许模型在处理序列时，直接关注序列中任意位置的信息，从而高效地捕捉长距离依赖关系。\n\n**Q3：Transformer 的基本结构由哪些部分组成？**  \nA3：Transformer 主要由编码器（Encoder）和解码器（Decoder）两部分组成。每个编码器和解码器都由多层堆叠而成，每一层通常包括多头自注意力机制、前馈神经网络以及残差连接和层归一化。\n\n**Q4：什么是多头注意力（Multi-Head Attention）？**  \nA4：多头注意力是将注意力机制并行地运行多次，每个“头”关注输入的不同子空间。这样可以让模型从多个角度理解序列信息，增强表达能力。\n\n**Q5：为什么 Transformer 不需要循环结构？**  \nA5：因为自注意力机制可以一次性处理整个序列，并通过位置编码（Positional Encoding）引入顺序信息，从而既能并行计算，又能保留序列顺序特征，这使得循环结构不再必要。"
  },
  {
    "instruction": "请用不少于300个字描述代码生成。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        300
      ]
    ],
    "topic": "代码生成",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "代码生成是指利用规则、模板或智能模型，根据需求描述自动产出可执行代码的过程。传统的代码生成多依赖于固定模板、领域特定语言以及配置文件，通过将业务规则结构化，再映射为目标语言的代码，从而减少重复劳动并提升一致性。随着软件工程复杂度提升，自动化代码生成逐渐成为提升开发效率和质量的重要手段，尤其在接口定义、数据模型、基础框架和测试用例等方面表现突出。开发者可以将精力从繁琐的样板代码中解放出来，更多关注系统设计和业务逻辑。\n\n近年来，基于人工智能的代码生成取得了显著进展。通过对海量代码与自然语言的学习，模型能够理解需求描述、上下文约束以及常见编程范式，并生成结构合理、语法正确的代码片段，甚至完成跨文件、跨模块的实现。这类代码生成不仅支持多种编程语言和框架，还能结合风格规范、性能要求与安全约束进行优化。同时，代码生成也面临挑战，例如需求歧义、上下文不完整、潜在缺陷以及可维护性问题。因此，在实际应用中，代码生成通常与人工审查、测试和持续集成流程相结合，形成“人机协作”的开发模式，从而在效率、质量与可靠性之间取得平衡。"
  },
  {
    "instruction": "用Markdown格式撰写关于联邦学习的说明，必须包含标题和分点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "联邦学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "```markdown\n# 联邦学习（Federated Learning）简介\n\n- **定义**：联邦学习是一种分布式机器学习方法，允许多个参与方在不共享原始数据的情况下协同训练模型。\n- **核心思想**：数据留在本地，模型参数或梯度在各方之间进行安全聚合。\n- **工作流程**：\n  - 各客户端在本地使用私有数据训练模型\n  - 将更新后的模型参数上传至服务器\n  - 服务器进行聚合并下发全局模型\n- **主要优势**：\n  - 保护数据隐私与安全\n  - 降低数据集中带来的合规风险\n  - 充分利用分散数据资源\n- **典型应用场景**：\n  - 移动设备上的个性化推荐\n  - 医疗机构之间的联合建模\n  - 金融风控与反欺诈\n- **挑战与限制**：\n  - 通信成本较高\n  - 数据分布不一致（非IID问题）\n  - 安全与鲁棒性需求高\n```"
  },
  {
    "instruction": "写一篇关于联邦学习的短文，必须以问句结尾。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "联邦学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "联邦学习是一种分布式机器学习范式，它通过在本地设备上训练模型并仅共享参数更新，而不是原始数据，从而有效保护用户隐私。该方法可以在不集中数据的情况下实现多方协作，减少数据泄露风险，并满足日益严格的数据合规要求。联邦学习已在移动输入法、医疗数据分析和金融风控等场景中展现出潜力，但仍面临通信成本、数据异质性和安全攻击等挑战。随着算法和系统不断进步，联邦学习能否在隐私保护与模型性能之间取得更理想的平衡并实现大规模落地呢？"
  },
  {
    "instruction": "写一段关于知识蒸馏的文字，字数要求在150到300之间。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        150
      ],
      [
        "max_length",
        300
      ]
    ],
    "topic": "知识蒸馏",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "知识蒸馏是一种模型压缩与迁移学习技术，通过让小模型学习大模型的行为来提升性能。教师模型通常具有更高的容量和表达能力，学生模型则通过模仿其软标签、特征表示或中间层信息，在保持较低计算成本的同时获得更好的泛化能力。该方法广泛应用于模型部署、边缘计算和实时推理场景。同时，蒸馏过程中常结合温度参数与损失函数设计，以平衡学习稳定性与效果。"
  },
  {
    "instruction": "描述语音识别的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "特别"
      ]
    ],
    "topic": "语音识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "语音识别的特点包括：\n\n- 将人类语音信号转换为可处理的文本或指令  \n- 基于声学模型与语言模型进行分析与匹配  \n- 对发音方式、口音差异、环境噪声存在影响  \n- 支持实时处理与离线处理两种模式  \n- 需要语音数据进行训练与持续优化  \n- 可适配多种语言与方言  \n- 广泛应用于输入法、智能助手、客服系统、字幕生成等场景"
  },
  {
    "instruction": "描述Transformer架构的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "非常"
      ]
    ],
    "topic": "Transformer架构",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "Transformer 架构的主要特点包括：\n\n- 采用自注意力机制，在序列内部建立任意位置之间的依赖关系。  \n- 不依赖循环或卷积结构，通过并行计算处理序列数据。  \n- 使用多头注意力，在不同表示子空间中建模信息。  \n- 引入位置编码，为模型提供序列顺序信息。  \n- 编码器与解码器由多层相同结构堆叠而成，结构清晰。  \n- 在各子层之间加入残差连接与层归一化，支持稳定训练。"
  },
  {
    "instruction": "写一段关于生成对抗网络的文字，必须包含以下关键词：梯度下降, 损失函数, 优化。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "优化",
          2
        ]
      ]
    ],
    "topic": "生成对抗网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "生成对抗网络（GAN）由生成器和判别器组成，两者在对抗训练中相互博弈：生成器试图生成以假乱真的样本，判别器则努力区分真假数据，训练过程中通过定义合适的损失函数来度量双方的表现，并利用梯度下降方法不断更新网络参数，从而实现对模型性能的持续优化。"
  },
  {
    "instruction": "请用不少于100个字描述大语言模型。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        100
      ]
    ],
    "topic": "大语言模型",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "大语言模型是一类基于深度学习的人工智能模型，通常采用大规模神经网络和海量语料进行训练。它能够理解和生成自然语言，在对话、写作、翻译、代码生成、信息检索等方面表现出较强能力。通过学习上下文关系和语义规律，大语言模型可以进行推理、总结和改写，但也可能受到数据偏差和幻觉问题的影响，因此需要合理使用与持续改进。"
  },
  {
    "instruction": "写一段关于联邦学习的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "非常"
      ]
    ],
    "topic": "联邦学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "联邦学习是一种机器学习方法，数据留在节点，模型在节点之间传递参数，参与方通过通信完成训练与更新，系统减少数据传输并支持协作，过程涉及协调、聚合与反馈，应用包括医疗、金融和设备。"
  },
  {
    "instruction": "写一段关于代码生成的文字，字数要求在200到350之间。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        200
      ],
      [
        "max_length",
        350
      ]
    ],
    "topic": "代码生成",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "代码生成是软件工程中的重要技术，它通过分析需求、模型或示例，自动产出可执行的程序代码，从而提升开发效率与一致性。借助模板、规则引擎以及近年来流行的人工智能模型，代码生成能够减少重复劳动，降低人为错误，并帮助开发者更专注于架构设计与业务逻辑。同时，合理的代码生成还需要良好的规范与审查机制，以避免生成代码质量不稳定或"
  },
  {
    "instruction": "用通俗易懂的语言向初学者解释计算机视觉。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "计算机视觉",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "计算机视觉，简单来说，就是**让计算机“看懂”图片和视频**的技术。\n\n人看到一张照片，能马上分辨出这是人、车还是动物；而计算机本来只认识数字。计算机视觉做的事情，就是把图片或视频变成一堆数字，再教计算机如何从这些数字中找规律，从而判断“这是什么”“它在哪里”“它在做什么”。\n\n举个生活中的例子：  \n- **人脸识别**：手机能从照片里找到你的脸并解锁，这是计算机视觉在“找脸、认脸”。  \n- **自动驾驶**：汽车通过摄像头识别道路、行人和红绿灯，这是计算机视觉在“看路”。  \n- **拍照识物**：拍一朵花，软件告诉你是什么品种，这是计算机视觉在“认东西”。\n\n计算机是怎么学会这些的呢？  \n通常要给它**大量图片和答案**（比如：这张图是猫，那张是狗），让它反复学习，总结出“猫和狗在形状、颜色上的区别”。这个学习过程常常会用到**机器学习和深度学习**。\n\n总结一句话：  \n**计算机视觉就是教计算机像人一样，用眼睛（摄像头）去理解世界。**"
  },
  {
    "instruction": "描述命名实体识别的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "很"
      ]
    ],
    "topic": "命名实体识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "命名实体识别（NER）的特点包括：\n\n- 任务目标是从文本中识别人名、地名、组织机构名、时间、数量等实体，并标注其类别。  \n- 处理对象以自然语言文本为主，常用于非结构化或半结构化数据。  \n- 依赖上下文信息判断词语在特定语境中的实体属性，同一词语在不同句子中可能对应不同类别。  \n- 结果形式通常是序列标注，将实体边界与类别信息同时给出。  \n- 可作为信息抽取、问答系统、知识图谱构建等应用的基础环节。  \n- 方法上既可以基于规则与词典，也可以采用统计模型和深度学习模型。"
  },
  {
    "instruction": "写一段关于迁移学习的文字，必须包含以下关键词：深度学习, 梯度下降, 人工智能。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "神经网络",
          3
        ]
      ]
    ],
    "topic": "迁移学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "迁移学习是一种重要的方法，它通过将已有任务中学到的知识迁移到新任务上，减少数据需求并提升模型性能，在深度学习领域尤为常见；通过在预训练模型的基础上继续使用梯度下降进行微调，模型能够更快地收敛并适应新场景，从而推动人工智能在计算机视觉、自然语言处理等应用中的高效落地。"
  },
  {
    "instruction": "用恰好100个字概括情感分析的核心要点。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "exact_length",
        100
      ]
    ],
    "topic": "情感分析",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "情感分析通过识别文本中主观态度情绪极性与强度挖掘观点来源对象与倾向结合语境语义与特征模型实现自动分类评估舆情辅助决策改进服务体验提升理解能力促进反馈监测风险预警洞察趋势价值支持多域应用持续优化实践发展"
  },
  {
    "instruction": "写一段关于提示工程的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "特别"
      ]
    ],
    "topic": "提示工程",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "提示工程是人与模型之间的交流方法。它涉及提示的设计、输入、测试和修改。使用者根据任务目标编写提示，并观察模型输出。随后根据结果调整提示内容或结构，以引导模型执行任务。该过程服务于问答、写作、编程和分析等场景。"
  },
  {
    "instruction": "用通俗易懂的语言向初学者解释代码生成。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "代码生成",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**代码生成**，简单来说，就是**让电脑帮你写代码**。\n\n你可以把它想成做饭：  \n- 你告诉厨师想吃什么（需求或规则），  \n- 厨师按照菜谱（模板或规则），  \n- 很快做出一盘菜（代码）。  \n\n### 它是怎么工作的？\n1. **先给规则或描述**  \n   比如：“我要一个用户登录功能”，或者提供一份配置文件、表结构。\n2. **工具自动处理**  \n   代码生成工具会根据这些信息，按照 заранее写好的规则，把代码一行行拼出来。\n3. **得到可用的代码**  \n   生成的代码可以直接用，或者稍微改一改再用。\n\n### 为什么要用代码生成？\n- **省时间**：不用每次从零写重复的代码  \n- **少出错**：机器按规则来，不容易写错  \n- **更专注核心逻辑**：把精力放在真正重要的地方\n\n### 常见的例子\n- 编译器把高级语言（如 C、Java）生成机器能懂的代码  \n- 用模板生成网页代码（HTML、CSS）  \n- AI 根据你的描述自动生成程序代码  \n\n一句话总结：  \n👉 **代码生成就是“用规则和描述，让电脑自动写代码”**，对初学者来说，它能减少重复劳动，让学习和开发更轻松。"
  },
  {
    "instruction": "用正式的学术语言描述模型量化。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "模型量化",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "模型量化（model quantization）是指在保证模型性能可接受的前提下，将原本以高精度数值表示的模型参数、激活值及中间计算结果，映射为低精度数值表示的一类模型压缩与加速技术。其核心目标在于降低模型的存储需求、计算复杂度与能耗开销，从而提升模型在资源受限环境（如嵌入式设备、移动终端与边缘计算平台）中的部署效率与推理速度。模型量化通常通过构建连续实值空间到离散数值集合的映射函数，实现浮点表示（如 FP32、FP16）向低比特整数表示（如 INT8、INT4）的转换。\n\n从方法论角度看，模型量化可分为训练后量化（post-training quantization）与量化感知训练（quantization-aware training）。前者在模型训练完成后直接对参数和激活进行量化，具有实现简便、成本较低的特点；后者则在训练过程中显式引入量化算子与量化误差建模，使模型在训练阶段适应低精度表示，从而在精度保持方面通常具有更优表现。此外，根据量化策略的不同，模型量化还可进一步区分为均匀量化与非均匀量化、对称量化与非对称量化等。总体而言，模型量化在模型精度、计算效率与系统资源之间提供了一种重要的权衡机制，是深度学习模型工程化与规模化应用中的关键技术之一。"
  },
  {
    "instruction": "在描述提示工程时，请确保'深度学习'这个词出现至少3次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "深度学习",
          3
        ]
      ]
    ],
    "topic": "提示工程",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "提示工程是为大模型设计高质量指令与上下文的实践，目标是稳定、可控地获得期望输出。它建立在深度学习模型的能力边界之上，通过明确角色、任务、约束和示例，减少歧义并提升一致性。在深度学习的发展背景下，提示工程利用模型的表示学习与推理特性，弥补训练数据与具体任务之间的差距。同时，理解深度学习的局限性，如幻觉与偏差，有助于通过更精确的提示来规避风险。总体而言，提示工程不是替代深度学习训练，而是与深度学习协同工作的实用方法。"
  },
  {
    "instruction": "写一段关于深度学习的文字，必须包含以下关键词：特征提取, 模型, 推理。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "推理",
          2
        ]
      ]
    ],
    "topic": "深度学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "深度学习是一种基于多层神经网络的方法，通过自动的特征提取从海量数据中学习复杂模式，研究者会设计和训练合适的模型来拟合数据分布，在实际应用中再利用训练好的网络进行高效而准确的推理，从而支持图像识别、语音理解和自然语言处理等任务。"
  },
  {
    "instruction": "写一段关于计算机视觉的文字，必须包含以下关键词：优化, 损失函数, 训练。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "梯度下降",
          3
        ]
      ]
    ],
    "topic": "计算机视觉",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "计算机视觉旨在让机器理解和分析图像与视频内容，在实际应用中通常通过深度学习模型来完成目标检测、图像分类等任务，而模型性能的提升依赖于合理的训练过程，其中需要设计合适的损失函数来衡量预测结果与真实标签之间的差距，并通过不断的参数优化来最小化误差，从而使模型逐步学习到更具判别力的特征表示。"
  },
  {
    "instruction": "请写4段关于异常检测的内容，每段以数字编号开头。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "异常检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. 异常检测是指在数据中识别出与大多数正常模式显著不同的样本或行为的过程，其核心目标是发现潜在的异常、错误或风险事件。它广泛应用于数据分析、监控系统和智能决策中，是保障系统稳定性和安全性的重要手段。\n\n2. 常见的异常检测方法包括基于统计的方法、基于距离或密度的方法以及基于机器学习的方法。统计方法依赖数据分布假设，距离和密度方法关注样本间的相似性，而机器学习方法（如孤立森林、自编码器）能够处理更复杂和高维的数据。\n\n3. 在实际应用中，异常检测被广泛用于金融欺诈检测、网络入侵检测、工业设备故障预测以及医疗健康监测等领域。通过及时发现异常行为，可以降低损失、提升效率，并为后续决策提供重要依据。\n\n4. 异常检测面临的主要挑战包括异常样本稀缺、数据噪声较大以及异常定义随场景变化等问题。为此，研究者不断探索自适应、无监督或半监督的方法，以提高模型在真实复杂环境中的鲁棒性和泛化能力。"
  },
  {
    "instruction": "请用不少于100个字描述大语言模型。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        100
      ]
    ],
    "topic": "大语言模型",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "大语言模型是一类以海量文本数据为基础训练的人工智能模型，通常采用深度学习中的变换器架构。它通过学习词与词、句与句之间的统计关系，能够理解和生成自然语言。模型具备问答、写作、翻译、总结、编程辅助等能力，并能在不同任务中迁移应用。通过持续训练与对齐技术，大语言模型在准确性、安全性和可控性方面不断提升，但仍需要人类监督与合理使用。"
  },
  {
    "instruction": "以幽默风趣的方式介绍自监督学习。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "自监督学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "想象一下自监督学习是一位“自学成才”的学霸：老师没来、作业没发、答案没给，但他自己给自己出题、自己批改，还越学越聪明。\n\n传统的监督学习像是家长式教育——“这是猫，这是狗，记住！”；而自监督学习更像是把孩子丢进房间：“来，把这本被撕成拼图的书自己拼回去。”于是模型开始琢磨：前后句怎么接？被遮住的词应该是谁？图片少了一块大概长啥样？在不断“猜错—改正—再猜”的过程中，它居然悟出了语言、图像里的规律。\n\n最妙的是，自监督学习不挑食：海量的无标签数据来者不拒，一边吃一边长脑子。等哪天真老师（少量标注数据）来了，它已经是个“见过世面”的老江湖，学新任务快得飞起。\n\n一句话总结：自监督学习就是——**没有标准答案，也能把自己卷成高手的学习方式**。"
  },
  {
    "instruction": "请写4段关于文本分类的内容，每段以数字编号开头。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "文本分类",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. 文本分类是自然语言处理中的一项基础任务，旨在根据文本内容将其自动归入预定义的类别，例如新闻主题、情感极性或垃圾邮件识别。它通过对文本语义和结构的理解，帮助计算机高效处理海量文本数据。\n\n2. 常见的文本分类方法包括基于规则的方法、传统机器学习方法以及深度学习方法。传统方法如朴素贝叶斯、支持向量机依赖人工特征工程，而深度学习方法如CNN、RNN和Transformer能够自动学习文本特征。\n\n3. 在文本分类过程中，特征表示至关重要，从早期的词袋模型、TF-IDF，到后来的词向量和上下文嵌入，如Word2Vec、BERT，这些技术不断提升模型对语义信息的捕捉能力。\n\n4. 文本分类被广泛应用于搜索引擎、舆情分析、推荐系统和客服机器人等场景，其效果通常通过准确率、召回率和F1值进行评估，持续的数据优化和模型迭代是提升性能的关键。"
  },
  {
    "instruction": "介绍RAG检索增强，但不要使用'极其'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "极其"
      ]
    ],
    "topic": "RAG检索增强",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "RAG（Retrieval-Augmented Generation，检索增强生成）是一种将信息检索与大模型生成能力相结合的技术框架。它的核心思想是在模型生成回答之前，先从外部知识库、文档或数据库中检索与问题相关的内容，再将这些内容作为上下文提供给大模型，从而生成更准确、更有依据的回答。\n\n在传统的大模型应用中，模型主要依赖训练时学到的知识，面对最新信息或专业领域资料时容易出现不准确甚至编造的问题。RAG通过“先检索、再生成”的流程，有效弥补了这一不足，使模型能够利用实时数据、企业内部文档或特定领域知识，提升回答的可靠性与可解释性。\n\n因此，RAG常被应用于智能问答、企业知识库、客服系统和专业搜索等场景。它不仅降低了模型对参数规模的依赖，也提高了系统的可扩展性和实用价值，是当前大模型落地应用中的重要技术路线之一。"
  },
  {
    "instruction": "在描述神经网络时，请确保'反向传播'这个词出现至少3次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "反向传播",
          3
        ]
      ]
    ],
    "topic": "神经网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "神经网络是一种由多层神经元组成的机器学习模型，常用于图像识别、自然语言处理和语音识别等任务。它通过前向传播计算输出结果，再利用反向传播根据误差调整各层的权重。反向传播的核心思想是将输出层的误差信号逐层传回网络内部，从而高效地更新参数。在训练过程中，反向传播与梯度下降等优化方法结合使用，使模型性能不断提升，因此反向传播被认为是现代神经网络成功的关键技术之一。"
  },
  {
    "instruction": "用正式的学术语言描述知识蒸馏。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "知识蒸馏",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "知识蒸馏（Knowledge Distillation）是一种模型压缩与迁移学习方法，其核心思想是通过引入“教师模型—学生模型”范式，将复杂、高容量的教师模型中蕴含的知识迁移至结构更为简洁、计算成本更低的学生模型。具体而言，教师模型通常在大规模数据集上训练并具备较强的泛化能力，其输出不仅包含对真实标签的预测结果，还通过软化的概率分布（即“软目标”）反映了类别间的相对关系与不确定性信息。学生模型在训练过程中同时最小化对真实标签的监督损失以及与教师模型输出分布之间的蒸馏损失，从而在保持较低模型复杂度的前提下，逼近教师模型的性能。\n\n从理论与实践角度看，知识蒸馏能够有效缓解小模型因表示能力受限而导致的性能退化问题，并在模型压缩、加速推理和降低存储开销等方面展现出显著优势。该方法已被广泛应用于计算机视觉、自然语言处理及语音识别等领域，并逐步扩展到在线学习、多教师蒸馏和跨模态知识迁移等研究方向，成为深度学习模型高效部署的重要技术之一。"
  },
  {
    "instruction": "以幽默风趣的方式介绍知识蒸馏。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "知识蒸馏",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "想象一下：  \n有一位**学霸老师**（大模型），脑容量堪比一整个图书馆，考试从不翻车；而你手里有个**小机灵学生**（小模型），反应快、吃得少、电费也省，但知识面有限。怎么办？让学霸直接“把脑子拷贝过来”？不行，脑容量不够。  \n于是——**知识蒸馏**登场了。\n\n知识蒸馏就像**老师熬汤**：  \n老师把多年学习的精华慢慢熬成一锅“高汤”（软标签、概率分布），不是只告诉你“答案是B”，而是告诉你：“A有点像，但不太行；C看着靠谱，其实是陷阱；B为什么最香。”  \n学生喝了这锅汤，瞬间**悟了**：原来重点在这里！\n\n和死记硬背相比，知识蒸馏更像**学做菜而不是背菜谱**。  \n小模型不仅学会“选哪个”，还学会“为什么选它”，于是虽然体型小，却能打出接近学霸的操作。\n\n最终效果？  \n- 老师：功成身退，继续当传说  \n- 学生：轻装上阵，跑得飞快  \n- 用户：模型又快又准，钱包还不痛  \n\n一句话总结：  \n**知识蒸馏 = 把大模型的智慧，浓缩成小模型也能喝得下的“灵魂鸡汤”。**"
  },
  {
    "instruction": "写一篇关于提示工程的短文，必须以问句结尾。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "提示工程",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "提示工程是与大模型高效协作的关键方法，它通过精心设计输入提示来引导模型产生更准确、更有用的输出。良好的提示往往包含清晰的目标、必要的上下文、明确的约束和示例，从而降低歧义并提升可控性。随着应用场景的复杂化，提示工程也逐渐发展出结构化模板、迭代优化和评测反馈等实践，你是否已经准备好系统化地设计提示来释放模型的潜力？"
  },
  {
    "instruction": "写一段关于分布式训练的文字，必须包含以下关键词：反向传播, 深度学习, 神经网络。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "人工智能",
          4
        ]
      ]
    ],
    "topic": "分布式训练",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "分布式训练是在大规模计算环境下提升模型训练效率的重要技术，通过将数据或模型划分到多台计算节点并行处理，可以显著缩短训练时间并提升可扩展性；在这一过程中，深度学习模型通常由复杂的神经网络构成，各节点在前向计算后需要通过高效的通信机制同步梯度，完成反向传播，从而保证参数一致性和收敛稳定性，最终使分布式系统能够在保证精度的同时充分利用算力资源。"
  },
  {
    "instruction": "描述自监督学习，开头必须是'众所周知'。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "自监督学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "众所周知，自监督学习是一种不依赖人工标注、通过从数据自身构造监督信号来训练模型的机器学习方法。它通常先设计预任务（如掩码预测、对比学习、时序预测等），让模型在大量无标签数据上学习通用表征，再将学到的表征迁移到下游任务中，从而降低标注成本、提升泛化能力，并在自然语言处理、计算机视觉和语音等领域取得了显著效果。"
  },
  {
    "instruction": "用通俗易懂的语言向初学者解释机器翻译。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "机器翻译",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "机器翻译，简单来说，就是**让电脑帮我们把一种语言自动翻成另一种语言**。\n\n你可以把它想成一个“非常勤奋的翻译助手”。比如你输入一句中文“你好，世界”，机器翻译会根据它学到的大量语言知识，把这句话变成英文“Hello, world”。\n\n那电脑是怎么做到的呢？  \n它并不是像人一样真正“理解”语言，而是**通过学习大量已经翻译好的例子**，找出不同语言之间的规律。就像一个学生，看了成千上万道“中文—英文对照题”，慢慢学会：这个词通常对应那个词，这种句子结构该怎么变。\n\n现在常见的机器翻译（比如在线翻译软件）多半使用的是**人工智能和神经网络**。你可以把它理解为一个会不断学习和改进的大脑，用来猜测“最可能、最自然”的翻译结果。\n\n需要注意的是，机器翻译**速度快、很方便**，但有时会翻得不够准确，特别是遇到双关语、成语或有文化背景的内容时，这时候还是需要人来帮忙检查和润色。"
  },
  {
    "instruction": "请用不少于150个字描述神经网络。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        150
      ]
    ],
    "topic": "神经网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "神经网络是一类受生物神经系统启发的计算模型，由大量相互连接的节点组成，这些节点通常被称为神经元。每个神经元通过加权求和、非线性激活等过程来处理输入信息，并将结果传递给下一层。通过多层结构，神经网络能够从原始数据中逐级学习抽象特征，从而在图像识别、语音识别和自然语言处理等任务中表现出强大能力。训练神经网络通常依赖反向传播算法和梯度下降方法，根据误差不断调整参数，使模型逐步逼近最优解。\n\n随着计算能力和数据规模的提升，深度神经网络不断发展，形成了卷积神经网络、循环神经网络和变换器等多种结构。这些模型在实际应用中推动了自动驾驶、医疗诊断和智能推荐的发展。但同时，神经网络也面临可解释性不足、训练成本高以及数据偏见等问题，因此相关研究仍在持续探索更加高效、可靠和可理解的方向。"
  },
  {
    "instruction": "以问答形式介绍分布式训练，包含至少3个问题和回答。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "分布式训练",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**问1：什么是分布式训练？**  \n答：分布式训练是指将模型训练任务分配到多台机器或多个计算设备（如多张GPU）上并行执行，以加速训练过程或支持更大规模的模型和数据。\n\n**问2：为什么需要分布式训练？**  \n答：当模型参数量或训练数据规模非常大时，单机计算和内存资源往往不足。分布式训练可以提升计算效率、缩短训练时间，并突破单机硬件的资源限制。\n\n**问3：分布式训练常见的方式有哪些？**  \n答：常见方式包括数据并行（每个节点训练相同模型但使用不同数据）、模型并行（不同节点负责模型的不同部分）以及混合并行（结合数据并行和模型并行）。\n\n**问4：分布式训练面临哪些挑战？**  \n答：主要挑战包括节点之间的通信开销、参数同步的效率、系统稳定性以及调试和部署的复杂性。"
  },
  {
    "instruction": "写一段关于RAG检索增强的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "特别"
      ]
    ],
    "topic": "RAG检索增强",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "RAG检索增强是一种将检索流程与生成流程结合的方法。系统在接收问题后，通过向量数据库对数据源进行检索，得到文档片段，并将片段作为上下文送入模型。模型在上下文范围内进行生成，从而在回答中使用数据源中的信息。该流程用于问答、搜索、客服等场景。"
  },
  {
    "instruction": "写一段关于提示工程的文字，字数要求在150到300之间。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        150
      ],
      [
        "max_length",
        300
      ]
    ],
    "topic": "提示工程",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "提示工程是指通过精心设计和优化提示语，引导大语言模型产生更准确、有用输出的技术。它强调对任务目标、上下文信息、约束条件和示例的清晰表达，帮助模型理解用户真实意图。良好的提示工程可以显著提升模型性能，减少歧义和错误，在内容创作、代码生成、数据分析等场景中具有重要价值。随着模型能力提升，提示工程也逐渐成为人机协作中的关键技能。"
  },
  {
    "instruction": "写一段关于大语言模型的文字，必须包含以下关键词：人工智能, 模型, 优化。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "人工智能",
          3
        ]
      ]
    ],
    "topic": "大语言模型",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "大语言模型是人工智能领域的重要成果，它通过对海量数据的学习构建出复杂的模型结构，从而具备理解和生成自然语言的能力，在实际应用中，研究者不断对模型进行优化，以提升其准确性、效率和泛化能力，使其在问答、写作和推理等任务中发挥更大的价值。"
  },
  {
    "instruction": "用简洁的技术文档风格说明推荐系统。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "推荐系统",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**推荐系统概述**\n\n**1. 定义**  \n推荐系统通过分析用户与内容/商品的交互数据，预测用户偏好并输出个性化排序结果。\n\n**2. 目标**  \n- 提升点击率、转化率与用户留存  \n- 降低信息过载\n\n**3. 输入与输出**  \n- 输入：用户特征、物品特征、上下文、历史行为  \n- 输出：候选物品列表及排序分数\n\n**4. 核心方法**  \n- 协同过滤（基于用户/物品）  \n- 内容推荐（特征匹配）  \n- 混合与深度学习模型（如Embedding、CTR模型）\n\n**5. 评估与部署**  \n- 指标：CTR、CVR、NDCG、AUC  \n- 流程：离线训练 → 在线推断 → A/B测试"
  },
  {
    "instruction": "写一篇关于强化学习的短文，必须以问句结尾。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "强化学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "强化学习是一种通过与环境交互来学习最优策略的机器学习方法，智能体在不断试错中根据奖励信号调整行为。它的核心在于状态、动作、奖励和策略的循环，使系统能够在不确定环境中逐步提升决策质量。强化学习已广泛应用于游戏对弈、机器人控制和资源调度等领域，并展现出超越人类经验规则的潜力，那么在更复杂且开放的现实世界中，强化学习能否实现安全、可靠且可解释的自主决策呢？"
  },
  {
    "instruction": "介绍代码生成，但不要使用'特别'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "特别"
      ]
    ],
    "topic": "代码生成",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "代码生成是指利用工具或模型，根据输入的需求、规则或示例，自动产出源代码的过程。它可以将自然语言描述、配置文件或模板转换为可运行的程序，覆盖从前端页面到后端逻辑、脚本与测试的多种场景。通过代码生成，开发者能够减少重复劳动，降低出错率，提高开发效率，并更专注于架构设计和业务创新。随着模型能力和工具链的发展，代码生成正在与 IDE、CI/CD 流程深度结合，成为现代软件工程中的重要组成部分。"
  },
  {
    "instruction": "介绍图像分割，但不要使用'很'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "很"
      ]
    ],
    "topic": "图像分割",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "图像分割是计算机视觉中的核心任务之一，目标是将图像划分为具有语义或视觉一致性的区域。通过分割，系统可以区分前景与背景，识别不同物体的边界，为后续的目标检测、识别与理解提供基础。常见方法包括基于阈值、区域生长、边缘检测，以及近年来广泛应用的深度学习模型，如卷积神经网络和 Transformer 结构。图像分割在医学影像、自动驾驶、遥感分析和工业检测等领域发挥关键作用，能够提升分析精度与自动化水平。"
  },
  {
    "instruction": "撰写模型量化的介绍，每段必须包含'特征提取'。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "特征提取",
          4
        ]
      ]
    ],
    "topic": "模型量化",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "模型量化是一种通过降低模型参数与计算精度来提升效率的技术，常用于在资源受限设备上部署深度学习模型。其核心思想是在尽量保持精度的前提下，将浮点运算转换为低比特表示，从而减少存储与计算开销，而在这一过程中需要兼顾模型在**特征提取**阶段的表达能力不被明显削弱。\n\n从方法上看，模型量化可分为训练后量化和量化感知训练两类，前者简单高效，后者精度更优。无论采用哪种方式，都需要分析网络各层对精度的敏感性，尤其是早期卷积层的**特征提取**质量，因为这些层对整体性能影响较大。\n\n在实际应用中，模型量化能够显著降低延迟和能耗，推动边缘计算与移动端智能的发展。然而，量化带来的数值误差可能影响模型稳定性，因此需要结合校准数据与优化策略，确保量化后模型在**特征提取**与决策阶段仍具备可靠表现。"
  },
  {
    "instruction": "用恰好120个字概括大语言模型的核心要点。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "exact_length",
        120
      ]
    ],
    "topic": "大语言模型",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "大语言模型以海量数据训练通过自监督学习掌握语言规律具备理解生成推理能力依托参数规模与算力提升表现可迁移到多任务但存在偏差幻觉成本与安全问题需对齐治理评测持续改进并推动应用落地协作人类提升效率促进创新发展价值同时加强法规伦理建设与透明性可解释"
  },
  {
    "instruction": "请用JSON格式输出情感分析的关键信息。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "情感分析",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "```json\n{\n  \"文本\": \"\",\n  \"情感倾向\": \"未知\",\n  \"情感极性\": 0,\n  \"置信度\": 0.0,\n  \"关键情感词\": [],\n  \"情绪类型\": []\n}\n```"
  },
  {
    "instruction": "用Markdown格式撰写关于强化学习的说明，必须包含标题和分点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "强化学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "```markdown\n# 强化学习（Reinforcement Learning）简介\n\n- **定义**：强化学习是一种机器学习方法，智能体通过与环境交互，根据获得的奖励或惩罚来学习最优策略。\n- **核心要素**：\n  - **智能体（Agent）**：做出决策的主体\n  - **环境（Environment）**：智能体所处并与之交互的外部系统\n  - **状态（State）**：对当前环境的描述\n  - **动作（Action）**：智能体在某一状态下可采取的行为\n  - **奖励（Reward）**：环境对动作的反馈信号\n- **学习目标**：\n  - 最大化长期累计奖励（回报）\n- **常见算法**：\n  - Q-learning\n  - SARSA\n  - 策略梯度（Policy Gradient）\n  - 深度强化学习（如 DQN、PPO）\n- **应用场景**：\n  - 游戏对弈（如围棋、Atari 游戏）\n  - 机器人控制\n  - 自动驾驶\n  - 资源调度与推荐系统\n```"
  },
  {
    "instruction": "请用不少于300个字描述文本分类。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        300
      ]
    ],
    "topic": "文本分类",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "文本分类是自然语言处理领域中的一项基础而重要的任务，指的是将给定的文本按照预先定义好的类别体系进行自动归类的过程。它的核心目标是让计算机理解文本所表达的主题、情感或功能，并据此做出合理判断。随着互联网信息的爆炸式增长，文本分类在信息管理、内容推荐、舆情分析等场景中发挥着越来越关键的作用。\n\n从实现流程上看，文本分类通常包括数据收集、文本预处理、特征表示、模型训练和结果评估等步骤。文本预处理阶段会进行分词、去停用词、去噪和规范化等操作，以减少无关信息对模型的干扰。特征表示是将文本转化为机器可以理解的数值形式，早期常见的方法包括词袋模型和TF-IDF，而近年来词向量、预训练语言模型等方法显著提升了语义表达能力。\n\n在分类算法方面，既有朴素贝叶斯、支持向量机、逻辑回归等传统机器学习方法，也有卷积神经网络、循环神经网络以及基于Transformer的深度学习模型。不同方法在效果、计算成本和可解释性上各有优劣，需要根据具体应用场景进行选择。文本分类被广泛应用于垃圾邮件过滤、新闻主题分类、情感分析、用户意图识别等领域，为智能系统提供了重要支撑。\n\n尽管文本分类技术已经取得了显著进展，但仍然面临语义歧义、领域迁移、数据不平衡和模型泛化能力不足等挑战。未来，随着多模态信息融合和更强大语言模型的发展，文本分类将在准确性、鲁棒性和应用范围上持续提升。"
  },
  {
    "instruction": "用简洁的技术文档风格说明机器学习。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "机器学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**机器学习（Machine Learning）概述**\n\n**定义**  \n机器学习是一种通过数据训练模型，使系统在无需显式编程的情况下完成预测、分类或决策的技术。\n\n**核心流程**  \n1. 数据采集与预处理  \n2. 特征工程  \n3. 模型选择与训练  \n4. 评估与调优  \n5. 部署与监控\n\n**主要类型**  \n- 监督学习：基于有标签数据（如回归、分类）  \n- 无监督学习：发现数据结构（如聚类、降维）  \n- 半监督学习：少量标签+大量无标签数据  \n- 强化学习：通过奖励机制进行策略学习\n\n**关键概念**  \n- 模型、特征、标签  \n- 损失函数、优化算法  \n- 过拟合与泛化能力\n\n**典型应用**  \n- 推荐系统  \n- 计算机视觉  \n- 自然语言处理  \n- 异常检测\n\n**优势与局限**  \n- 优势：自动学习模式、可扩展性强  \n- 局限：依赖数据质量、可解释性有限"
  },
  {
    "instruction": "在描述联邦学习时，请确保'梯度下降'这个词出现至少4次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "梯度下降",
          4
        ]
      ]
    ],
    "topic": "联邦学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "联邦学习是一种分布式机器学习范式，数据留在本地设备或机构中，通过安全聚合来训练共享模型。在每一轮训练中，各参与方在本地利用梯度下降更新模型参数，只上传参数更新而不上传原始数据。服务器对来自多方的更新进行聚合后，再将全局模型下发，下一轮各方继续采用梯度下降进行本地优化。由于设备和数据的异质性，联邦学习常结合自适应学习率、正则化与通信压缩，使梯度下降在不稳定网络环境下仍能收敛。整体而言，联邦学习通过多轮迭代的梯度下降实现隐私保护、协同建模与性能提升。"
  },
  {
    "instruction": "请写5段关于生成对抗网络的内容，每段以数字编号开头。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "生成对抗网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. 生成对抗网络（GAN，Generative Adversarial Networks）是一种由伊恩·古德费洛在2014年提出的深度学习模型，核心思想是通过对抗机制来学习数据分布，从而生成与真实数据高度相似的新样本。\n\n2. GAN主要由两个部分组成：生成器和判别器，生成器负责从随机噪声中生成数据，而判别器负责判断输入数据是真实数据还是生成器生成的伪造数据。\n\n3. 在训练过程中，生成器和判别器进行博弈式的优化，生成器不断尝试“欺骗”判别器，而判别器则努力提高识别真假的能力，二者相互促进，共同提升模型性能。\n\n4. 生成对抗网络在图像生成、图像修复、风格迁移、语音合成以及数据增强等领域得到了广泛应用，尤其在生成高质量图像方面展现出强大能力。\n\n5. 尽管GAN效果显著，但其训练过程不稳定、容易出现模式崩溃等问题，因此研究者提出了多种改进模型，如DCGAN、WGAN和StyleGAN，以提升训练稳定性和生成质量。"
  },
  {
    "instruction": "请写3段关于代码生成的内容，每段以数字编号开头。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "代码生成",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. 代码生成是指通过工具或模型自动生成源代码的过程，它可以根据自然语言描述、示例或规则输出可运行的程序。这种方式降低了编程门槛，让非专业开发者也能参与软件构建。\n\n2. 在实际开发中，代码生成能够显著提升效率，减少重复劳动，并帮助团队保持代码风格一致。但同时也需要注意生成代码的可维护性、安全性以及对业务逻辑的准确理解。\n\n3. 随着人工智能的发展，代码生成正从模板化走向智能化，能够理解上下文并进行复杂推理。未来，它可能成为开发者的“协作伙伴”，而不是简单的辅助工具。"
  },
  {
    "instruction": "写一段关于注意力机制的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "很"
      ]
    ],
    "topic": "注意力机制",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "注意力机制用于神经网络。机制根据输入序列计算权重，并对信息进行加权求和。模型在编码和解码过程中使用结果生成输出。自注意力在序列内部建立关联，并通过矩阵运算完成计算。应用场景包括机器翻译和语音识别。"
  },
  {
    "instruction": "在描述分布式训练时，请确保'反向传播'这个词出现至少2次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "反向传播",
          2
        ]
      ]
    ],
    "topic": "分布式训练",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "在分布式训练中，模型通常被部署在多台计算节点上，通过数据并行或模型并行的方式协同工作。每个节点先进行前向计算，然后执行反向传播来计算本地梯度；随后，这些梯度会通过通信机制进行聚合与同步，以保证参数更新的一致性。高效的反向传播实现对于减少通信开销、提升整体训练吞吐量至关重要。\n\n此外，在大规模场景下，常见的优化包括梯度压缩、异步更新以及流水线并行等，这些方法都会影响反向传播的执行顺序和同步策略。合理设计分布式训练流程，确保反向传播与参数更新高效协同，是提升模型训练速度和稳定性的关键。"
  },
  {
    "instruction": "介绍目标检测，但不要使用'特别'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "特别"
      ]
    ],
    "topic": "目标检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "目标检测是计算机视觉中的一项核心任务，目标是在图像或视频中同时完成目标的定位与类别识别。它不仅要判断画面中是否存在某类物体，还需要给出每个目标的具体位置，通常以边界框或掩码的形式表示。\n\n在技术实现上，目标检测方法大致分为两类：两阶段方法和单阶段方法。两阶段方法先生成候选区域，再对候选区域进行分类与回归，代表模型有 Faster R-CNN；单阶段方法则直接在特征图上预测目标的位置与类别，代表模型包括 YOLO 和 SSD。这类方法在速度与精度之间各有取舍。目标检测被广泛应用于自动驾驶、安防监控、医疗影像分析和工业质检等领域，是推动智能感知系统发展的重要技术之一。"
  },
  {
    "instruction": "介绍自监督学习，但不要使用'极其'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "极其"
      ]
    ],
    "topic": "自监督学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "自监督学习是一种介于监督学习与无监督学习之间的机器学习范式。它的核心思想是：不依赖人工标注的数据，而是从原始数据本身构造“伪标签”，通过设计预测任务来训练模型，从而学习有用的表示。\n\n在自监督学习中，数据的结构或内在关系被用来生成学习信号。例如，在计算机视觉中，模型可能被要求预测被遮挡的图像区域、判断两张图片是否来自同一原图，或学习不同数据增强下的一致性表示；在自然语言处理中，常见做法包括预测被掩盖的词语或判断句子之间的顺序关系。这些任务不需要人工标注，却能迫使模型理解数据的语义和结构。\n\n自监督学习的优势在于能够充分利用海量未标注数据，降低对标注成本的依赖，并提升模型在下游任务（如分类、检测、翻译等）中的性能。近年来，它已成为深度学习研究中的重要方向，并在视觉、语言、语音等多个领域展现出强大的潜力。"
  },
  {
    "instruction": "介绍自然语言处理，但不要使用'很'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "很"
      ]
    ],
    "topic": "自然语言处理",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "自然语言处理是人工智能的重要方向，研究计算机如何理解、生成与人类语言相关的内容。它结合语言学、计算机科学与统计学习，通过分词、句法分析、语义建模等方法，让机器能够完成文本分类、信息抽取、机器翻译、对话系统等任务。随着深度学习与大模型的发展，自然语言处理在搜索、客服、教育、医疗等领域得到广泛应用，并持续推动人机交互能力的提升。"
  },
  {
    "instruction": "描述语音识别的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "很"
      ]
    ],
    "topic": "语音识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "语音识别的特点包括：  \n- 将语音信号转换为文本或可执行指令  \n- 支持连续语音输入与固定命令识别  \n- 依赖声学模型、语言模型和解码算法协同工作  \n- 对口音、语速和背景噪声具备适应机制  \n- 可用于实时交互场景，也可进行离线处理  \n- 需要通过数据训练和模型更新来保持性能  \n- 在应用中需要考虑隐私保护与数据安全"
  },
  {
    "instruction": "用简洁的技术文档风格说明强化学习。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "强化学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**强化学习（Reinforcement Learning, RL）概述**\n\n**定义**  \n强化学习是一种通过与环境交互、基于奖励信号进行决策优化的机器学习方法。智能体（Agent）学习在给定状态下选择动作，以最大化长期累计回报。\n\n**核心要素**  \n- **Agent**：决策主体  \n- **Environment**：外部环境  \n- **State (S)**：环境状态  \n- **Action (A)**：可执行动作  \n- **Reward (R)**：即时反馈  \n- **Policy (π)**：状态到动作的映射  \n- **Value Function (V/Q)**：状态或状态-动作的期望回报\n\n**交互流程**  \n1. Agent 观察状态 S  \n2. 根据策略 π 选择动作 A  \n3. 环境返回奖励 R 并转移到新状态 S'  \n4. Agent 更新策略/价值函数\n\n**优化目标**  \n最大化期望累计回报（折扣回报）：\n\\[\n\\max \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^t R_t\\right]\n\\]\n\n**常见算法**  \n- **基于价值**：Q-learning、DQN  \n- **基于策略**：REINFORCE、PPO  \n- **Actor-Critic**：A2C、SAC\n\n**典型应用**  \n游戏对弈、机器人控制、资源调度、推荐系统。"
  },
  {
    "instruction": "在描述分布式训练时，请确保'模型'这个词出现至少2次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "模型",
          2
        ]
      ]
    ],
    "topic": "分布式训练",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "分布式训练是一种将计算任务分散到多台设备或多节点上的方法，通过并行处理来加速深度学习模型的训练过程。在这种方式下，模型会被复制或切分到不同的计算节点上，各节点同时对模型进行前向与反向计算，并通过通信机制同步参数更新，从而保证整体模型的一致性。分布式训练能够有效提升大规模模型训练的效率与稳定性，尤其适合数据量大、模型复杂的场景。"
  },
  {
    "instruction": "用通俗易懂的语言向初学者解释Transformer架构。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "Transformer架构",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "下面用**通俗易懂**的方式，向**初学者**解释什么是 **Transformer 架构**。\n\n---\n\n## 一句话先理解\n**Transformer 是一种让模型“重点关注关键信息”的神经网络结构，特别擅长处理文本。**\n\n---\n\n## 先想一个生活中的例子\n你在读一句话：\n\n> “小明把球扔给了小红，因为**她**站得更近。”\n\n你能立刻知道“她”指的是“小红”。  \n这是因为你会**根据上下文，重点关注相关的词**。\n\nTransformer 做的事情，和你这个理解过程很像。\n\n---\n\n## Transformer 的核心思想：注意力（Attention）\nTransformer 最重要的概念叫 **注意力机制**。\n\n可以把它理解为一句话中：\n\n- 每个词都会“看一眼”其他词  \n- 判断：**哪些词和我关系更大？**\n- 关系越大，就“关注”得越多\n\n比如在这句话里：\n> “我喜欢吃苹果，因为**它**很甜。”\n\n“它”会重点关注“苹果”，而不是“我”或“吃”。\n\n这种机制叫做 **自注意力（Self-Attention）**。\n\n---\n\n## Transformer 的基本结构\nTransformer 通常由两大部分组成：\n\n### 1️⃣ 编码器（Encoder）\n作用：**理解输入内容**\n\n- 把一句话里的每个词  \n- 变成一个“包含上下文信息的数字表示”\n- 知道每个词在整句话中的含义\n\n可以理解为：  \n👉 **读懂一句话**\n\n---\n\n### 2️⃣ 解码器（Decoder）\n作用：**生成输出内容**\n\n- 一边参考编码器理解的内容  \n- 一边一步步生成新词\n\n可以理解为：  \n👉 **根据理解，写出答案**\n\n（像翻译、写文章、聊天）\n\n---\n\n## Transformer 为什么厉害？\n和以前的模型相比，它有几个明显优点：\n\n✅ **可以同时处理整句话**（不用一个词一个词慢慢来）  \n✅ **更容易抓住长距离关系**（句子再长也不容易忘）  \n✅ **训练速度快，效果好**\n\n这也是为什么：\n- ChatGPT\n- 翻译软件\n- 文本生成模型  \n\n几乎都基于 Transformer。\n\n---\n\n## 最后用一句话总结\n**Transformer 就像一个特别会“抓重点”的阅读高手，能快速理解整段文字，并生成高质量的回应。**\n\n如果你愿意，我也可以用**图解思路**、**代码视角**或**更生活化的比喻**继续讲解 😊"
  },
  {
    "instruction": "用正式的学术语言描述目标检测。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "目标检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "目标检测（Object Detection）是计算机视觉领域中的一项核心研究任务，旨在从给定的图像或视频序列中自动识别并定位具有语义意义的目标实例。该任务不仅需要判定目标的类别属性，还需通过边界框或其他几何表示形式对目标在空间中的位置进行精确估计。作为连接图像分类与实例级理解的重要桥梁，目标检测在理论上涉及特征表示学习、判别模型构建与空间回归建模等关键问题，在实践中面临尺度变化、姿态多样性、遮挡、光照变化及复杂背景干扰等挑战。近年来，随着深度学习技术的发展，基于卷积神经网络和端到端训练框架的检测方法在精度与效率方面均取得了显著进展，使目标检测在自动驾驶、视频监控、医学影像分析和人机交互等领域得到了广泛应用。"
  },
  {
    "instruction": "描述边缘计算的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "极其"
      ]
    ],
    "topic": "边缘计算",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "边缘计算的主要特点包括：\n\n- **就近处理**：计算与存储部署在数据源附近，减少数据回传路径。  \n- **低时延响应**：本地完成分析与决策，适合对实时性有要求的业务。  \n- **带宽占用下降**：仅上传必要数据或结果，减轻网络传输压力。  \n- **数据隐私保护**：敏感数据留在本地处理，降低集中传输带来的风险。  \n- **分布式架构**：节点分散部署，系统具备弹性与容错能力。  \n- **与云协同**：边缘侧负责即时处理，云端承担集中管理与深度分析。"
  },
  {
    "instruction": "介绍Transformer架构，但不要使用'非常'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "非常"
      ]
    ],
    "topic": "Transformer架构",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "Transformer 是一种基于注意力机制的神经网络架构，最早用于序列到序列的任务，如机器翻译。与传统依赖循环或卷积的模型不同，Transformer 通过并行计算来处理序列，大幅提升了训练效率。\n\n其核心是自注意力机制（Self-Attention），模型在处理序列中的每个位置时，会根据与其他位置的关联程度分配权重，从而捕捉长距离依赖关系。为了增强表达能力，Transformer 使用多头注意力（Multi-Head Attention），让模型从多个子空间同时关注不同的信息。\n\n经典的 Transformer 由编码器（Encoder）和解码器（Decoder）组成。编码器负责将输入序列映射为上下文表示，解码器在此基础上逐步生成输出。每一层都包含注意力模块、前馈全连接网络，以及残差连接和层归一化，以保证训练的稳定性。\n\n由于 Transformer 不包含循环结构，需要通过位置编码（Positional Encoding）来引入序列顺序信息。凭借高效并行和强大的建模能力，Transformer 已成为自然语言处理和多模态领域的重要基础架构。"
  },
  {
    "instruction": "撰写Transformer架构的介绍，每段必须包含'反向传播'。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "反向传播",
          3
        ]
      ]
    ],
    "topic": "Transformer架构",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "Transformer架构是一种基于注意力机制的深度学习模型，最早用于自然语言处理任务，其核心思想是通过自注意力在序列中建模全局依赖关系。在整体结构上，Transformer摒弃了循环和卷积，采用编码器—解码器堆叠方式，使并行计算成为可能，而模型参数的更新仍然依赖于反向传播来最小化训练目标函数。\n\n在具体组成上，Transformer的编码器和解码器都由多头自注意力层和前馈神经网络层构成，并通过残差连接与层归一化来稳定训练过程。多头机制允许模型在不同子空间中关注不同位置的信息，这些复杂权重在训练阶段通过反向传播不断调整，从而学习到有效的表示。\n\n在训练与应用层面，Transformer通常结合大规模数据和优化算法（如Adam）进行端到端训练，其优势体现在高效并行、长距离依赖建模能力强等方面。无论是在机器翻译、文本生成还是多模态任务中，模型性能的提升都离不开反向传播对注意力权重和网络参数的精细更新。"
  },
  {
    "instruction": "用恰好50个字概括文本分类的核心要点。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "exact_length",
        50
      ]
    ],
    "topic": "文本分类",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "文本分类通过特征表示与模型学习将文本映射到预定义类别强调数据质量特征选择算法评估与应用场景适配优化性"
  },
  {
    "instruction": "写一段关于生成对抗网络的文字，必须包含以下关键词：算法, 优化, 神经网络。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "模型",
          4
        ]
      ]
    ],
    "topic": "生成对抗网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "生成对抗网络（GAN）是一种重要的深度学习模型，由生成器和判别器两个相互博弈的神经网络组成，其核心思想是一种对抗式算法：生成器负责产生逼真的数据样本，而判别器负责区分真实数据与生成数据，通过不断的对抗训练实现整体性能的优化，在这一过程中，模型参数在损失函数的引导下持续更新，使生成结果逐步接近真实分布，因此生成对抗网络在图像生成、数据增强等领域展现出强大的能力。"
  },
  {
    "instruction": "在描述计算机视觉时，请确保'反向传播'这个词出现至少4次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "反向传播",
          4
        ]
      ]
    ],
    "topic": "计算机视觉",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "计算机视觉是让机器理解图像与视频的技术，涵盖目标检测、语义分割和三维重建等任务。现代计算机视觉大量依赖深度学习，通过卷积神经网络从数据中自动学习特征。在训练过程中，模型利用反向传播来计算损失对参数的梯度，并不断更新权重；反向传播使网络能够逐层纠正错误，提高表示能力。随着数据规模和算力提升，反向传播在大规模视觉模型中依然是核心机制，而结合正则化、优化器和自监督学习，反向传播推动了计算机视觉在自动驾驶、医学影像等领域的突破。"
  },
  {
    "instruction": "撰写计算机视觉的介绍，每段必须包含'训练'。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "训练",
          4
        ]
      ]
    ],
    "topic": "计算机视觉",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "计算机视觉是人工智能的重要分支，旨在让计算机理解和分析图像与视频中的信息。通过对大量视觉数据进行训练，模型能够学习到物体的形状、颜色、纹理以及空间关系，从而实现图像分类、目标检测和场景理解等任务。随着深度学习的发展，基于神经网络的训练方法显著提升了计算机视觉系统的准确性和鲁棒性。\n\n在实际应用中，计算机视觉被广泛用于自动驾驶、医学影像、安防监控和工业检测等领域。为了适应复杂多变的现实环境，研究者通常需要进行持续的训练和优化，包括数据增强、迁移学习和自监督训练等技术，这些方法帮助模型在有限标注数据下依然获得良好性能。"
  },
  {
    "instruction": "介绍机器学习，但不要使用'非常'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "非常"
      ]
    ],
    "topic": "机器学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "机器学习是人工智能的一个重要分支，研究如何让计算机从数据中学习规律，并在没有明确规则编程的情况下做出预测或决策。通过统计方法、算法设计和计算能力，模型可以在训练数据上不断调整参数，以提升在新数据上的表现。\n\n常见的机器学习类型包括监督学习、无监督学习和强化学习，应用覆盖图像识别、自然语言处理、推荐系统和金融风控等领域。随着数据规模增长和算力提升，机器学习正在改变产业流程，也对数据质量、模型解释性和伦理提出新的要求。"
  },
  {
    "instruction": "用简洁的技术文档风格说明异常检测。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "异常检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**异常检测（Anomaly Detection）**\n\n**定义**  \n识别与大多数正常数据模式显著不同的数据点、序列或行为的技术。\n\n**目标**  \n- 发现潜在故障、欺诈或安全事件  \n- 提前预警，降低风险与损失\n\n**输入/输出**  \n- 输入：结构化或非结构化数据（数值、时间序列、日志、图像等）  \n- 输出：异常分数、异常标签（正常/异常）、告警\n\n**常用方法**  \n- 统计方法：阈值、分布假设（Z-score、IQR）  \n- 机器学习：孤立森林、LOF、One-Class SVM  \n- 深度学习：自编码器、LSTM、变分自编码器  \n- 规则/领域方法：业务规则、专家系统\n\n**评估指标**  \n- Precision / Recall / F1  \n- ROC-AUC、PR-AUC  \n- 误报率（FPR）、漏报率（FNR）\n\n**应用场景**  \n- 运维监控、设备故障检测  \n- 金融欺诈、风控  \n- 网络安全、入侵检测  \n- 质量控制\n\n**注意事项**  \n- 类别不平衡与标签稀缺  \n- 概念漂移与阈值更新  \n- 可解释性与实时性需求"
  },
  {
    "instruction": "介绍图像分割，但不要使用'特别'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "特别"
      ]
    ],
    "topic": "图像分割",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "图像分割是计算机视觉中的一项基础任务，目标是将图像划分为若干具有一致属性的区域或对象，使像素级信息更易于理解和分析。通过分割，系统可以定位目标边界、区分前景与背景，并为后续的识别、测量和语义理解提供结构化输入。\n\n常见的图像分割方法包括基于阈值、边缘、区域生长和聚类的传统算法，以及以卷积神经网络为核心的深度学习方法，如 FCN、U-Net 和 Mask R-CNN。传统方法依赖人工设计特征，而深度学习方法通过大规模数据进行端到端训练，在复杂场景中通常更具鲁棒性。\n\n图像分割已广泛应用于医学影像分析、自动驾驶、遥感解译、工业质检和增强现实等领域，有助于提升自动化水平和决策精度。随着算法、算力和数据规模的持续进步，图像分割技术正不断向更高精度和更强泛化能力发展。"
  },
  {
    "instruction": "在描述模型量化时，请确保'优化'这个词出现至少3次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "优化",
          3
        ]
      ]
    ],
    "topic": "模型量化",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "模型量化是一种通过将高精度参数和计算转换为低比特表示的方法，以降低模型的存储和计算成本。在这一过程中，量化策略的选择直接影响推理精度，因此需要通过多种手段进行优化，以在性能与效率之间取得平衡。常见做法包括后训练量化和量化感知训练，前者侧重部署阶段的快速优化，后者则在训练阶段引入量化误差以实现更充分的优化。此外，结合硬件特性进行算子级优化，能够进一步提升吞吐与能效，使整体系统在资源受限场景下表现更优。"
  },
  {
    "instruction": "写一篇关于Transformer架构的短文，必须以问句结尾。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "Transformer架构",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "Transformer架构是一种以自注意力机制为核心的神经网络模型，通过并行计算取代循环结构，有效捕捉序列中长距离依赖关系，其典型结构由编码器和解码器堆叠而成，结合多头注意力、位置编码和前馈网络，在机器翻译、文本生成和多模态任务中展现出卓越性能，那么Transformer为何能够在深度学习领域迅速成为主流架构并持续推动模型能力的跃升？"
  },
  {
    "instruction": "写一段关于自然语言处理的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "非常"
      ]
    ],
    "topic": "自然语言处理",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "语言处理技术研究文本与语音中的符号与结构，通过算法与数据完成分词、标注、解析与生成。该领域结合语言学、统计学与计算方法，用于搜索、问答、翻译与对话系统。研究过程依赖语料、模型训练与评测指标，并在应用中迭代更新。"
  },
  {
    "instruction": "描述注意力机制，开头必须是'众所周知'。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "注意力机制",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "众所周知，注意力机制是一种让模型在处理信息时动态关注关键部分的方法，它通过计算查询（Query）与键（Key）之间的相关性来为对应的值（Value）分配权重，从而突出对当前任务最重要的信息、抑制无关内容；这一机制能够捕捉长距离依赖、提升表示能力，并已广泛应用于自然语言处理、计算机视觉和多模态模型中，是现代深度学习模型（如 Transformer）的核心组成部分。"
  },
  {
    "instruction": "用Markdown格式撰写关于机器翻译的说明，必须包含标题和分点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "机器翻译",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "```markdown\n# 机器翻译简介\n\n机器翻译（Machine Translation, MT）是指利用计算机技术将一种自然语言自动转换为另一种自然语言的过程。\n\n## 主要特点\n- **自动化程度高**：无需人工逐句翻译，可快速处理大量文本  \n- **多语言支持**：能够在多种语言之间进行互译  \n- **实时性强**：适用于即时通信、在线内容翻译等场景  \n\n## 常见方法\n- **基于规则的机器翻译**：依赖人工编写的语言规则和词典  \n- **统计机器翻译**：通过大规模双语语料进行概率建模  \n- **神经机器翻译**：使用深度学习模型，提高翻译的流畅性和准确性  \n\n## 应用场景\n- **跨语言交流**：帮助不同语言背景的人沟通  \n- **信息获取**：快速理解外文资料和文献  \n- **本地化服务**：支持软件、网站和产品的多语言版本  \n```"
  },
  {
    "instruction": "用通俗易懂的语言向初学者解释Transformer架构。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "Transformer架构",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "下面用**通俗易懂、不用公式**的方式，给初学者讲一讲 **Transformer 架构**。\n\n---\n\n## 一句话先懂它\n**Transformer 是一种让模型“一次性看懂整句话里所有词之间关系”的方法，特别擅长处理语言。**\n\n---\n\n## 为什么需要 Transformer？\n在 Transformer 出现前，常用的是 **RNN / LSTM**，它们是**按顺序一个词一个词读**的，就像读书要从头读到尾：\n- 读得慢（不能并行）\n- 记忆长句子很吃力\n\n**Transformer 的想法是：**\n> 不按顺序读，而是把整句话摊开，同时看，看看每个词和其他词有什么关系。\n\n---\n\n## Transformer 的核心：注意力（Attention）\n可以把 **注意力** 理解成一句话：\n\n> **“我在看一个词时，应该重点关注句子里的哪些其他词？”**\n\n举个例子：\n> “小明把球扔给了小红，因为**他**很开心。”\n\n这里的“他”指的是谁？\n- Transformer 会“看”整句话\n- 发现“他”和“小明”关系最紧密  \n👉 所以判断“他”指的是“小明”\n\n这种“找关系”的能力，就是 **注意力机制**。\n\n---\n\n## 什么是自注意力（Self-Attention）？\n**自注意力 = 句子里的每个词，都在和“同一句话里的所有词”互相看关系**\n\n也就是说：\n- 每个词都会问：“我和其他词关系有多大？”\n- 关系大的，就多看一点\n- 关系小的，就少看一点\n\n这样，每个词都会得到一个**更懂上下文的新表示**\n\n---\n\n## Transformer 的整体结构\n可以把 Transformer 想成 **两大部分**：\n\n### 1️⃣ 编码器（Encoder）\n作用：**理解输入**\n- 输入一句话\n- 输出“这句话被理解后的表示”\n\n编码器由多层组成，每一层都有：\n1. **自注意力**：看清词与词的关系\n2. **前馈神经网络**：对每个词再“思考”一下\n\n---\n\n### 2️⃣ 解码器（Decoder）\n作用：**生成输出**\n- 比如：根据英文生成中文\n- 或根据前面的词，预测下一个词\n\n解码器在生成时：\n- 看已经生成的词\n- 关注编码器给出的“理解结果”\n- 一步一步生成新词\n\n---\n\n## 多头注意力（Multi-Head Attention）是啥？\n可以理解成：\n\n> **“从多个角度同时看一句话”**\n\n比如：\n- 一个“头”关注主语和动词\n- 一个“头”关注时间\n- 一个“头”关注因果关系\n\n最后把这些角度合在一起，理解得更全面。\n\n---\n\n## 位置编码（Positional Encoding）\n你可能会问：\n> Transformer 一次看所有词，那它怎么知道词的顺序？\n\n答案是：**加“位置信息”**\n- 每个词不仅有“词的意思”\n- 还有“我在第几个位置”\n\n这样模型就知道：\n- “我爱你”和“你爱我”是不一样的\n\n---\n\n## Transformer 的优点总结\n✅ 可以并行计算，训练快  \n✅ 擅长处理长文本  \n✅ 能捕捉复杂的词语关系  \n✅ 是 GPT、BERT 等模型的基础  \n\n---\n\n## 再用一句话总结\n**Transformer 就像一个聪明的读者：**\n> 它会把整句话摊开，从多个角度看清每个词和其他词的关系，然后再做出判断或生成内容。\n\n如果你愿意，我也可以用 **画图式比喻** 或 **一步步带你看一个具体例子** 来继续讲解。"
  },
  {
    "instruction": "用通俗易懂的语言向初学者解释迁移学习。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "迁移学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "迁移学习可以理解为：**把已经学会的知识，拿来帮你更快地学新东西。**\n\n打个比方：  \n你已经会骑自行车了，再去学骑摩托车就会容易很多。因为你已经懂得**保持平衡、看路、转弯**这些基础技能。迁移学习就是这样——**先学一个任务，再把学到的经验用到另一个相关任务上**。\n\n放到人工智能里：  \n假设有一个模型，已经通过大量图片学会了“识别猫和狗”。这个模型其实已经懂了很多**通用的东西**，比如边缘、颜色、形状。现在你想让它学会“识别老虎和狮子”，就不需要从零开始训练，只要在原来的基础上稍微再学一学就行了。这就是迁移学习。\n\n**为什么要用迁移学习？**  \n- ✅ 省时间：不用从头学  \n- ✅ 省数据：新任务只需要少量数据  \n- ✅ 效果更好：站在“前人的肩膀”上学习  \n\n**一句话总结：**  \n迁移学习就是——**先学会一个，再用它来更快学下一个。**"
  },
  {
    "instruction": "用Markdown格式撰写关于大语言模型的说明，必须包含标题和分点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "大语言模型",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "```markdown\n# 大语言模型（LLM）简介\n\n## 定义\n- 大语言模型（Large Language Model, LLM）是一类基于深度学习的自然语言处理模型  \n- 通过海量文本数据训练，能够理解和生成自然语言  \n\n## 核心特性\n- **规模大**：拥有数十亿甚至更多参数  \n- **泛化能力强**：可在多种任务上表现良好  \n- **上下文理解**：能够利用上下文进行连贯回答  \n\n## 主要应用\n- **文本生成**：写作、摘要、翻译  \n- **对话系统**：智能客服、聊天机器人  \n- **代码辅助**：代码生成、调试建议  \n\n## 局限与挑战\n- **幻觉问题**：可能生成不准确或虚构的信息  \n- **计算成本高**：训练和推理需要大量算力  \n- **数据与偏见**：模型可能继承训练数据中的偏见  \n```"
  },
  {
    "instruction": "请用列表形式列出模型压缩的5个要点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "列表"
      ]
    ],
    "topic": "模型压缩",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. **剪枝（Pruning）**：去除对模型贡献较小的权重或结构，减少参数量和计算量。  \n2. **量化（Quantization）**：将高精度参数和计算转换为低精度表示，降低存储和推理成本。  \n3. **知识蒸馏（Knowledge Distillation）**：用大模型指导小模型学习，在保持性能的同时缩小模型规模。  \n4. **低秩分解（Low-rank Factorization）**：将大矩阵分解为多个低秩矩阵，减少参数和计算复杂度。  \n5. **参数共享与结构优化**：通过共享权重或设计高效网络结构，提高模型紧凑性和推理效率。"
  },
  {
    "instruction": "在描述图像分割时，请确保'深度学习'这个词出现至少3次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "深度学习",
          3
        ]
      ]
    ],
    "topic": "图像分割",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "图像分割是计算机视觉中的关键任务，旨在将图像划分为具有语义意义的区域。随着深度学习的发展，基于深度学习的卷积神经网络在图像分割中取得了显著进展。深度学习模型如 U-Net 和 Mask R-CNN 能够自动学习多尺度特征，使深度学习方法在医学影像分析和自动驾驶等领域表现出色。"
  },
  {
    "instruction": "写一段关于代码生成的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "极其"
      ]
    ],
    "topic": "代码生成",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "代码生成是一种过程。系统根据规则产生代码。人编写规则，系统执行转换。结果用于开发、测试、部署。过程涉及输入、处理、输出。"
  },
  {
    "instruction": "用简洁的技术文档风格说明强化学习。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "强化学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**强化学习（Reinforcement Learning, RL）概述**\n\n**定义**  \n强化学习是一种通过与环境交互、基于奖励信号进行决策优化的机器学习方法，目标是学习最优策略以最大化累积回报。\n\n**核心要素**  \n- **智能体（Agent）**：做出决策的主体  \n- **环境（Environment）**：智能体交互的外部系统  \n- **状态（State）**：环境的当前描述  \n- **动作（Action）**：智能体可执行的行为  \n- **奖励（Reward）**：对动作结果的反馈  \n- **策略（Policy）**：状态到动作的映射  \n- **价值函数（Value Function）**：状态或动作的期望回报\n\n**学习过程**  \n1. 智能体观察当前状态  \n2. 根据策略选择动作  \n3. 环境返回奖励并转移到新状态  \n4. 更新策略或价值函数\n\n**常见算法**  \n- 基于价值：Q-learning、SARSA  \n- 基于策略：Policy Gradient  \n- 混合方法：Actor-Critic、PPO、DDPG\n\n**应用场景**  \n- 游戏与仿真  \n- 机器人控制  \n- 资源调度与推荐系统  \n- 自动驾驶与运筹优化"
  },
  {
    "instruction": "请写5段关于分布式训练的内容，每段以数字编号开头。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "分布式训练",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. 分布式训练是指将机器学习模型的训练过程分散到多台计算节点上协同完成，通过并行计算来加速训练过程，尤其适用于大规模数据集和超大参数模型的场景。  \n2. 在分布式训练中，常见的并行方式包括数据并行和模型并行，前者将数据划分到不同节点上训练相同模型，后者则将模型参数拆分到不同设备上共同计算。  \n3. 高效的通信机制是分布式训练的关键，例如参数服务器架构和基于全量或分块 All-Reduce 的通信方式，直接影响训练速度和系统的可扩展性。  \n4. 分布式训练也面临诸多挑战，如节点间通信开销、负载不均衡、容错处理以及梯度同步带来的性能瓶颈，需要在系统和算法层面进行优化。  \n5. 目前主流深度学习框架如 TensorFlow、PyTorch 和 JAX 都提供了成熟的分布式训练支持，结合高速网络和加速硬件，使大规模模型训练变得更加高效和可行。"
  },
  {
    "instruction": "用通俗易懂的语言向初学者解释迁移学习。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "迁移学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "迁移学习可以理解为“把以前学到的经验，拿来解决新问题”。\n\n举个通俗的例子：  \n你已经会骑自行车了，现在去学骑电动车。虽然不完全一样，但平衡、转弯这些基本能力是相通的，所以你学得会更快。这就是迁移学习的思想。\n\n在机器学习里也是这样：  \n1. **先学一个任务**：比如用大量图片训练一个模型，让它学会识别“猫、狗、汽车”等常见东西。  \n2. **再做新任务**：后来你想让模型识别“医学影像”或“工厂零件缺陷”。  \n3. **直接利用已有能力**：不从零开始训练，而是用之前那个模型学到的“看形状、看边缘、看纹理”的能力，再稍微调整一下，就能更快学会新任务。\n\n为什么要用迁移学习？  \n- **省时间**：不用从头训练。  \n- **省数据**：新任务就算数据不多，也能有不错效果。  \n- **效果更好**：站在“前人的肩膀上”学习。\n\n一句话总结：  \n**迁移学习就是让模型把“旧知识”迁移到“新问题”上，用已有经验更快、更好地学习。**"
  },
  {
    "instruction": "撰写强化学习的介绍，每段必须包含'人工智能'。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "人工智能",
          2
        ]
      ]
    ],
    "topic": "强化学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "强化学习是人工智能领域中的一种重要学习范式，研究智能体如何在环境中通过试错来学习最优行为策略。与监督学习不同，强化学习强调从环境反馈的奖励信号中不断改进决策能力，使人工智能系统能够在不确定和动态的情境下自主学习。\n\n在强化学习框架中，智能体、环境、状态、动作和奖励构成了核心要素，这些要素共同定义了学习过程。通过策略评估与策略改进，人工智能模型可以在长期回报最大化的目标下逐步优化行为，这一过程体现了人工智能对决策与控制问题的深度建模能力。\n\n强化学习已被广泛应用于游戏对弈、机器人控制、推荐系统和自动驾驶等场景，展示了人工智能在复杂任务中的潜力。随着算法与算力的发展，强化学习正推动人工智能从被动感知走向主动决策，同时也带来了样本效率、安全性和可解释性等新的研究挑战。"
  },
  {
    "instruction": "撰写情感分析的介绍，每段必须包含'模型'。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "模型",
          4
        ]
      ]
    ],
    "topic": "情感分析",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "情感分析是自然语言处理领域的重要研究方向，旨在从文本中识别和理解情绪倾向与主观态度。通过构建不同类型的模型，研究者可以对评论、社交媒体内容或用户反馈进行自动化分析，从而辅助决策制定与舆情监测。\n\n随着深度学习的发展，情感分析模型不断演进，从传统的机器学习方法到基于神经网络的复杂模型，性能和适用范围显著提升。这些模型能够更好地捕捉语义上下文和隐含情绪，为实际应用提供更加准确和细致的分析结果。"
  },
  {
    "instruction": "写一篇关于深度学习的短文，必须以问句结尾。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "深度学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "深度学习是人工智能的重要分支，通过多层神经网络从大量数据中自动学习特征。它在计算机视觉、语音识别和自然语言处理等领域取得了突破性进展，但也面临数据依赖、可解释性和能耗等挑战。随着算力提升与算法创新并行推进，深度学习能否在保证安全与公平的前提下持续推动智能系统走向更高层次？"
  },
  {
    "instruction": "用恰好120个字概括机器翻译的核心要点。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "exact_length",
        120
      ]
    ],
    "topic": "机器翻译",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "机器翻译通过统计方法规则模型与神经网络"
  },
  {
    "instruction": "用正式的学术语言描述文本分类。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "文本分类",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "文本分类是自然语言处理领域中的一项基础性研究任务，其目标是依据文本内容的语义特征与结构特征，将文本自动归入预先定义的类别体系之中。该过程通常涉及文本表示、特征提取与分类决策等关键环节，既可以基于传统的统计学习方法（如朴素贝叶斯、支持向量机等），也可以依托深度学习模型（如卷积神经网络、循环神经网络及预训练语言模型）实现。文本分类既可在监督学习框架下利用标注语料进行建模，也可在半监督或无监督条件下开展研究，广泛应用于信息检索、情感分析、主题识别与舆情监测等场景，对于提升大规模文本信息处理的自动化与智能化水平具有重要意义。"
  },
  {
    "instruction": "在描述深度学习时，请确保'特征提取'这个词出现至少3次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "特征提取",
          3
        ]
      ]
    ],
    "topic": "深度学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "深度学习是一种以多层神经网络为核心的方法，能够从大量数据中自动学习表示。与传统方法不同，深度学习通过端到端训练完成特征提取，减少人工设计。随着网络层数加深，模型可以逐级进行特征提取，从低级模式到高级语义的特征提取，使得在图像、语音和自然语言处理中取得突破。"
  },
  {
    "instruction": "描述生成对抗网络的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "非常"
      ]
    ],
    "topic": "生成对抗网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "生成对抗网络（GAN）具有以下特点：  \n- 由生成器与判别器组成，两者通过对抗训练共同优化。  \n- 生成器学习数据分布，用于产生与真实数据相似的样本。  \n- 判别器负责区分真实数据与生成数据，提供反馈信号。  \n- 训练过程体现博弈思想，目标是达到纳什均衡状态。  \n- 可用于图像生成、数据增强、风格迁移等任务。"
  },
  {
    "instruction": "写一段关于图像分割的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "极其"
      ]
    ],
    "topic": "图像分割",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "图像分割是一种处理流程，用于将图像中的像素划分为区域。分割依据规则或模型，对像素集合进行标记。结果支持目标检测、测量和分析任务。算法接收输入图像，输出区域标签。"
  },
  {
    "instruction": "描述注意力机制的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "非常"
      ]
    ],
    "topic": "注意力机制",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "注意力机制的主要特点包括：\n\n- 通过为不同信息分配权重，突出关键部分，抑制无关内容。  \n- 权重随输入变化而调整，适应不同上下文。  \n- 支持对长距离依赖的建模，缓解序列信息衰减问题。  \n- 注意力权重可视化，便于分析模型关注的区域，增强可解释性。  \n- 在自注意力结构中具备并行计算特性，适用于大规模序列处理。  \n- 与多头结构结合时，可在不同特征子空间中学习多样化表示。"
  },
  {
    "instruction": "以幽默风趣的方式介绍自然语言处理。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "自然语言处理",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "想象一下，有一天你对着电脑说：“帮我写个周报，但别太像周报。”电脑愣了一下，居然真的写出来了——这时，自然语言处理（NLP）就像一位偷偷练成“读心术”的理工科同学，在你背后得意地推了推眼镜。\n\n自然语言处理，简单说，就是教机器学会“说人话”。可这事一点也不简单：人类说话爱拐弯、爱省略、爱阴阳怪气，比如“呵呵”到底是开心、尴尬，还是“你完了”？NLP要做的，就是在一堆文字里分清谁是主语、谁在吐槽、谁在认真干活，还要顺便判断你是不是在生气。\n\n所以，每当你用语音助手点歌、被输入法“猜中你下一句话”、或者被客服机器人一本正经地安慰时，请记住：那不是机器突然开窍了，而是自然语言处理在背后疯狂学习人类的语言套路——努力装得像个人。"
  },
  {
    "instruction": "用三段式结构（引言、正文、结论）介绍RAG检索增强。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "RAG检索增强",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**引言：**  \nRAG（Retrieval-Augmented Generation，检索增强生成）是一种将信息检索与大语言模型生成能力相结合的技术，旨在解决模型“只凭记忆回答”带来的知识过时、幻觉等问题。通过在生成回答前引入外部知识，RAG能够让模型的输出更加准确、可追溯和贴近真实数据。\n\n**正文：**  \nRAG的核心流程通常分为两步：首先是检索（Retrieval），系统根据用户问题从外部知识库（如文档、数据库或向量库）中检索出最相关的信息；其次是生成（Generation），将检索到的内容与用户问题一同输入大语言模型，由模型综合上下文生成最终回答。这种方式使模型不再完全依赖参数内的静态知识，而是可以动态利用最新、最相关的数据，从而显著提升专业问答、企业知识库、智能客服等场景的效果。\n\n**结论：**  \n总体来看，RAG检索增强是一种有效弥补大语言模型知识局限的重要方法。它通过“先查再答”的机制，提高了回答的准确性、可靠性和可解释性。随着向量检索和大模型能力的不断进步，RAG正逐渐成为构建高质量智能应用的关键技术之一。"
  },
  {
    "instruction": "请用JSON格式输出文本分类的关键信息。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "文本分类",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "```json\n{\n  \"text\": \"\",\n  \"language\": \"\",\n  \"predicted_label\": \"\",\n  \"confidence\": 0.0,\n  \"labels_scores\": {\n    \"label_1\": 0.0,\n    \"label_2\": 0.0\n  },\n  \"keywords\": [],\n  \"model\": \"\"\n}\n```"
  },
  {
    "instruction": "请用不少于150个字描述强化学习。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        150
      ]
    ],
    "topic": "强化学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "强化学习是一种以智能体与环境持续交互为核心的机器学习方法。智能体在不同状态下选择行动，环境根据行动反馈奖励并转移到新状态。学习的目标不是直接获得正确答案，而是通过反复试错，逐步形成能够最大化长期累计回报的策略。强化学习强调探索与利用之间的平衡，既要尝试未知行动，又要利用"
  },
  {
    "instruction": "写一段关于情感分析的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "很"
      ]
    ],
    "topic": "情感分析",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "情感分析是对文本中的情绪和态度进行分析的过程。该过程使用数据和模型，对语句进行分类与预测。流程包括数据收集、标注、训练和推断。结果可用于舆情监测、产品评价和服务反馈，帮助研究者理解用户观点并支持决策。"
  },
  {
    "instruction": "请用不少于250个字描述模型压缩。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        250
      ]
    ],
    "topic": "模型压缩",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "模型压缩是指在尽量保持模型性能不明显下降的前提下，减少模型参数规模、计算量和存储开销的一类技术总称。随着深度学习模型不断变大，模型在服务器、移动端和嵌入式设备上的部署成本不断上升，模型压缩因此成为连接算法研究与实际应用的重要桥梁。通过压缩，可以显著降低推理延迟、能耗和内存占用，使模型更容易落地。  \n\n常见的模型压缩方法包括参数剪枝、量化、知识蒸馏和低秩分解等。参数剪枝通过分析权重的重要性，删除对结果影响较小的连接或通道，从而得到更稀疏的网络结构。量化则将高精度的浮点参数和计算替换为低比特表示，在硬件支持下能够大幅提升推理效率。知识蒸馏通过让小模型学习大模型的输出分布或中间表示，在较小规模下获得接近大模型的性能。  \n\n此外，低秩分解和结构重参数化等方法从数学结构角度减少冗余计算，使模型在推理阶段更加高效。实际应用中，模型压缩往往不是单一技术，而是多种方法的组合，需要在精度、速度和资源之间进行权衡。随着硬件和算法的共同发展，模型压缩将持续推动人工智能在更多真实场景中的普及与应用。"
  },
  {
    "instruction": "请用不少于300个字描述分布式训练。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        300
      ]
    ],
    "topic": "分布式训练",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "分布式训练是指在多台计算节点或多块计算设备上协同完成同一个模型训练任务的一种技术方案，广泛应用于深度学习和大规模机器学习领域。随着数据规模和模型参数量的迅速增长，单机训练在计算能力、内存容量和训练时间等方面逐渐成为瓶颈，分布式训练因此成为提升训练效率和可扩展性的关键手段。\n\n从基本思想上看，分布式训练通过将计算任务拆分并行执行来加速训练过程。常见的方式包括数据并行、模型并行以及流水线并行。数据并行是最常用的方法，它将训练数据划分为多个子集，分别分配给不同的计算节点，每个节点拥有完整模型并独立计算梯度，随后通过通信机制对梯度进行聚合并同步参数。模型并行则是将模型结构本身拆分，不同节点负责模型的不同部分，适用于参数规模极其庞大的模型。流水线并行结合了时间维度的调度思想，使不同节点在不同阶段同时工作，从而提高整体吞吐率。\n\n在实现层面，分布式训练高度依赖高效的通信机制与一致性策略，例如参数服务器架构或去中心化的全量同步方式。通信开销、网络带宽和延迟都会直接影响训练效率，因此需要在计算与通信之间进行精细的系统优化。同时，还要处理诸如节点故障、负载不均衡、数值稳定性等工程问题。\n\n总体而言，分布式训练不仅是提升训练速度的工具，更是支撑大模型和大数据时代的重要基础技术。通过合理的并行策略和系统设计，分布式训练能够在保证模型效果的前提下显著降低训练成本，为人工智能在科研和产业中的广泛应用提供有力支撑。"
  },
  {
    "instruction": "在描述注意力机制时，请确保'训练'这个词出现至少3次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "训练",
          3
        ]
      ]
    ],
    "topic": "注意力机制",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "注意力机制是一种让模型在处理序列数据时动态分配权重的方法，它能够根据上下文选择最重要的信息。在模型**训练**过程中，注意力机制通过反向传播不断调整参数，使关键特征获得更高权重；这种**训练**方式可以提升模型对长距离依赖的建模能力。随着多轮**训练**的进行，注意力分布会逐渐稳定，从而提高预测效果；因此，高效的**训练**策略对注意力机制的性能至关重要。"
  },
  {
    "instruction": "写一段关于代码生成的文字，必须包含以下关键词：神经网络, 人工智能, 参数。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "参数",
          3
        ]
      ]
    ],
    "topic": "代码生成",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "代码生成是现代软件工程中的重要方向，依托人工智能技术，系统可以通过神经网络学习大量源代码的结构与模式，从而自动生成高质量的程序片段，在这一过程中，模型会根据训练数据不断调整内部参数，以提升对语法、语义以及开发者意图的理解能力。"
  },
  {
    "instruction": "在描述强化学习时，请确保'损失函数'这个词出现至少3次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "损失函数",
          3
        ]
      ]
    ],
    "topic": "强化学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "强化学习是一种通过与环境交互来学习策略的机器学习方法。智能体根据奖励信号更新策略，常见算法会通过构建**损失函数**来度量预测与目标之间的差异。无论是策略梯度还是Q学习，核心训练过程都需要定义合适的**损失函数**，用于反向传播和参数更新。在实践中，设计稳定的**损失函数**能够缓解训练不稳定、提升收敛速度，而多目标场景下还可能组合多个**损失函数**进行联合优化。"
  },
  {
    "instruction": "请用列表形式列出图像分割的6个要点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "列表"
      ]
    ],
    "topic": "图像分割",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. **目标与定义**：将图像划分为具有语义或视觉一致性的区域或对象。  \n2. **常用方法**：阈值分割、区域生长、边缘检测、聚类方法以及深度学习分割。  \n3. **特征与表示**：利用颜色、纹理、形状、空间关系等特征进行区分。  \n4. **数据与标注**：高质量训练数据和精确标注对分割效果至关重要。  \n5. **评价指标**：常用指标包括IoU、Dice系数、像素准确率等。  \n6. **应用与挑战**：广泛应用于医学影像、自动驾驶等领域，同时面临噪声、尺度变化等挑战。"
  },
  {
    "instruction": "请用列表形式列出分布式训练的4个要点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "列表"
      ]
    ],
    "topic": "分布式训练",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. **数据并行与模型并行**：合理选择数据并行、模型并行或混合并行方式，以适配模型规模和硬件资源。  \n2. **通信与同步机制**：高效的参数同步（如 All-Reduce）和通信优化是提升分布式训练性能的关键。  \n3. **负载均衡与资源管理**：确保各节点计算负载均衡，充分利用 GPU、网络和存储资源。  \n4. **容错与一致性**：通过检查点、重试机制等手段保证训练在节点故障下的稳定性和结果一致性。"
  },
  {
    "instruction": "写一篇关于文本分类的短文，必须以问句结尾。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "文本分类",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "文本分类是自然语言处理中的一项基础任务，旨在将文本按照主题、情感或功能等标准自动归入预定义的类别，它广泛应用于垃圾邮件过滤、舆情分析、内容推荐和客户服务等场景，随着深度学习和预训练模型的发展，文本分类在特征表示和泛化能力上取得了显著进步，但仍面临数据不平衡、领域迁移和可解释性等挑战，我们该如何在数据噪声和偏见中持续提升文本分类的准确性与可靠性？"
  },
  {
    "instruction": "以幽默风趣的方式介绍边缘计算。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "边缘计算",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "想象一下：你点了外卖，但不是把订单先送到外卖总部、总部再开会讨论“这份麻辣烫该不该加香菜”，最后才告诉楼下小哥来送——那你早就饿到啃桌角了。  \n**边缘计算**就是：别折腾了，直接让楼下小哥当场决定，加不加香菜立刻出发！\n\n传统的云计算像个“中央大脑”，什么事都往云里丢；而边缘计算更像“就近处理的聪明邻居”，数据在离你最近的地方就被分析、处理、解决。摄像头不用把每一帧视频都传到天边的服务器，自动驾驶的车也不用一边开一边“等等云那边回消息”。  \n\n一句话总结：**边缘计算就是让设备学会自己动脑子，少打电话，多办事，世界也因此少了点延迟，多了点丝滑。**"
  },
  {
    "instruction": "介绍目标检测，但不要使用'特别'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "特别"
      ]
    ],
    "topic": "目标检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "目标检测是计算机视觉中的重要任务，旨在在图像或视频中同时识别目标的类别并定位其位置。系统通常通过预测边界框及其置信度来完成这一过程。与只进行图像分类不同，目标检测需要在复杂背景、尺度变化和遮挡条件下给出准确的类别判断与空间位置。\n\n常见方法大致分为两类：两阶段方法与单阶段方法。两阶段方法先生成候选区域，再对其进行分类与位置回归，如 R-CNN 系列；单阶段方法则直接在特征图上进行密集预测，如 YOLO 和 SSD，更强调速度与端到端训练。评价目标检测性能时常使用 IoU、mAP 等指标。该技术已广泛应用于自动驾驶、安防监控、医疗影像和工业检测等场景，同时仍需应对小目标识别、实时性和泛化能力等挑战。"
  },
  {
    "instruction": "用恰好150个字概括命名实体识别的核心要点。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "exact_length",
        150
      ]
    ],
    "topic": "命名实体识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "命名实体识别旨在从文本中自动识别并分类具有特定意义的实体如人名地名机构时间等核心要点包括"
  },
  {
    "instruction": "描述机器学习的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "很"
      ]
    ],
    "topic": "机器学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "机器学习的特点包括：\n\n- 以数据为核心，通过样本学习规律并形成模型。  \n- 依赖算法进行参数更新，而非手工编写规则。  \n- 具备从经验中改进性能的能力，随着数据更新而调整结果。  \n- 能处理复杂模式，适用于分类、预测、聚类等任务。  \n- 对数据质量与特征表示存在要求，输入变化会影响输出。  \n- 可应用于多种领域，如图像识别、语音处理、推荐系统。"
  },
  {
    "instruction": "撰写边缘计算的介绍，每段必须包含'神经网络'。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "神经网络",
          3
        ]
      ]
    ],
    "topic": "边缘计算",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "边缘计算是一种将数据处理能力从云端下沉到网络边缘的计算模式，能够在靠近数据源的位置完成分析与决策，从而降低时延、提升隐私与可靠性。在这一过程中，神经网络常被部署在边缘设备上，用于实现实时感知、识别和预测等智能功能。\n\n随着物联网设备和传感器数量的增长，边缘计算可以减少海量数据回传云端的压力，提高系统整体效率。通过在本地运行轻量化的神经网络模型，边缘节点能够快速处理图像、语音和行为数据，满足工业、交通和医疗等场景的实时需求。\n\n在实际应用中，边缘计算通常与云计算协同工作，形成“云—边—端”协作架构。云端负责训练和更新复杂的神经网络模型，而边缘侧则负责模型推理和即时响应，从而在性能、成本与安全性之间取得平衡。"
  },
  {
    "instruction": "撰写模型量化的介绍，每段必须包含'推理'。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "推理",
          2
        ]
      ]
    ],
    "topic": "模型量化",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "模型量化是指将神经网络中的高精度浮点参数与激活值转换为低比特表示（如 INT8），以降低模型存储和计算成本的技术，主要目标是在尽量保持精度的同时加速模型推理。通过减少数值精度，模型在硬件上的访存和算力需求显著下降，从而提升推理速度并降低能耗。\n\n常见的模型量化方法包括训练后量化（PTQ）和量化感知训练（QAT），前者在模型训练完成后直接量化，后者在训练阶段引入量化噪声以提升鲁棒性。这些方法在不同场景下对精度和推理性能有不同权衡，合理选择位宽和量化策略是获得高效推理的关键。\n\n在实际应用中，模型量化广泛用于移动端、嵌入式设备和云端推理服务，能够显著降低延迟与成本。然而，量化也可能带来精度下降、分布漂移等问题，需要通过校准和优化来缓解，以确保推理结果的稳定性和可靠性。"
  },
  {
    "instruction": "写一段关于大语言模型的文字，字数要求在150到300之间。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        150
      ],
      [
        "max_length",
        300
      ]
    ],
    "topic": "大语言模型",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "大语言模型是一类基于深度学习的人工智能系统，通过在海量文本数据上训练，学习语言的统计规律和语义结构。它们能够完成问答、写作、翻译、代码生成等多种任务，在教育、科研、医疗和产业中展现出广泛应用价值。同时，大语言模型也带来了数据偏见、隐私保护和可解释性等挑战，促使人们在技术进步的同时，加强治理与伦理研究。"
  },
  {
    "instruction": "用简洁的技术文档风格说明分布式训练。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "分布式训练",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "## 分布式训练（Distributed Training）\n\n### 概述\n分布式训练是指在多台计算节点或多张加速卡（GPU/TPU）上并行训练同一模型，以提升训练速度、扩展模型规模并提高资源利用率。\n\n### 典型架构\n- **数据并行（Data Parallelism）**：各节点持有完整模型，处理不同数据分片，梯度通过通信聚合（如 AllReduce）。\n- **模型并行（Model Parallelism）**：模型参数按层或张量切分到不同节点，适合超大模型。\n- **流水线并行（Pipeline Parallelism）**：按模型阶段切分，数据以流水线方式流动，提升吞吐。\n\n### 通信与同步\n- **通信原语**：AllReduce、Broadcast、ReduceScatter。\n- **同步方式**：同步更新（强一致性）与异步更新（更高吞吐，可能引入梯度延迟）。\n\n### 一致性与容错\n- **一致性**：参数服务器或去中心化（环形 AllReduce）。\n- **容错**：检查点（Checkpoint）、节点重启与弹性训练。\n\n### 优势与挑战\n- **优势**：加速训练、支持大模型、提升资源利用率。\n- **挑战**：通信开销、负载不均、调参复杂度、数值稳定性。\n\n### 常用框架\n- PyTorch Distributed、TensorFlow Distributed、Horovod、DeepSpeed、Megatron-LM"
  },
  {
    "instruction": "请用列表形式列出语音识别的3个要点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "列表"
      ]
    ],
    "topic": "语音识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. **特征提取**：将语音信号转换为可用于建模的特征（如梅尔频率倒谱系数）。  \n2. **模型建模**：利用声学模型和语言模型将特征映射为文字序列。  \n3. **解码与后处理**：通过解码算法生成最可能的文本，并进行纠错与优化。"
  },
  {
    "instruction": "以问答形式介绍命名实体识别，包含至少4个问题和回答。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "命名实体识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**问1：什么是命名实体识别（NER）？**  \n答：命名实体识别是自然语言处理中的一项基础任务，目的是从文本中自动识别并分类具有特定意义的实体，如人名、地名、组织机构名、时间、金额等。\n\n**问2：命名实体识别有什么作用？**  \n答：NER可以帮助计算机更好地理解文本内容，常用于信息抽取、搜索引擎、问答系统、知识图谱构建以及舆情分析等应用场景。\n\n**问3：命名实体识别通常识别哪些类型的实体？**  \n答：常见的实体类型包括人名（Person）、地点（Location）、组织机构（Organization），此外还可能包括时间（Time）、日期（Date）、货币（Money）、百分比（Percent）等，具体类型取决于应用需求。\n\n**问4：命名实体识别是如何实现的？**  \n答：早期方法主要基于规则和词典，后来发展为基于统计学习的方法，如隐马尔可夫模型和条件随机场；近年来，深度学习方法（如BiLSTM、Transformer、BERT等）已成为主流。\n\n**问5：命名实体识别面临哪些挑战？**  \n答：主要挑战包括实体歧义、新实体不断出现、领域差异明显以及口语化或不规范文本带来的识别困难。"
  },
  {
    "instruction": "在描述图像分割时，请确保'神经网络'这个词出现至少4次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "神经网络",
          4
        ]
      ]
    ],
    "topic": "图像分割",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "图像分割是计算机视觉中的核心任务，常通过**神经网络**来实现。现代方法依赖深度**神经网络**学习像素级特征，通过卷积结构，**神经网络**能够捕捉丰富的空间与上下文信息。在训练过程中，**神经网络**利用大量标注数据优化损失函数，最终由**神经网络**输出精确的分割掩膜，从而实现对图像中不同目标的准确区分。"
  },
  {
    "instruction": "写一段关于联邦学习的文字，必须包含以下关键词：神经网络, 反向传播, 人工智能。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "神经网络",
          2
        ]
      ]
    ],
    "topic": "联邦学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "联邦学习是一种分布式机器学习范式，它允许数据保留在本地设备上，通过聚合模型参数来实现协同训练，从而在保护隐私的同时提升模型性能，在这一过程中，各参与方通常基于神经网络构建本地模型，并利用反向传播算法更新参数，最终由服务器进行安全聚合，这种方法已成为人工智能领域中解决数据孤岛与隐私保护问题的重要技术路线。"
  },
  {
    "instruction": "描述分布式训练的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "很"
      ]
    ],
    "topic": "分布式训练",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "分布式训练具有以下特点：\n\n- 训练任务在多节点或多设备上并行执行。  \n- 数据并行、模型并行或混合并行用于划分计算与存储。  \n- 依赖通信机制完成参数同步与梯度聚合。  \n- 对网络带宽与延迟提出要求。  \n- 具备容错与弹性恢复机制以应对节点故障。  \n- 支持横向扩展以提升吞吐与资源利用率。  \n- 需要一致性策略保证收敛与结果可复现。  \n- 系统配置、任务调度与调试带来工程复杂度。"
  },
  {
    "instruction": "用通俗易懂的语言向初学者解释迁移学习。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "迁移学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "迁移学习可以理解为“把已经学会的本事，用到新的事情上”。\n\n打个通俗的比方：你已经会骑自行车了，再去学骑电动车就会很快，因为很多平衡、转弯的感觉是一样的。你不是从零开始学，而是把以前学到的经验“迁移”过来用。这就是迁移学习的核心思想。\n\n在人工智能里也是这样。比如，一个模型已经通过大量图片学会了识别“边缘、形状、颜色”等基础特征。现在我们想让它学会识别猫和狗，如果从零开始，需要很多数据和时间。但用迁移学习，我们可以直接拿这个“已经很有经验的模型”，稍微再训练一下，它就能很快学会新任务。\n\n迁移学习的好处主要有三点：  \n1. **省时间**：不用从头训练模型。  \n2. **省数据**：即使新任务的数据不多，也能学得不错。  \n3. **效果好**：通常比从零开始训练更准确。\n\n简单总结一句话：**迁移学习就是让模型站在“前人的肩膀上”学习新东西。**"
  },
  {
    "instruction": "用简洁的技术文档风格说明异常检测。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "异常检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**异常检测（Anomaly Detection）**\n\n**概述**  \n异常检测用于识别与正常模式显著偏离的数据点、事件或行为，常用于发现故障、欺诈或安全威胁。\n\n**目标**  \n- 及时发现异常  \n- 降低误报与漏报  \n- 支持实时或离线分析\n\n**常见方法**  \n- 统计方法：阈值、分布假设（Z-score、IQR）  \n- 机器学习：聚类（k-means）、孤立森林、One-Class SVM  \n- 深度学习：自编码器、LSTM、变分自编码器（VAE）\n\n**基本流程**  \n1. 数据采集与预处理  \n2. 特征工程  \n3. 模型训练/规则设定  \n4. 异常评分与判定  \n5. 告警与反馈\n\n**评估指标**  \n- 精确率（Precision）  \n- 召回率（Recall）  \n- F1-score  \n- AUC（如适用）\n\n**应用场景**  \n- 系统与网络监控  \n- 金融风控与反欺诈  \n- 工业设备故障检测  \n- 日志与行为分析"
  },
  {
    "instruction": "描述机器学习，开头必须是'在当今时代'。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "机器学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "在当今时代，机器学习是一种让计算机通过数据自动发现规律并不断改进性能的技术，它不需要为每个任务编写明确规则，而是依靠算法从大量样本中学习模型。通过监督学习、无监督学习和强化学习等方式，机器学习已广泛应用于图像识别、语音处理、推荐系统、医疗诊断和自动驾驶等领域，推动了人工智能从“被动执行”向“主动决策”的转变。"
  },
  {
    "instruction": "用正式的学术语言描述边缘计算。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "边缘计算",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "边缘计算（Edge Computing）是一种分布式计算范式，其核心思想是将数据处理、存储与分析能力从传统的集中式云计算中心下沉至网络边缘，即更接近数据源或终端设备的位置。通过在边缘节点（如网关、基站、路由器或本地服务器）执行计算任务，边缘计算能够有效降低数据传输时延、减少网络带宽消耗，并提升对实时性和可靠性要求较高应用的响应性能。\n\n从体系结构上看，边缘计算通常与云计算形成协同互补关系：云端负责全局视角下的集中管理、复杂分析与长期存储，而边缘侧则侧重于本地化、低时延的数据处理与决策支持。该范式在物联网、工业互联网、自动驾驶、智慧城市及实时智能感知等领域具有重要应用价值。然而，边缘计算在资源受限环境下的任务调度、安全与隐私保护、系统异构性管理等方面仍面临诸多挑战，有待通过体系结构设计、算法优化与标准化机制加以解决。"
  },
  {
    "instruction": "介绍知识蒸馏，但不要使用'很'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "很"
      ]
    ],
    "topic": "知识蒸馏",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "知识蒸馏是一种模型压缩与迁移方法。其核心思想是利用性能较强、规模较大的教师模型，将其学到的知识传递给结构更轻量的学生模型。训练过程中，学生模型不仅学习真实标签，还学习教师模型输出的软标签，这些软标签通过引入温度参数，包含类别之间的相对关系信息。\n\n通过知识蒸馏，学生模型在参数量与计算开销受限的情况下，仍能获得接近教师模型的性能。这一方法常用于模型部署、边缘设备应用以及提升推理效率，同时也有助于缓解过拟合，增强模型泛化能力。在实践中，蒸馏形式包括基于输出分布的蒸馏、基于中间特征的蒸馏，以及结合多任务或自监督信号的扩展方案。"
  },
  {
    "instruction": "写一段关于推荐系统的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "非常"
      ]
    ],
    "topic": "推荐系统",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "推荐系统通过收集用户行为数据，进行数据处理和模型训练，为用户提供内容推荐。系统在平台中运行，涉及排序、过滤和反馈机制，用于信息分发和决策支持。开发过程中需要考虑数据来源、算法选择和结果评估。"
  },
  {
    "instruction": "描述情感分析的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "特别"
      ]
    ],
    "topic": "情感分析",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "情感分析的特点包括：  \n- 以文本、语音或图像等数据为对象，识别情绪取向与态度类别。  \n- 结合自然语言处理与机器学习方法，实现情绪标签的自动判定。  \n- 能处理主观性内容，提取观点、立场与情感极性。  \n- 支持多语言与跨领域应用，适用于评论、社交媒体与客服记录等场景。  \n- 可用于趋势监测与决策支持，为产品改进与舆情管理提供依据。"
  },
  {
    "instruction": "写一段关于Transformer架构的文字，必须包含以下关键词：算法, 深度学习, 推理。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "优化",
          4
        ]
      ]
    ],
    "topic": "Transformer架构",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "Transformer架构是现代深度学习中的重要模型，它以自注意力机制为核心算法，能够在不依赖循环结构的情况下高效建模序列数据，通过并行计算显著提升训练与推理效率；在自然语言处理等任务中，Transformer通过多头注意力和前馈网络捕捉全局依赖关系，使模型在表示学习和语义理解上表现出色，推动了从预训练到下游任务应用的广泛发展。"
  },
  {
    "instruction": "请用不少于200个字描述计算机视觉。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        200
      ]
    ],
    "topic": "计算机视觉",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "计算机视觉是人工智能领域中的重要分支，旨在让计算机具备类似人类视觉的感知与理解能力。它通过对图像和视频等视觉信息的获取、处理与分析，使机器能够识别物体、理解场景并作出决策。计算机视觉通常结合数学模型、统计方法与机器学习技术，对像素数据进行特征提取与语义建模，从而实现从低层信号到高层认知的转化。常见研究任务包括图像分类、目标检测、语义分割、人脸识别以及行为理解等。这些技术已经广泛应用于自动驾驶、医学影像分析、工业检测、安防监控和智能终端等领域。随着计算能力的提升与数据规模的扩大，计算机视觉在准确性与实时性方面不断进步。同时，该领域也面临光照变化、遮挡干扰、数据偏差与隐私保护等挑战。未来，计算机视觉将更加注重多模态融合、通用理解能力以及与人类社会的安全协同发展，从而在生产与生活中发挥更深远的价值。"
  },
  {
    "instruction": "用三段式结构（引言、正文、结论）介绍图像分割。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "图像分割",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**引言**  \n图像分割是计算机视觉中的基础任务，旨在将一幅图像划分为若干具有语义或视觉一致性的区域。通过分割，计算机能够更好地理解图像内容，为后续的识别、检测与分析提供结构化信息。\n\n**正文**  \n常见的图像分割方法包括基于阈值、边缘、区域以及深度学习的方法。传统方法依赖像素的灰度、颜色或纹理等低层特征，适用于场景简单、对实时性要求高的任务；而以卷积神经网络为代表的深度学习方法，如FCN、U-Net和Mask R-CNN，则通过端到端学习获得更强的语义表达能力，在医学影像、自动驾驶等复杂场景中表现突出。\n\n**结论**  \n总体而言，图像分割在视觉理解中扮演着关键角色，其方法从规则驱动逐步演进到数据驱动。随着计算能力和数据规模的提升，图像分割技术将持续发展，并在更多实际应用中发挥重要价值。"
  },
  {
    "instruction": "请用JSON格式输出注意力机制的关键信息。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "注意力机制",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "```json\n{\n  \"名称\": \"注意力机制\",\n  \"定义\": \"注意力机制是一种在模型中动态分配权重的方法，用于突出与当前任务最相关的信息，降低无关信息的影响。\",\n  \"核心思想\": \"通过计算相关性得分，为不同输入分配不同的重要性权重。\",\n  \"关键组成\": {\n    \"查询(Query)\": \"表示当前需要关注的信息需求\",\n    \"键(Key)\": \"用于与查询进行匹配的特征表示\",\n    \"值(Value)\": \"在匹配后被加权汇总的信息内容\"\n  },\n  \"基本计算流程\": [\n    \"计算查询与键之间的相似度得分\",\n    \"对得分进行归一化（如Softmax）\",\n    \"使用归一化权重对值进行加权求和\"\n  ],\n  \"常见类型\": [\n    \"加性注意力（Additive Attention）\",\n    \"点积注意力（Dot-Product Attention）\",\n    \"缩放点积注意力（Scaled Dot-Product Attention）\",\n    \"自注意力（Self-Attention）\",\n    \"多头注意力（Multi-Head Attention）\"\n  ],\n  \"优点\": [\n    \"提高模型对关键信息的建模能力\",\n    \"增强长距离依赖的捕获能力\",\n    \"提升模型的可解释性\"\n  ],\n  \"典型应用\": [\n    \"机器翻译\",\n    \"文本摘要\",\n    \"语音识别\",\n    \"计算机视觉\",\n    \"大语言模型\"\n  ]\n}\n```"
  },
  {
    "instruction": "请用JSON格式输出命名实体识别的关键信息。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "命名实体识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "```json\n{\n  \"text\": \"<待识别的文本>\",\n  \"entities\": [\n    {\n      \"text\": \"<实体文本>\",\n      \"type\": \"<实体类型，如PERSON/ORG/LOC/DATE等>\",\n      \"start\": 0,\n      \"end\": 4,\n      \"confidence\": 0.95\n    }\n  ],\n  \"language\": \"zh\",\n  \"model\": \"<使用的NER模型名称>\",\n  \"timestamp\": \"<生成时间ISO-8601>\"\n}\n```"
  },
  {
    "instruction": "用简洁的技术文档风格说明异常检测。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "异常检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**异常检测（Anomaly Detection）**\n\n**定义**  \n识别与大多数数据模式显著不同的样本或行为的技术。\n\n**目标**  \n- 发现潜在故障、欺诈或安全威胁  \n- 提前预警，降低风险与损失\n\n**常见方法**  \n- **统计方法**：阈值、分布假设（如 Z-score）  \n- **机器学习**：Isolation Forest、One-Class SVM  \n- **深度学习**：自编码器（Autoencoder）、LSTM  \n- **规则/经验法**：业务规则、专家系统\n\n**流程**  \n数据采集 → 预处理 → 特征工程 → 模型训练/规则配置 → 检测与告警 → 评估与迭代\n\n**评估指标**  \nPrecision、Recall、F1、ROC-AUC、误报率\n\n**应用场景**  \n运维监控、金融风控、网络安全、制造质检、医疗监测"
  },
  {
    "instruction": "写一篇关于情感分析的短文，必须以问句结尾。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "情感分析",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "情感分析是自然语言处理中的重要任务，旨在从文本中识别和理解人的情绪与态度。它被广泛应用于舆情监测、产品评价分析和客户服务中，通过算法将主观表达转化为可量化的信息。随着深度学习和大模型的发展，情感分析在语境理解、多情绪识别和跨语言迁移方面不断进步，但也面临讽刺、隐喻和偏见等挑战，那么我们该如何在保证准确性的同时实现更公平、可解释的情感分析呢？"
  },
  {
    "instruction": "以问答形式介绍计算机视觉，包含至少3个问题和回答。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "计算机视觉",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**问1：什么是计算机视觉？**  \n**答：**计算机视觉是人工智能的一个重要分支，研究如何让计算机从图像或视频中获取、理解和分析信息，从而实现类似人类“看”和“理解”的能力。\n\n**问2：计算机视觉主要能做哪些事情？**  \n**答：**计算机视觉可以完成图像分类、目标检测、人脸识别、图像分割、姿态识别等任务，广泛应用于安防监控、自动驾驶、医疗影像分析等领域。\n\n**问3：计算机视觉的核心技术有哪些？**  \n**答：**核心技术包括图像处理、特征提取、机器学习以及深度学习，其中卷积神经网络（CNN）在现代计算机视觉中起着关键作用。\n\n**问4：计算机视觉在日常生活中有哪些应用？**  \n**答：**日常生活中的应用包括手机人脸解锁、相册中的自动人物分类、智能摄像头、自动驾驶辅助系统以及电商平台的以图搜图功能。"
  },
  {
    "instruction": "描述边缘计算的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "非常"
      ]
    ],
    "topic": "边缘计算",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "边缘计算的主要特点包括：\n\n- **靠近数据源部署**：计算与存储资源部署在数据产生位置附近，减少数据传输路径。  \n- **降低时延**：在本地完成数据处理，减少往返云端的通信过程。  \n- **带宽占用减少**：只将必要结果上传至中心节点，缓解网络传输压力。  \n- **数据隐私保护**：数据在本地处理，降低外部传输带来的泄露风险。  \n- **支持分布式架构**：多个边缘节点协同工作，适应多场景应用需求。  \n- **提升系统可靠性**：在网络不稳定情况下，边缘节点仍可独立运行。"
  },
  {
    "instruction": "撰写文本分类的介绍，每段必须包含'损失函数'。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "损失函数",
          2
        ]
      ]
    ],
    "topic": "文本分类",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "文本分类是自然语言处理中的一项基础任务，旨在将文本自动分配到预定义的类别中，常见应用包括情感分析、垃圾邮件识别和主题标注等。在模型训练过程中，特征表示、模型结构与损失函数共同决定了分类效果，其中损失函数用于度量模型预测结果与真实标签之间的差异。\n\n在实际应用中，文本分类模型可以采用传统机器学习方法或深度学习方法，如朴素贝叶斯、支持向量机以及基于神经网络的模型。无论采用哪种方法，合理选择和设计损失函数都至关重要，因为损失函数直接影响模型的优化方向和最终性能表现。"
  },
  {
    "instruction": "在描述边缘计算时，请确保'神经网络'这个词出现至少2次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "神经网络",
          2
        ]
      ]
    ],
    "topic": "边缘计算",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "边缘计算是一种将计算与数据处理能力部署在靠近数据源头的技术架构，它能够降低延迟、节省带宽并提升实时性。在这一模式下，设备可以直接在本地运行神经网络，对传感器数据进行快速分析和决策，从而减少对云端的依赖。随着硬件性能的提升，越来越多的边缘设备开始支持复杂的神经网络模型，使边缘计算在智能制造、自动驾驶和物联网等场景中发挥更大价值。"
  },
  {
    "instruction": "撰写生成对抗网络的介绍，每段必须包含'推理'。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "推理",
          3
        ]
      ]
    ],
    "topic": "生成对抗网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "生成对抗网络（Generative Adversarial Network，简称GAN）是一种由生成器和判别器组成的深度学习模型，通过对抗训练来学习数据分布。生成器负责从随机噪声中生成逼真的样本，而判别器则尝试区分真实数据与生成数据，二者在博弈过程中不断优化，这一过程本质上涉及复杂的概率建模与推理，从而提升生成结果的真实性。\n\n在应用层面，生成对抗网络被广泛用于图像生成、图像修复、风格迁移等领域，其核心优势在于能够通过对抗机制进行高质量数据建模。随着研究的深入，学者们不断改进网络结构与训练策略，使模型在稳定性和可控性方面取得进展，而这些改进同样依赖对模型行为的深入推理与理解。"
  },
  {
    "instruction": "请用JSON格式输出注意力机制的关键信息。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "注意力机制",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "{\n  \"名称\": \"注意力机制\",\n  \"定义\": \"一种在模型中为不同输入分配不同权重，以突出关键信息的机制。\",\n  \"核心思想\": \"根据当前任务动态关注最相关的输入部分，忽略不重要的信息。\",\n  \"基本组成\": {\n    \"查询(Query)\": \"表示当前需要关注什么\",\n    \"键(Key)\": \"表示可被匹配的信息特征\",\n    \"值(Value)\": \"与键对应的实际信息内容\"\n  },\n  \"计算过程\": [\n    \"计算查询与键的相似度\",\n    \"对相似度进行归一化（如Softmax）\",\n    \"使用权重对值进行加权求和\"\n  ],\n  \"常见类型\": [\n    \"软注意力(Soft Attention)\",\n    \"硬注意力(Hard Attention)\",\n    \"自注意力(Self-Attention)\",\n    \"多头注意力(Multi-Head Attention)\"\n  ],\n  \"常用公式\": \"Attention(Q,K,V) = softmax(QK^T / sqrt(d_k))V\",\n  \"优点\": [\n    \"提升模型对关键信息的建模能力\",\n    \"增强长距离依赖建模效果\",\n    \"提高模型可解释性\"\n  ],\n  \"典型应用\": [\n    \"机器翻译\",\n    \"文本摘要\",\n    \"语音识别\",\n    \"计算机视觉\",\n    \"大语言模型\"\n  ]\n}"
  },
  {
    "instruction": "用恰好100个字概括机器学习的核心要点。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "exact_length",
        100
      ]
    ],
    "topic": "机器学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "机器学习以数据为核心通过算法从样本中自动发现模式并进行预测与决策强调特征表示模型选择训练评估泛化能力避免过拟合迭代优化并在应用中持续改进确保数据质量与伦理安全提升可靠性与可解释性并服务实际问题持续演进"
  },
  {
    "instruction": "在描述自监督学习时，请确保'损失函数'这个词出现至少4次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "损失函数",
          4
        ]
      ]
    ],
    "topic": "自监督学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "自监督学习是一种不依赖人工标注的数据学习范式，通过构造预文本任务从数据自身生成监督信号。在这一过程中，模型通过最小化损失函数来学习有意义的表示，损失函数用于度量预测与目标之间的差异。不同任务会设计不同的损失函数，例如对比学习中的损失函数强调正负样本间的相对距离。训练时，优化器围绕损失函数进行参数更新，而评估表示质量也常间接依赖损失函数的收敛情况。总体而言，合理设计损失函数是自监督学习成功的关键，因为损失函数直接影响表示的稳定性与泛化能力。"
  },
  {
    "instruction": "以幽默风趣的方式介绍机器翻译。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "机器翻译",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "想象一下，机器翻译就像一位**不睡觉、不喝咖啡、但偶尔会“翻车”的超级语言学霸**。你对它说一句中文：“我有点饿”，它立刻转身用英文告诉世界：“I’m a little hungry。”速度快到连外卖软件都自愧不如。\n\n机器翻译的工作原理也很有趣：它不是逐字查字典，而是通过大量学习人类说过的“废话、情话和官方套话”，总结出语言之间的规律。所以它有时翻得优雅流畅，有时却会一本正经地胡说八道，比如把“加油”翻成“add oil”，让外国朋友一脸困惑地寻找厨房。\n\n总的来说，机器翻译就像一个正在努力成长的语言实习生——效率极高、态度认真，但关键时刻你还是得看一眼，确认它没有把“祝你一路顺风”送成“wish you get blown away”。"
  },
  {
    "instruction": "介绍图像分割，但不要使用'特别'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "特别"
      ]
    ],
    "topic": "图像分割",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "图像分割是计算机视觉中的一项基础任务，目标是将一幅图像划分为若干具有一致性特征的区域或对象，使后续分析更加清晰和高效。通过图像分割，可以把感兴趣的目标从背景中分离出来，为目标识别、测量和理解提供结构化的信息基础。\n\n常见的图像分割方法包括基于阈值的分割、边缘检测、区域生长、聚类方法以及近年来广泛应用的深度学习方法，如卷积神经网络和全卷积网络。图像分割在医学影像分析、自动驾驶、遥感影像处理和工业检测等领域具有重要作用，能够显著提升系统对复杂场景的理解和决策能力。"
  },
  {
    "instruction": "撰写自然语言处理的介绍，每段必须包含'参数'。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "参数",
          4
        ]
      ]
    ],
    "topic": "自然语言处理",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "自然语言处理（NLP）是人工智能的重要分支，致力于让计算机理解、生成和处理人类语言。通过统计方法与深度学习模型，系统可以从大量文本中学习语言规律，而模型的性能往往取决于结构设计以及关键参数的选择与优化。\n\n在技术层面，自然语言处理涵盖分词、句法分析、语义理解和文本生成等任务，常用方法包括词向量、循环神经网络和预训练大模型。训练过程中，需要通过调整学习率、层数等参数，使模型在准确性与泛化能力之间取得平衡。\n\n在应用方面，自然语言处理已广泛用于机器翻译、智能客服、情感分析和信息检索等场景。实际部署时，工程师会根据业务需求反复调试模型参数，并结合评估指标不断改进系统效果。"
  },
  {
    "instruction": "写一段关于自然语言处理的文字，必须包含以下关键词：人工智能, 深度学习, 优化。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "损失函数",
          3
        ]
      ]
    ],
    "topic": "自然语言处理",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "自然语言处理是人工智能领域的重要分支，旨在让计算机理解和生成自然语言，近年来随着深度学习技术的发展取得了显著进展，通过大规模语料训练的模型在语义理解、机器翻译和对话系统中表现出色，而在实际应用中还需要不断进行算法优化与模型优化，以提升效率、准确性和可扩展性。"
  },
  {
    "instruction": "介绍自监督学习，但不要使用'极其'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "极其"
      ]
    ],
    "topic": "自监督学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "自监督学习是一种介于监督学习与无监督学习之间的机器学习范式。它的核心思想是：不依赖人工标注的数据，而是从数据本身构造监督信号。通过设计预任务（pretext task），模型可以在大量未标注数据上学习有用的表示，再将这些表示迁移到下游任务中，如分类、检测或生成任务。\n\n在实践中，自监督学习常见的方法包括对比学习、掩码预测和时序预测等。例如，在对比学习中，模型通过拉近同一样本不同视角的表示、拉远不同样本的表示来学习特征；在自然语言处理中，掩码语言模型通过预测被遮挡的词来理解上下文语义。由于减少了对人工标注的依赖，自监督学习在数据获取成本高或标注困难的场景中具有重要价值，并已成为深度学习研究中的重要方向。"
  },
  {
    "instruction": "用Markdown格式撰写关于机器学习的说明，必须包含标题和分点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "机器学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "# 机器学习简介\n\n## 什么是机器学习\n- 机器学习（Machine Learning）是人工智能的一个分支  \n- 通过数据训练模型，使计算机具备从经验中改进性能的能力  \n- 无需为每个任务显式编程即可完成预测或决策\n\n## 主要类型\n- **监督学习**：使用带标签的数据进行训练（如分类、回归）\n- **无监督学习**：从无标签数据中发现结构（如聚类、降维）\n- **强化学习**：通过与环境交互并根据奖励学习策略\n\n## 典型应用\n- 图像与语音识别  \n- 推荐系统（电商、视频平台）  \n- 金融风控与预测分析  \n- 自动驾驶与智能机器人\n\n## 基本流程\n- 数据收集与预处理  \n- 特征工程  \n- 模型选择与训练  \n- 模型评估与部署"
  },
  {
    "instruction": "写一段关于神经网络的文字，必须包含以下关键词：深度学习, 损失函数, 优化。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "反向传播",
          2
        ]
      ]
    ],
    "topic": "神经网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "神经网络是一种受人脑启发的计算模型，通过多层结构对数据进行表示和学习，在深度学习中尤为常见。训练过程中，模型会根据预测结果与真实标签之间的差异计算损失函数，用以衡量当前模型的性能。随后，利用梯度下降等优化方法不断调整参数，使损失函数逐步减小，从而提升神经网络在实际任务中的表现。"
  },
  {
    "instruction": "以幽默风趣的方式介绍情感分析。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "情感分析",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "情感分析，简单说，就是教电脑“读空气”。你发一句话，它不光看字面意思，还要猜你的心情：是开心到起飞，还是气到想摔键盘。比如你说“真是太棒了”，人类秒懂你是在真心点赞还是阴阳怪气，但电脑得靠情感分析这门“读心术”来判断——不然一不小心就把吐槽当表扬了。\n\n在情感分析眼里，文字就像一锅火锅：有“开心”“喜欢”这种小肥牛，也有“失望”“生气”这种魔鬼辣椒。算法要做的事，就是数一数锅里哪种料更多，再严肃地宣布：“这条评论，情绪偏辣，建议谨慎入口。”\n\n所以，下次你看到APP说“检测到你情绪低落，是否需要安慰？”别慌，那不是它成精了，而是情感分析正在努力当一个情商还在学习阶段的朋友——虽然偶尔会翻车，但至少很努力。"
  },
  {
    "instruction": "用Markdown格式撰写关于边缘计算的说明，必须包含标题和分点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "边缘计算",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "# 边缘计算说明\n\n**边缘计算（Edge Computing）** 是一种将计算、存储和数据处理能力从云端下沉到网络边缘的计算范式，靠近数据产生源头进行处理。\n\n- **核心概念**：在靠近终端设备（如传感器、摄像头、IoT设备）的位置处理数据，减少对中心云的依赖  \n- **主要优势**：  \n  - 降低延迟，提高实时响应能力  \n  - 减少带宽占用，降低网络传输成本  \n  - 提升数据隐私与安全性  \n- **典型应用场景**：  \n  - 工业互联网与智能制造  \n  - 智慧城市与智能交通  \n  - 自动驾驶与视频监控  \n- **与云计算的关系**：  \n  - 边缘计算与云计算互补  \n  - 云端负责集中分析与长期存储，边缘侧负责实时处理与快速决策"
  },
  {
    "instruction": "撰写推荐系统的介绍，每段必须包含'参数'。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "参数",
          2
        ]
      ]
    ],
    "topic": "推荐系统",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "推荐系统是一类用于根据用户行为和偏好提供个性化内容的技术，广泛应用于电商、视频与新闻平台。其核心思想是通过分析历史数据，学习用户与物品之间的关系，而这些关系通常由模型中的各种参数来刻画，例如用户向量和物品向量等关键参数。\n\n从技术路线来看，推荐系统可以分为协同过滤、基于内容的方法以及融合多种信息的混合模型。协同过滤依赖用户行为矩阵，基于内容的方法依赖特征表示，而现代系统多采用深度学习，通过大量可训练参数来提升表达能力，同时需要合理设置和优化超参数以避免过拟合。\n\n在实际应用中，推荐系统的效果评估与持续优化同样重要。工程师会通过A/B测试和离线指标不断调整模型参数，使系统在准确性、多样性和实时性之间取得平衡，因此对参数的理解和管理贯穿了推荐系统从设计到部署的全过程。"
  },
  {
    "instruction": "写一段关于分布式训练的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "非常"
      ]
    ],
    "topic": "分布式训练",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "分布式训练在机器学习中使用两台或三台设备参与模型训练。系统将数据划分到节点，节点执行计算并产生梯度。节点通过网络发送结果，服务器接收并更新参数。流程在训练周期内反复进行，直到达到停止条件。"
  },
  {
    "instruction": "写一段关于自然语言处理的文字，字数要求在150到350之间。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        150
      ],
      [
        "max_length",
        350
      ]
    ],
    "topic": "自然语言处理",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "自然语言处理是人工智能的重要分支，旨在让计算机理解、生成人类语言。它综合了语言学、统计学和深度学习方法，通过分词、句法分析、语义表示等技术，将复杂文本转化为可计算的结构。在搜索引擎、智能客服、机器翻译和内容生成等场景中，自然语言处理显著提升了信息获取效率。同时，如何处理歧义、低资源语言和模型偏见，仍是该领域持续探索的方向。"
  },
  {
    "instruction": "用三段式结构（引言、正文、结论）介绍RAG检索增强。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "RAG检索增强",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**引言**  \nRAG（Retrieval-Augmented Generation，检索增强生成）是一种将信息检索与大语言模型生成能力相结合的技术，旨在缓解模型“幻觉”、知识陈旧和领域覆盖不足等问题。通过在生成回答前引入外部知识，RAG让模型不再只依赖参数记忆，而是基于可追溯、可更新的数据进行回答。\n\n**正文**  \nRAG的核心流程通常包括三步：首先将用户问题转化为向量表示，并在向量数据库或搜索引擎中检索相关文档；其次对检索结果进行筛选、排序或压缩，形成高质量的上下文；最后将这些上下文与原始问题一并输入大语言模型生成回答。该机制使模型能够结合实时数据、企业私有知识或专业文档，显著提升准确性、可解释性和可靠性，常用于问答系统、知识助手和企业搜索等场景。\n\n**结论**  \n总体而言，RAG检索增强通过“先查再答”的方式，弥补了大语言模型在知识更新和事实一致性方面的不足。随着向量检索、文档理解和上下文优化技术的发展，RAG正在成为构建可信、可落地大模型应用的重要基础架构。"
  },
  {
    "instruction": "请用列表形式列出提示工程的6个要点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "列表"
      ]
    ],
    "topic": "提示工程",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. 明确目标：清楚说明你希望模型完成的任务和期望输出。  \n2. 提供上下文：给出必要背景信息，帮助模型理解问题情境。  \n3. 设定角色：为模型指定角色或身份，以引导回答风格和视角。  \n4. 约束条件：明确格式、长度、语言、禁止或必须包含的内容。  \n5. 给出示例：通过示例（Few-shot）展示理想的输入与输出。  \n6. 迭代优化：根据输出结果不断调整和细化提示。"
  },
  {
    "instruction": "写一段关于边缘计算的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "非常"
      ]
    ],
    "topic": "边缘计算",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "边缘计算是一种计算模式。数据在设备节点处理，任务在网络节点执行。系统减少数据传输，支持业务运行。应用包括工业、交通、医疗。平台提供管理、控制、分析功能。"
  },
  {
    "instruction": "用三段式结构（引言、正文、结论）介绍自监督学习。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "自监督学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**引言**  \n自监督学习是一种介于监督学习与无监督学习之间的机器学习范式，旨在利用数据本身蕴含的结构自动生成监督信号，从而减少对人工标注的依赖。随着数据规模的爆炸式增长和标注成本的不断上升，自监督学习逐渐成为深度学习领域的重要研究方向。\n\n**正文**  \n在自监督学习中，模型通过预设的“预任务”从原始数据中构造标签，例如在计算机视觉中预测图像的缺失区域、旋转角度，在自然语言处理中预测被遮蔽的词或下一句内容。通过完成这些任务，模型能够学习到对下游任务有价值的通用表示。随后，这些表示可以通过少量标注数据进行微调，显著提升分类、检测或生成等任务的性能。\n\n**结论**  \n总体而言，自监督学习为缓解数据标注瓶颈、提升模型泛化能力提供了有效途径。随着算法设计和计算资源的不断进步，自监督学习有望在更多领域发挥核心作用，成为构建通用人工智能系统的重要基础。"
  },
  {
    "instruction": "写一段关于目标检测的文字，必须包含以下关键词：训练, 模型, 推理。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "优化",
          4
        ]
      ]
    ],
    "topic": "目标检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "目标检测是计算机视觉中的重要任务，其核心流程包括数据准备与训练、模型设计与优化以及实际应用中的推理阶段：在训练过程中，利用大量标注数据让模型学习目标的特征与位置；经过不断迭代后，模型能够在复杂场景中准确识别目标；最终在推理时，模型对新输入的图像进行快速分析，输出目标类别和边界框，实现高效而稳定的检测效果。"
  },
  {
    "instruction": "介绍提示工程，但不要使用'特别'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "特别"
      ]
    ],
    "topic": "提示工程",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "提示工程（Prompt Engineering）是指在与大语言模型或生成式人工智能交互时，设计、优化和组织输入提示语的方法与技巧。通过精心编写提示，可以更清晰地表达目标、约束条件和期望输出形式，从而引导模型生成更准确、更有用的结果。它不仅关注“问什么”，也关注“怎么问”，包括用词选择、上下文提供、角色设定以及输出格式的说明等。\n\n在实际应用中，提示工程常用于提升问答质量、改进文本生成效果、提高代码生成的可用性，以及增强复杂任务的可控性。随着模型能力的提升，提示工程逐渐成为连接人类意图与模型能力的重要桥梁，对开发者、产品经理和普通用户都具有很高的实用价值。"
  },
  {
    "instruction": "请用列表形式列出分布式训练的4个要点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "列表"
      ]
    ],
    "topic": "分布式训练",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. **并行策略**：选择数据并行、模型并行或混合并行以适配模型规模与硬件资源。  \n2. **通信与同步**：高效的梯度通信（如 All-Reduce）与同步机制以降低通信开销。  \n3. **性能与扩展性**：优化计算与通信重叠，确保随节点增加仍能线性或近线性加速。  \n4. **容错与一致性**：支持故障恢复、检查点与参数一致性，保证训练稳定可靠。"
  },
  {
    "instruction": "描述语音识别，开头必须是'在当今时代'。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "语音识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "在当今时代，语音识别是一项将人类语音信号自动转换为可处理文本或指令的人工智能技术。它依托于声学建模、语言建模和深度学习算法，对语音中的音素、语调和语境进行分析，从而实现对不同说话人、口音和环境噪声的适应。语音识别已广泛应用于智能助手、语音输入、客服系统和无障碍技术等领域，不仅提升了人机交互的自然性和效率，也在不断推动信息获取与服务方式的变革。"
  },
  {
    "instruction": "写一段关于语音识别的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "很"
      ]
    ],
    "topic": "语音识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "语音识别是一种将语音转化为文字的技术。系统接收音频数据，进行分帧、特征提取、模型推断和结果输出。模型训练依赖语料、参数和算法，运行时根据输入产生序列。应用场景包括输入法、客服、会议记录和设备控制。该技术涉及信号处理、统计方法和计算平台。"
  },
  {
    "instruction": "介绍生成对抗网络，但不要使用'很'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "很"
      ]
    ],
    "topic": "生成对抗网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "生成对抗网络（GAN）是一类用于数据生成的深度学习模型，由生成器和判别器两个网络组成。生成器负责从随机噪声中合成样本，判别器负责区分真实数据与生成数据。二者在对抗博弈中交替训练：生成器努力提升以假乱真的能力，判别器不断增强辨别水平，最终促使生成分布逼近真实分布。\n\nGAN 在图像合成、风格迁移、数据增强、超分辨率等任务中表现突出，也被应用于音频与文本领域。该模型在训练过程中可能出现不稳定、模式崩溃等现象，因此研究者提出了多种改进方法，如 Wasserstein GAN、条件 GAN 与正则化策略，用于改善收敛性与生成质量。"
  },
  {
    "instruction": "描述深度学习，开头必须是'在当今时代'。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "深度学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "在当今时代，深度学习作为人工智能的重要分支，依托多层神经网络对海量数据进行特征自动学习与表示，已在计算机视觉、自然语言处理、语音识别和科学计算等领域取得突破性进展。它通过端到端训练减少人工特征工程的依赖，借助大数据、强算力与优化算法不断提升性能，但同时也面临可解释性不足、数据与算力消耗高等挑战。"
  },
  {
    "instruction": "在描述自然语言处理时，请确保'模型'这个词出现至少4次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "模型",
          4
        ]
      ]
    ],
    "topic": "自然语言处理",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "自然语言处理是人工智能的重要领域，核心在于通过模型理解和生成人类语言。早期方法依赖规则，而现代方法主要依靠数据驱动的模型进行训练。随着深度学习的发展，神经网络模型在语义理解、机器翻译和对话系统中表现突出。不同任务需要不同结构的模型，而大规模预训练模型进一步提升了泛化能力。评估和部署这些模型时，还需要考虑效率与安全。"
  },
  {
    "instruction": "用正式的学术语言描述推荐系统。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "推荐系统",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "推荐系统（Recommender Systems）是信息检索与数据挖掘领域中的重要研究方向，旨在通过分析用户行为数据、内容特征及其交互关系，对用户潜在兴趣与偏好进行建模，从而在信息过载的环境中为用户提供个性化的信息或服务推荐。该系统通常以用户—项目交互数据为基础，综合运用统计学习、机器学习及优化理论等方法，构建用户模型与项目模型，并在此基础上进行预测与排序。根据建模假设与数据来源的不同，推荐系统可分为基于内容的方法、协同过滤方法以及融合多种信息源的混合方法。\n\n在实际应用中，推荐系统不仅关注预测精度，还需综合考虑多样性、新颖性、公平性与可解释性等指标，以提升用户体验与系统效用。同时，随着大规模数据与复杂场景的出现，推荐系统面临冷启动、数据稀疏性、隐私保护及动态偏好建模等挑战。近年来，深度学习与因果推断等先进方法的引入，进一步推动了推荐系统在理论建模与实际应用层面的发展，使其成为支撑电子商务、社交媒体与数字内容平台的重要技术基础。"
  },
  {
    "instruction": "请写5段关于生成对抗网络的内容，每段以数字编号开头。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "生成对抗网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. 生成对抗网络（GAN）是一类由两个神经网络组成的深度学习模型，分别是生成器和判别器，它们通过对抗式训练共同提升性能，从而学习数据的真实分布。  \n2. 在GAN的训练过程中，生成器负责生成尽可能逼真的样本，而判别器则尝试区分真实数据与生成数据，二者形成类似博弈的关系。  \n3. 通过不断迭代，生成器逐渐学会捕捉真实数据的统计特征，而判别器的判别能力也随之增强，最终达到一种动态平衡状态。  \n4. 生成对抗网络在图像生成、图像修复、风格迁移等领域表现突出，例如可以生成高分辨率人脸图像或将草图转化为逼真的照片。  \n5. 尽管GAN具有强大的生成能力，但其训练过程不稳定、容易出现模式崩溃等问题，因此研究者提出了多种改进模型来提升稳定性和效果。"
  },
  {
    "instruction": "请用JSON格式输出神经网络的关键信息。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "神经网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "```json\n{\n  \"定义\": \"神经网络是一种受生物神经系统启发的机器学习模型，用于从数据中学习复杂的非线性关系。\",\n  \"基本结构\": {\n    \"输入层\": \"接收原始数据特征\",\n    \"隐藏层\": \"对特征进行非线性变换和抽象表示\",\n    \"输出层\": \"产生最终预测或分类结果\",\n    \"神经元\": \"包含加权求和、偏置和激活函数的计算单元\"\n  },\n  \"常见类型\": [\n    \"前馈神经网络\",\n    \"卷积神经网络（CNN）\",\n    \"循环神经网络（RNN）\",\n    \"长短期记忆网络（LSTM）\",\n    \"Transformer\"\n  ],\n  \"训练过程\": {\n    \"损失函数\": \"衡量模型预测与真实值之间的差距\",\n    \"反向传播\": \"计算梯度并更新参数的方法\",\n    \"优化算法\": [\n      \"随机梯度下降（SGD）\",\n      \"Adam\",\n      \"RMSProp\"\n    ]\n  },\n  \"应用领域\": [\n    \"计算机视觉\",\n    \"自然语言处理\",\n    \"语音识别\",\n    \"推荐系统\",\n    \"自动驾驶\"\n  ],\n  \"优缺点\": {\n    \"优点\": [\n      \"表达能力强\",\n      \"可自动学习特征\",\n      \"适合处理大规模数据\"\n    ],\n    \"缺点\": [\n      \"训练数据需求大\",\n      \"计算资源消耗高\",\n      \"可解释性较差\"\n    ]\n  }\n}\n```"
  },
  {
    "instruction": "写一段关于机器翻译的文字，字数要求在150到350之间。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        150
      ],
      [
        "max_length",
        350
      ]
    ],
    "topic": "机器翻译",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "机器翻译是人工智能在语言领域的重要应用，它通过统计模型、规则方法以及近年来迅速发展的神经网络，实现不同语言之间的自动转换。随着大规模语料和算力的提升，现代机器翻译在流畅度和准确性上取得了显著进步，能够支持跨文化交流、国际贸易和信息获取。然而，它仍然会在专业术语、文化隐喻和语境理解上出现偏差，需要与人工校对结合使用。未来，机器翻译将朝着更高质量、更强语义理解和更个性化的方向发展。"
  },
  {
    "instruction": "写一段关于深度学习的文字，必须包含以下关键词：损失函数, 梯度下降, 人工智能。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "模型",
          3
        ]
      ]
    ],
    "topic": "深度学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "深度学习是人工智能领域中的重要分支，它通过多层神经网络从大量数据中自动学习特征和规律，在训练过程中模型会根据预测结果与真实标签之间的差异来计算损失函数，并利用梯度下降等优化方法不断调整参数，从而逐步提升模型性能，使计算机在视觉、语音和自然语言处理等任务上表现出接近甚至超越人类的能力。"
  },
  {
    "instruction": "描述异常检测的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "非常"
      ]
    ],
    "topic": "异常检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "异常检测具有以下特点：\n\n- 目标是识别与正常模式不一致的数据点或行为。  \n- 异常样本数量少，数据分布存在不均衡现象。  \n- 正常模式会随时间或环境变化，模型需要应对数据漂移。  \n- 异常形式多样，缺乏统一定义，人工标注成本较高。  \n- 方法对噪声存在敏感性，需要区分噪声与真实异常。  \n- 应用涉及多个领域，如金融风控、设备监测与网络安全。  \n- 评价指标依赖具体场景，需要在误报与漏报之间进行权衡。"
  },
  {
    "instruction": "请用不少于100个字描述代码生成。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        100
      ]
    ],
    "topic": "代码生成",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "代码生成是指通过工具或模型自动产生源代码的过程，常见方式包括模板驱动、规则引擎以及基于机器学习的生成模型。它能够根据需求说明、接口定义或自然语言描述，快速输出可编译、可维护的代码，从而提升开发效率、减少重复劳动并降低人为错误。同时，代码生成也需要配合规范、测试和审查机制，避免安全隐患、性能问题和风格不一致，确保生成结果可读、可控、可持续演进。在实际应用中，它常被用于接口层、数据访问层、测试用例和脚手架的创建，并与持续集成流程结合，形成高效的软件工程实践。"
  },
  {
    "instruction": "写一段关于推荐系统的文字，必须包含以下关键词：反向传播, 优化, 神经网络。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "反向传播",
          4
        ]
      ]
    ],
    "topic": "推荐系统",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "推荐系统通过分析用户行为和内容特征来预测兴趣偏好，近年来广泛采用神经网络作为核心模型，以提升对复杂关系的建模能力；在训练过程中，模型利用反向传播算法计算误差梯度，并结合各种优化方法不断更新参数，从而逐步提高推荐的准确性和个性化程度，使系统能够在海量数据和动态场景中保持良好性能。"
  },
  {
    "instruction": "用通俗易懂的语言向初学者解释边缘计算。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "边缘计算",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "边缘计算可以理解为：**把“计算”和“处理数据”的工作，尽量放在离数据产生地点更近的地方去做**。\n\n---\n\n### 先打个比方  \n以前我们用手机拍照、玩游戏，很多事情要把数据传到“远方的大电脑”（云服务器）处理，再把结果传回来。这就像你每次做作业，都要跑到很远的图书馆请老师帮你改，路上既花时间又麻烦。\n\n**边缘计算**就是：  \n在你家附近、甚至就在你手里的设备上，先把作业改一部分，只有必要的时候才送到远方。\n\n---\n\n### 什么是“边缘”  \n“边缘”指的是**靠近用户或设备的一端**，比如：  \n- 手机、摄像头、智能音箱  \n- 路由器、基站  \n- 工厂里的机器、传感器  \n\n这些地方都可以进行简单甚至较复杂的计算。\n\n---\n\n### 为什么要用边缘计算  \n1. **更快**：不用把数据传很远，反应速度更快  \n2. **更省流量**：本地处理后，只上传有用的数据  \n3. **更安全**：敏感数据不必全部传到云端  \n4. **更稳定**：网络不好时，也能正常工作\n\n---\n\n### 举个生活中的例子  \n- **智能摄像头**：  \n  直接在摄像头里识别人是不是“人”，只有发现异常才报警，而不是把所有视频都传到云端。  \n- **自动驾驶**：  \n  汽车必须立刻判断路况，不能等云端回复，所以很多计算要在车上完成。\n\n---\n\n### 一句话总结  \n**云计算像“远程大脑”，边缘计算像“身边的小脑”，让设备更聪明、反应更快。**"
  },
  {
    "instruction": "请用列表形式列出深度学习的6个要点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "列表"
      ]
    ],
    "topic": "深度学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. **表示学习**：通过多层非线性变换自动学习数据的高层特征表示。  \n2. **神经网络结构**：以多层神经网络为核心，如CNN、RNN、Transformer等。  \n3. **大数据依赖**：通常需要大量高质量数据来获得良好性能。  \n4. **反向传播与优化**：利用反向传播算法和梯度下降等方法进行参数更新。  \n5. **计算资源需求**：训练过程计算量大，常依赖GPU/TPU等加速硬件。  \n6. **广泛应用**：在计算机视觉、自然语言处理、语音识别等领域表现突出。"
  },
  {
    "instruction": "请写5段关于联邦学习的内容，每段以数字编号开头。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "联邦学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. 联邦学习是一种分布式机器学习范式，其核心思想是在数据不出本地的前提下进行模型训练，各参与方仅共享模型参数或梯度，从而在保护数据隐私的同时实现协同建模。\n\n2. 在联邦学习框架中，通常包含中心服务器与多个客户端，客户端在本地数据上训练模型后，将更新结果上传至服务器，服务器再进行聚合并下发新的全局模型。\n\n3. 联邦学习在隐私保护方面具有显著优势，能够减少数据集中带来的泄露风险，尤其适用于金融、医疗、政务等对数据安全和合规性要求较高的领域。\n\n4. 尽管优势明显，联邦学习仍面临诸多挑战，如通信成本高、数据分布不均（非独立同分布）以及系统异构性问题，这些都会影响模型的收敛速度和性能。\n\n5. 随着隐私计算、加密技术和边缘计算的发展，联邦学习正在不断演进，未来有望与差分隐私、安全多方计算等技术深度融合，推动更安全、可信的人工智能应用落地。"
  },
  {
    "instruction": "以问答形式介绍大语言模型，包含至少4个问题和回答。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "大语言模型",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**问1：什么是大语言模型（LLM）？**  \n答：大语言模型是一类基于深度学习的人工智能模型，通常使用海量文本数据进行训练，能够理解和生成自然语言，如回答问题、写文章、翻译文本等。\n\n**问2：大语言模型是如何工作的？**  \n答：大语言模型通常基于神经网络中的“Transformer”架构，通过学习上下文中词与词之间的概率关系，预测下一个最可能出现的词，从而实现连贯的语言生成。\n\n**问3：大语言模型可以做哪些事情？**  \n答：它们可以用于对话问答、文本摘要、代码生成、语言翻译、情感分析、知识辅助等多种场景，广泛应用于客服、教育、科研和内容创作领域。\n\n**问4：大语言模型有哪些局限性？**  \n答：尽管能力强大，但大语言模型可能产生不准确或过时的信息，也缺乏真正的理解和情感，需要人类监督和合理使用。\n\n**问5：大语言模型的发展趋势是什么？**  \n答：未来的大语言模型将朝着更高效、更安全、更可控以及与多模态（如图像、音频）结合的方向发展，更好地服务于实际应用。"
  },
  {
    "instruction": "介绍模型量化，但不要使用'特别'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "特别"
      ]
    ],
    "topic": "模型量化",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "模型量化是指在不显著降低模型性能的前提下，将神经网络中的高精度数值（如32位浮点数）转换为低精度表示（如8位整数或更低）的技术。通过减少参数和计算的数值精度，模型在存储、计算和能耗方面都会更加高效。\n\n在实际应用中，模型量化主要用于加速推理过程和降低模型体积，适合部署在算力和内存受限的环境中，例如移动设备、嵌入式系统和边缘计算场景。量化后的模型通常能够在保持可接受精度的同时，大幅减少内存占用并提升推理速度。\n\n常见的量化方式包括后训练量化和量化感知训练。前者在模型训练完成后直接进行量化，实现简单；后者在训练阶段就考虑量化误差，通常能获得更好的精度表现。总体来看，模型量化是深度学习工程化和落地过程中一项重要技术。"
  },
  {
    "instruction": "写一篇关于代码生成的短文，必须以问句结尾。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "代码生成",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "代码生成正在改变软件开发的方式。通过将需求、示例或自然语言转化为可运行的程序，开发者可以更快地验证想法、减少重复劳动，并提升一致性。然而，代码生成也带来了质量控制、可维护性和安全性的挑战，需要工具、流程与人的判断共同把关。那么，在效率与可靠性之间，我们该如何平衡代码生成的使用？"
  },
  {
    "instruction": "请写4段关于深度学习的内容，每段以数字编号开头。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "深度学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. 深度学习是机器学习的重要分支，其核心思想是通过多层神经网络自动学习数据中的特征表示，从而减少人工特征工程的依赖，提高模型在复杂任务中的表现。  \n\n2. 在实际应用中，深度学习已广泛用于计算机视觉、自然语言处理和语音识别等领域，例如图像分类、机器翻译和智能语音助手，都依赖深度神经网络的强大建模能力。  \n\n3. 深度学习模型通常需要大量数据和计算资源进行训练，尤其是在使用卷积神经网络或Transformer等复杂结构时，GPU或专用加速芯片在提升训练效率方面发挥着关键作用。  \n\n4. 尽管深度学习取得了显著成果，但仍面临可解释性不足、训练成本高以及对数据质量敏感等挑战，这也推动了学术界和工业界不断探索更高效、更可靠的新方法。"
  },
  {
    "instruction": "写一段关于联邦学习的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "非常"
      ]
    ],
    "topic": "联邦学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "联邦学习是一种机器学习方法，参与方在各方设备上进行模型训练，不共享数据。训练过程中，设备向服务器发送参数，"
  },
  {
    "instruction": "介绍图像分割，但不要使用'极其'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "极其"
      ]
    ],
    "topic": "图像分割",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "图像分割是计算机视觉中的一项基础任务，目标是将一幅图像划分为若干具有语义意义的区域或对象，使同一区域内的像素在特征上更为一致，而不同区域之间存在明显差异。通过图像分割，计算机可以更好地理解图像内容，为后续的分析与决策提供结构化信息。\n\n常见的图像分割方法包括基于阈值的分割、边缘检测、区域生长、聚类方法以及基于深度学习的语义分割和实例分割。随着卷积神经网络和Transformer等模型的发展，分割精度和适用场景不断提升。图像分割被广泛应用于医学影像分析、自动驾驶、遥感测绘、工业检测等领域，是连接“看见”和“理解”的关键技术之一。"
  },
  {
    "instruction": "写一段关于边缘计算的文字，必须包含以下关键词：深度学习, 神经网络, 人工智能。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "优化",
          3
        ]
      ]
    ],
    "topic": "边缘计算",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "边缘计算通过将数据处理和分析能力下沉到靠近数据源的设备侧，能够显著降低延迟、提升隐私与可靠性，在此过程中，人工智能技术发挥着关键作用：依托深度学习模型在边缘设备上的高效推理，结合神经网络对复杂模式的快速识别，边缘计算可以实现对实时数据的智能决策与响应，广泛应用于智能制造、智慧城市和物联网等场景。"
  },
  {
    "instruction": "用通俗易懂的语言向初学者解释提示工程。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "提示工程",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**提示工程（Prompt Engineering）**，简单来说，就是**学会“怎么跟 AI 说话”，才能让它更好地帮你干活**。\n\n你可以把 AI 想象成一个**很聪明、但需要明确指令的助手**。你给它的“提示”（Prompt），就是你对它说的话。提示工程就是研究：**怎样提问、怎样下指令，AI 才能给出更准确、更有用的回答**。\n\n### 举个生活中的例子\n- 你说：“帮我写一篇文章。”\n  → AI 不知道你要写什么，结果可能很笼统。\n- 你说：“请用通俗的语言，写一篇 300 字的文章，给小学生讲什么是环保，并举 2 个生活中的例子。”\n  → AI 就更容易给你满意的结果。\n\n**提示越清楚，结果通常越好。**\n\n### 提示工程通常包括哪些要点？\n1. **说明角色**：让 AI 扮演谁  \n   > 例如：“你是一名有 10 年经验的数学老师。”\n2. **说明任务**：你要它做什么  \n   > 例如：“给初中生讲解一元二次方程。”\n3. **说明要求**：格式、长度、风格等  \n   > 例如：“用列表形式，不超过 200 字。”\n4. **给例子（可选）**：告诉它你想要的样子  \n   > 例如：“像下面这样回答：……”\n\n### 一句话总结\n**提示工程不是编程，而是把需求说清楚。**  \n谁更会“提问”，谁就更能发挥 AI 的能力。\n\n如果你愿意，我也可以教你几个**立刻能用的提示模板**。"
  },
  {
    "instruction": "用通俗易懂的语言向初学者解释文本分类。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "文本分类",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "文本分类，简单来说，就是**让电脑学会给文字“贴标签”**。\n\n你可以把它想象成整理文件：  \n你有一堆文章，有的是新闻，有的是广告，有的是聊天记录。人一看就知道该把它们放到哪个文件夹里，而**文本分类就是教电脑做同样的事情**。\n\n**举个例子：**\n- 邮箱里，电脑会把邮件分成“垃圾邮件”和“正常邮件”\n- 新闻网站，把文章分成“体育”“娱乐”“科技”\n- 评论区，把留言分成“正面评价”“负面评价”\n\n**电脑是怎么做到的呢？**\n1. **看很多例子**：先给电脑一些已经分好类的文本，比如“这是一封垃圾邮件”“这是一条正常消息”  \n2. **找规律**：电脑会发现，垃圾邮件里常出现“免费”“中奖”等词  \n3. **做判断**：当来了一段新文字，电脑就根据学到的规律，判断它更像哪一类\n\n**一句话总结：**  \n文本分类就是利用文字内容的特点，让电脑自动判断这段文字属于哪一类。\n\n对初学者来说，只要记住：**文本分类 = 给文字自动分组**，就够了。"
  },
  {
    "instruction": "写一段关于命名实体识别的文字，必须包含以下关键词：损失函数, 人工智能, 梯度下降。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "梯度下降",
          3
        ]
      ]
    ],
    "topic": "命名实体识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "命名实体识别是自然语言处理中的重要任务，旨在从文本中识别出人名、地名、组织机构等关键信息，它是人工智能理解语言语义的基础能力之一。在实际应用中，模型通常通过标注数据进行训练，并利用损失函数来衡量预测结果与真实标签之间的差异，再通过梯度下降等优化方法不断更新参数，从而提升识别的准确率和鲁棒性。"
  },
  {
    "instruction": "以幽默风趣的方式介绍模型量化。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "模型量化",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "想象一下，模型是个“吃货”，原本吃的是满汉全席：32位浮点数，牛羊鱼虾样样不落，营养是够了，但**费钱、费时间、还占地方**。  \n这时候，模型量化登场了——它就像一位精打细算的营养师，说：“别吃那么奢侈，换成盒饭吧，照样有力气干活！”\n\n**模型量化**做的事情，其实很简单：  \n👉 把模型里“高精度、体型庞大”的数值（比如 FP32）  \n👉 换成“更苗条、便宜耐用”的数值（比如 INT8）\n\n结果呢？  \n- 模型：**瘦了**（占用内存更小）  \n- 推理：**快了**（计算更省力）  \n- 硬件：**笑了**（CPU/边缘设备终于不喘了）  \n\n你可能会问：“那精度不会饿瘦了吗？”  \n放心，大多数时候只是“少吃甜点”，**核心营养还在**。有些模型甚至量化完还能正常上岗，老板都看不出来它已经在“减脂期”。\n\n所以一句话总结：  \n👉 **模型量化 = 给深度学习模型办健身卡**  \n不追求肌肉最大块，只追求**性价比最高**。"
  },
  {
    "instruction": "请用列表形式列出自监督学习的4个要点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "列表"
      ]
    ],
    "topic": "自监督学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. **无需人工标注**：通过从原始数据中自动构造监督信号，减少对人工标注数据的依赖。  \n2. **预训练任务设计**：利用预文本任务或对比学习等方式，让模型从数据中学习有意义的表示。  \n3. **表示学习为核心**：目标是学习通用、可迁移的特征表示，而非直接解决具体下游任务。  \n4. **利于下游迁移**：学到的表示可通过微调或线性评估，高效应用于多种下游任务。"
  },
  {
    "instruction": "撰写文本分类的介绍，每段必须包含'损失函数'。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "损失函数",
          3
        ]
      ]
    ],
    "topic": "文本分类",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "文本分类是自然语言处理中的核心任务之一，目标是将文本自动归入预定义的类别中，常见应用包括情感分析、主题识别和垃圾信息过滤。在模型训练过程中，特征表示、模型结构以及损失函数共同决定了分类效果，其中损失函数用于度量模型预测结果与真实标签之间的差异。\n\n随着深度学习的发展，文本分类方法从传统的机器学习逐步演进到基于神经网络和预训练模型的方案，如CNN、RNN和Transformer等。在这些方法中，合理选择和设计损失函数对于模型收敛速度、稳定性以及最终性能具有重要影响，因此损失函数始终是文本分类研究与实践中的关键组成部分。"
  },
  {
    "instruction": "用正式的学术语言描述深度学习。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "深度学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "深度学习是机器学习领域中的一个重要分支，其核心在于通过构建具有多层非线性变换的神经网络模型，实现对数据高层次抽象特征的自动学习与表征。该方法以大规模数据和高性能计算为基础，通常采用反向传播算法与梯度优化技术对模型参数进行端到端训练，从而在复杂函数逼近与模式识别任务中表现出卓越性能。深度学习模型能够通过层级化结构逐级提取特征，有效刻画数据的内在结构与统计规律，因而在计算机视觉、自然语言处理、语音识别等领域取得了显著的理论进展与广泛的实际应用。"
  },
  {
    "instruction": "用恰好120个字概括边缘计算的核心要点。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "exact_length",
        120
      ]
    ],
    "topic": "边缘计算",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "边缘计算将数据处理从云端下沉到靠近数据源的边缘节点以降低时延提升响应可靠性并节省带宽它适用于物联网工业互联网与智能终端通过本地分析实时决策增强隐私安全并与云协同实现弹性扩展"
  },
  {
    "instruction": "用简洁的技术文档风格说明机器翻译。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "机器翻译",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**机器翻译（Machine Translation, MT）概述**\n\n**定义**\n- 机器翻译是利用计算机模型将文本或语音从源语言自动转换为目标语言的技术。\n\n**主要方法**\n- 规则驱动（RBMT）：基于语法与词典规则。\n- 统计机器翻译（SMT）：基于双语语料的概率模型。\n- 神经机器翻译（NMT）：基于深度学习的端到端模型（如 Transformer）。\n\n**工作流程**\n1. 输入预处理（分词、规范化）\n2. 编码表示（语义建模）\n3. 解码生成（目标语言输出）\n4. 后处理（格式与一致性）\n\n**评估指标**\n- BLEU、TER、COMET 等自动指标\n- 人工评测（准确性、流畅性、忠实度）\n\n**应用场景**\n- 跨语言沟通、内容本地化、搜索与客服\n\n**局限性**\n- 语境理解不足、低资源语言表现受限"
  },
  {
    "instruction": "请用JSON格式输出计算机视觉的关键信息。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "计算机视觉",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "```json\n{\n  \"领域名称\": \"计算机视觉\",\n  \"定义\": \"计算机视觉是研究如何使计算机从图像和视频中获取、理解并利用信息的学科。\",\n  \"核心任务\": [\n    \"图像分类\",\n    \"目标检测\",\n    \"图像分割\",\n    \"人脸识别\",\n    \"姿态估计\",\n    \"视频理解\"\n    ],\n  \"关键技术\": [\n    \"卷积神经网络（CNN）\",\n    \"视觉Transformer（ViT）\",\n    \"深度学习\",\n    \"特征提取与表示学习\",\n    \"多模态学习\"\n  ],\n  \"常用数据集\": [\n    \"ImageNet\",\n    \"COCO\",\n    \"MNIST\",\n    \"CIFAR-10/100\",\n    \"Open Images\"\n  ],\n  \"应用场景\": [\n    \"自动驾驶\",\n    \"医学影像分析\",\n    \"安防监控\",\n    \"工业质检\",\n    \"增强现实与虚拟现实\"\n  ],\n  \"主要挑战\": [\n    \"数据标注成本高\",\n    \"复杂场景下的鲁棒性\",\n    \"实时性与计算资源限制\",\n    \"模型可解释性\"\n  ],\n  \"常用工具与框架\": [\n    \"OpenCV\",\n    \"PyTorch\",\n    \"TensorFlow\",\n    \"Detectron2\",\n    \"MMDetection\"\n  ],\n  \"发展趋势\": [\n    \"大模型与基础模型\",\n    \"自监督与弱监督学习\",\n    \"多模态融合\",\n    \"边缘计算与低功耗部署\"\n  ]\n}\n```"
  },
  {
    "instruction": "撰写Transformer架构的介绍，每段必须包含'反向传播'。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "反向传播",
          2
        ]
      ]
    ],
    "topic": "Transformer架构",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "Transformer架构是一种以自注意力机制为核心的深度学习模型，最初用于自然语言处理任务。它通过并行计算取代传统循环结构，大幅提升了训练效率，并且在训练过程中依然依赖反向传播来更新模型参数，从而学习序列中复杂的依赖关系。\n\n在结构上，Transformer由编码器和解码器堆叠而成，每一层包含多头自注意力、前馈神经网络以及残差连接与层归一化。这些组件在前向计算中高效建模上下文信息，而在参数优化阶段，误差信号会通过反向传播在各层之间传递，以保证注意力权重和网络参数得到有效调整。\n\n从训练和应用角度来看，Transformer易于扩展，适合在大规模数据和算力条件下训练，因此催生了BERT、GPT等预训练模型。无论是在预训练还是微调阶段，模型性能的提升都依赖稳定而高效的反向传播机制，使其能够在翻译、问答和生成等任务中取得卓越表现。"
  },
  {
    "instruction": "请用列表形式列出RAG检索增强的5个要点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "列表"
      ]
    ],
    "topic": "RAG检索增强",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. **引入外部知识**：通过检索文档或数据库，为生成模型提供实时、可更新的背景信息。  \n2. **降低幻觉风险**：基于可验证的检索结果生成答案，减少凭空编造内容。  \n3. **提升答案准确性**：将问题与相关资料匹配，使回答更贴近事实和上下文。  \n4. **易于领域定制**：只需更换或扩展检索源，即可快速适配不同行业或场景。  \n5. **增强可解释性**：生成结果可追溯到具体检索内容，便于审计和引用。"
  },
  {
    "instruction": "请用JSON格式输出模型压缩的关键信息。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "模型压缩",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "```json\n{\n  \"定义\": \"模型压缩是通过减少模型参数量、计算量或存储需求，在尽量保持性能的前提下提升推理效率的技术集合。\",\n  \"主要方法\": [\n    \"剪枝（结构化/非结构化）\",\n    \"量化（PTQ/QAT，INT8/INT4）\",\n    \"知识蒸馏\",\n    \"低秩分解\",\n    \"参数共享与权重聚类\",\n    \"稀疏化\"\n  ],\n  \"优点\": [\n    \"降低模型大小\",\n    \"减少推理延迟\",\n    \"降低算力与能耗\",\n    \"便于边缘与移动端部署\"\n  ],\n  \"缺点与权衡\": [\n    \"可能带来精度下降\",\n    \"训练与调参复杂度增加\",\n    \"硬件与框架支持差异\"\n  ],\n  \"适用场景\": [\n    \"移动端与嵌入式设备\",\n    \"实时推理系统\",\n    \"大规模在线服务\",\n    \"资源受限环境\"\n  ],\n  \"常用评估指标\": [\n    \"模型大小\",\n    \"参数量\",\n    \"FLOPs/算力\",\n    \"推理延迟\",\n    \"精度/损失变化\"\n  ],\n  \"注意事项\": [\n    \"选择与硬件匹配的压缩策略\",\n    \"评估精度-效率权衡\",\n    \"结合多种方法效果更佳\"\n  ]\n}\n```"
  },
  {
    "instruction": "写一篇关于机器学习的短文，必须以问句结尾。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "机器学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "机器学习是人工智能的重要分支，通过从数据中学习规律来提升系统的预测与决策能力。它已广泛应用于图像识别、自然语言处理和推荐系统等领域，推动了产业效率与创新速度。然而，数据偏见、模型可解释性以及隐私保护等问题仍然值得关注，在快速发展的今天，我们该如何让机器学习更安全、更公平并真正造福社会呢？"
  },
  {
    "instruction": "在描述语音识别时，请确保'深度学习'这个词出现至少3次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "深度学习",
          3
        ]
      ]
    ],
    "topic": "语音识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "语音识别是一种将人类语音转换为文本或指令的技术，核心方法依赖于**深度学习**模型对声音信号进行建模与理解。通过**深度学习**，系统可以从大量语音数据中自动学习特征，提高对不同口音、语速和噪声环境的适应能力。现代语音识别通常结合神经网络、声学模型和语言模型，而这些组件大多建立在**深度学习**框架之上。随着算力和数据规模的提升，**深度学习**持续推动语音识别在准确率和实时性方面取得显著进步。"
  },
  {
    "instruction": "用正式的学术语言描述联邦学习。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "联邦学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "联邦学习（Federated Learning）是一种分布式机器学习范式，旨在在不集中原始数据的前提下实现多方协同建模。其核心思想是将模型训练过程从中心化的数据聚合转变为去中心化的参数或梯度协同：数据保留在本地节点（如终端设备或机构服务器），各参与方基于本地数据独立更新模型参数，并仅将必要的模型更新信息上传至协调方进行聚合，从而迭代优化全局模型。\n\n从方法论上看，联邦学习通常采用“本地训练—全局聚合—模型下发”的迭代机制，典型算法如联邦平均（Federated Averaging, FedAvg）。该范式在保障数据隐私与合规性的同时，能够有效利用分布式异构数据资源，降低数据传输成本与隐私泄露风险。然而，联邦学习仍面临诸多挑战，包括非独立同分布（Non-IID）数据导致的收敛与性能问题、系统与统计异构性、通信效率、以及对抗性攻击与隐私推断风险等。因此，联邦学习的研究不仅涉及优化算法与系统设计，还与隐私保护、加密技术及可信计算等方向密切相关，已成为当前分布式人工智能领域的重要研究热点。"
  },
  {
    "instruction": "介绍计算机视觉，但不要使用'很'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "很"
      ]
    ],
    "topic": "计算机视觉",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "计算机视觉是人工智能的重要分支，研究如何让计算机从图像和视频中获取信息、理解内容并作出决策。它结合数学、统计学、机器学习与光学等领域，通过特征提取、模式识别和深度学习模型，实现目标检测、图像分类、语义分割与三维重建等任务。\n\n在应用方面，计算机视觉被广泛用于自动驾驶、医学影像、工业检测、安防监控与内容生成。随着算力提升与数据规模扩大，相关算法在准确性、效率与泛化能力上持续进步，推动智能系统在现实场景中的落地。"
  },
  {
    "instruction": "以幽默风趣的方式介绍模型压缩。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "模型压缩",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "想象一下：你训练了一个**肌肉猛男级别**的深度学习模型，力量惊人，但问题来了——它走两步就喘，部署到手机上直接“原地躺平”。这时候，**模型压缩**就像请来了一位专业健身教练：不减实力，只减赘肉。\n\n模型压缩的核心思想很简单：**让模型变瘦、变快、变省钱**。比如  \n- **剪枝**：把“长期摸鱼、不干活”的神经元请出公司，模型立刻清爽不少。  \n- **量化**：原本吃“32 位精致西餐”的参数，改成“8 位家常便饭”，照样能干活，还更省资源。  \n- **蒸馏**：让大模型当老师，小模型当学生，把“毕生功力”浓缩成几招必杀技传下去。\n\n最终效果就是：模型从“服务器豪宅专属”进化成“手机也能随身携带”，推理速度飞起，功耗直降，还不太影响智商。  \n一句话总结：**模型压缩，就是让 AI 学会少吃、多干、跑得快，还不掉链子。**"
  },
  {
    "instruction": "描述边缘计算，开头必须是'众所周知'。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "边缘计算",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "众所周知，边缘计算是一种将数据处理和计算能力从集中式云端下沉到网络边缘的技术架构，它通过在靠近数据源或用户的设备上进行实时分析与决策，减少数据传输延迟和带宽消耗，提高系统响应速度与可靠性。边缘计算广泛应用于物联网、智能制造、自动驾驶和智慧城市等场景，有助于提升数据隐私保护能力，并在网络不稳定或对时延要求极高的情况下保持业务连续性。"
  },
  {
    "instruction": "请用列表形式列出RAG检索增强的3个要点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "列表"
      ]
    ],
    "topic": "RAG检索增强",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. **外部知识检索**：通过向量检索等方式从文档库中获取相关信息，弥补模型参数内知识的不足。  \n2. **降低幻觉、提升准确性**：基于真实检索结果生成答案，使输出更可验证、更可靠。  \n3. **知识可更新与可扩展**：无需重新训练模型，只需更新检索库即可引入最新或领域知识。"
  },
  {
    "instruction": "以问答形式介绍情感分析，包含至少5个问题和回答。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "情感分析",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**问1：什么是情感分析？**  \n**答：**情感分析（Sentiment Analysis）是自然语言处理中的一种技术，用于识别和判断文本中所表达的情绪倾向，如正面、负面或中性。\n\n**问2：情感分析可以应用在哪些场景？**  \n**答：**情感分析常用于产品评论分析、社交媒体舆情监测、客户满意度评估、品牌声誉管理等场景。\n\n**问3：情感分析一般如何实现？**  \n**答：**常见方法包括基于规则的方法、基于传统机器学习的方法，以及基于深度学习的方法，如使用神经网络和预训练语言模型。\n\n**问4：情感分析的结果通常有哪些类型？**  \n**答：**结果通常分为正向、负向和中性三类，有时也会细分为多级情感强度，如“非常满意”“一般”“非常不满”等。\n\n**问5：情感分析面临哪些挑战？**  \n**答：**主要挑战包括语言的歧义性、讽刺和反讽的识别、上下文依赖以及不同领域和文化带来的差异。\n\n**问6：情感分析的准确性如何提高？**  \n**答：**可以通过引入高质量标注数据、结合上下文信息、使用更先进的模型以及进行领域自适应训练来提高准确性。"
  },
  {
    "instruction": "用简洁的技术文档风格说明大语言模型。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "大语言模型",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**大语言模型（LLM）说明**\n\n**1. 概述**  \n大语言模型是基于深度学习的自然语言处理模型，使用大规模语料训练，能够理解、生成和转换自然语言文本。\n\n**2. 架构**  \n- 核心结构：Transformer  \n- 关键组件：自注意力机制（Self-Attention）、前馈网络、位置编码  \n- 参数规模：通常为数十亿至万亿级\n\n**3. 训练方式**  \n- 预训练：在海量无标注文本上进行自监督学习（如下一词预测）  \n- 微调：使用任务数据或人类反馈（如SFT、RLHF）优化行为\n\n**4. 推理机制**  \n- 基于概率分布逐词生成  \n- 支持上下文窗口内的条件建模\n\n**5. 主要能力**  \n- 文本生成与理解  \n- 问答与摘要  \n- 代码生成与分析  \n- 多语言处理\n\n**6. 局限性**  \n- 可能产生不准确或虚构内容  \n- 对上下文长度和数据质量敏感  \n- 缺乏真实世界感知和因果理解\n\n**7. 典型应用**  \n- 智能助手与客服  \n- 文档与代码辅助  \n- 搜索与知识管理  \n- 教育与内容生成"
  },
  {
    "instruction": "请用不少于150个字描述自监督学习。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        150
      ]
    ],
    "topic": "自监督学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "自监督学习是一种介于监督学习与无监督学习之间的重要范式，其核心思想是从大量未标注数据中自动构造监督信号。模型通过设计预任务或预文本任务，如预测缺失部分、对比不同视角的一致性、时序排序等，从数据自身产生标签，在无需人工标注的情况下学习有用的表示。经过自监督预训练后，模型能够捕获数据中的结构、语义和统计规律，再利用少量标注数据进行下游任务微调，往往可以显著提升性能与泛化能力。该方法已在计算机视觉、自然语言处理和语音等领域得到广泛应用，有效降低了标注成本，提高了数据利用效率。"
  },
  {
    "instruction": "用Markdown格式撰写关于异常检测的说明，必须包含标题和分点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "异常检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "# 异常检测说明\n\n- **定义**：异常检测（Anomaly Detection）是指在数据中识别与大多数正常模式显著不同的样本或行为的过程。\n- **常见方法**：\n  - 基于统计的方法（如均值、方差、分布假设）\n  - 基于距离或密度的方法（如KNN、LOF）\n  - 基于机器学习的方法（如孤立森林、One-Class SVM）\n  - 基于深度学习的方法（如自编码器、变分自编码器）\n- **应用场景**：\n  - 金融风控中的欺诈检测\n  - 工业设备的故障诊断\n  - 网络安全中的入侵检测\n  - 医疗健康中的异常生理信号识别\n- **主要挑战**：\n  - 异常样本稀少且难以标注\n  - 数据分布随时间变化（概念漂移）\n  - 高维数据带来的计算与建模难度"
  },
  {
    "instruction": "以幽默风趣的方式介绍Transformer架构。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "Transformer架构",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "想象一下，Transformer 架构就像一家**不加班、还效率爆表的翻译公司**。\n\n以前的模型（比如 RNN）工作方式很“社畜”：  \n单词一个接一个地处理，前面的不干完，后面的只能干等着，排队排到天荒地老。  \nTransformer 看不下去了，说了一句名言：**“为什么要排队？我全都要一起看！”**\n\n---\n\n### Transformer 的核心绝活：注意力机制（Attention）\nAttention 就像开会时的**重点笔记能力**。  \n当 Transformer 看到一句话，比如：\n\n> “我昨天用手机拍了一只正在吃披萨的猫”\n\n它会想：  \n- “现在我要理解‘猫’，那‘吃’和‘披萨’是不是比‘昨天’更重要？”  \n于是它给“披萨”“吃”打高分，给“昨天”打低分。  \n这就叫：**抓重点，不走神**。\n\n而且它还不止一个人记笔记——  \n**多头注意力（Multi-Head Attention）**就像一群人从不同角度做会议记录：  \n一个关注语法，一个关注语义，一个专盯“猫到底在干嘛”。\n\n---\n\n### 不看顺序？不行，那就加点“位置感”\nTransformer 虽然喜欢“同时看全部”，但它也知道：  \n“我爱你”和“你爱我”不能搞混。  \n于是它偷偷加了**位置编码（Positional Encoding）**，  \n相当于给每个词贴上“我是第几个出场”的号码牌。\n\n---\n\n### 总结一下\nTransformer 就是一个：  \n- 不爱排队（并行计算）  \n- 特别会抓重点（注意力机制）  \n- 多线程思考（多头注意力）  \n- 还记得顺序（位置编码）  \n\n的高效打工人。\n\n也正因为它这么能干，  \n才有了后来的 **BERT、GPT，以及你现在正在用的我** 😄"
  },
  {
    "instruction": "用三段式结构（引言、正文、结论）介绍Transformer架构。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "Transformer架构",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**引言**  \nTransformer 是一种用于序列建模的深度学习架构，最早由 Vaswani 等人在 2017 年提出。它最初应用于机器翻译领域，但由于其高效性和强大的建模能力，很快成为自然语言处理乃至多模态任务中的核心模型基础。\n\n**正文**  \nTransformer 架构的核心思想是“注意力机制”，尤其是自注意力（Self-Attention）。与传统的循环神经网络（RNN）或卷积神经网络（CNN）不同，Transformer 不依赖序列的时间顺序来逐步处理输入，而是通过自注意力机制同时建模序列中任意位置之间的关系。经典的 Transformer 由编码器（Encoder）和解码器（Decoder）堆叠而成，每一层包含多头注意力机制、前馈神经网络以及残差连接和层归一化。这种设计不仅提升了并行计算能力，还能更好地捕捉长距离依赖关系。\n\n**结论**  \n总体而言，Transformer 架构通过引入自注意力机制，突破了传统序列模型在效率和表达能力上的限制。它为后续模型如 BERT、GPT 等奠定了基础，推动了人工智能在语言理解与生成等领域的快速发展，已成为现代深度学习中不可或缺的关键架构。"
  },
  {
    "instruction": "写一段关于目标检测的文字，必须包含以下关键词：梯度下降, 损失函数, 神经网络。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "推理",
          3
        ]
      ]
    ],
    "topic": "目标检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "目标检测是计算机视觉中的重要任务，旨在从图像或视频中定位并识别感兴趣的目标，现代方法通常基于深度学习中的神经网络，通过端到端的训练方式自动提取特征并预测目标的类别与位置，在训练过程中需要设计合理的损失函数来衡量预测结果与真实标注之间的差异，再利用梯度下降算法不断更新模型参数，从而提升检测精度和鲁棒性。"
  },
  {
    "instruction": "以幽默风趣的方式介绍生成对抗网络。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "生成对抗网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "想象一下，生成对抗网络（GAN）就像一档永不完结的真人秀节目。\n\n节目里有两位主角：**生成器**和**判别器**。  \n生成器是个“造假艺术家”，天天琢磨着怎么画出以假乱真的名画；判别器则是个“火眼金睛的鉴定师”，专门负责挑刺：“这幅画是真的，还是你又在糊弄我？”\n\n一开始，生成器水平很菜，画出来的东西一眼假，判别器轻松碾压，内心OS是：“就这？”  \n但生成器不服气啊，不断复盘失败经验，偷偷升级画技；判别器也不敢松懈，疯狂进修鉴定知识。于是，两人开启了**互相折磨、共同成长**的循环。\n\n训练到后期，生成器已经能画出让判别器都犹豫半天的作品，而判别器也被逼成了专家级评委。最终的结果是：  \n👉 生成器学会了如何“创造看起来像真的数据”，  \n👉 判别器学会了如何“分辨真假”，  \n👉 而我们人类，白嫖到了高质量的图片、音乐、甚至虚拟人脸。\n\n所以，生成对抗网络的精髓就是：  \n**在一场“你骗我、我拆穿你”的相爱相杀中，智能悄悄诞生了。**"
  },
  {
    "instruction": "写一段关于代码生成的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "极其"
      ]
    ],
    "topic": "代码生成",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "代码生成是软件开发中的一个过程。开发者通过规则、模板或模型生成代码。过程可以用于创建函数、接口和测试。它帮助减少编写代码的次数，并支持维护和扩展。代码生成也可以与版本控制和构建流程结合使用。"
  },
  {
    "instruction": "用恰好80个字概括RAG检索增强的核心要点。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "exact_length",
        80
      ]
    ],
    "topic": "RAG检索增强",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "通过外部知识检索补充模型上下文将相关文档引入生成流程实现事实增强降低幻觉提升可追溯性时效性与领域适配并支持更新与扩展降低成本增强可信度促进落地应用与评估治理安全"
  },
  {
    "instruction": "描述文本分类，开头必须是'随着技术发展'。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "文本分类",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "随着技术发展，文本分类成为自然语言处理中的重要任务，它通过对文本内容进行特征提取与建模，将文本自动归入预定义的类别。传统方法依赖人工特征和统计模型，而现代方法更多采用机器学习与深度学习技术，如朴素贝叶斯、支持向量机以及基于神经网络的模型。文本分类广泛应用于垃圾邮件过滤、情感分析、主题识别和信息检索等领域，能够提升信息处理效率并降低人工成本。"
  },
  {
    "instruction": "在描述大语言模型时，请确保'反向传播'这个词出现至少3次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "反向传播",
          3
        ]
      ]
    ],
    "topic": "大语言模型",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "大语言模型基于大规模语料和深度神经网络，通常采用 Transformer 架构进行预训练与微调。在训练过程中，模型通过前向计算得到预测，再利用损失函数进行反向传播来更新参数；这种反向传播使得梯度能够从输出层传回各个隐藏层。无论是在预训练还是指令微调阶段，反向传播都是核心机制，它与优化器配合提升模型的收敛速度与泛化能力。此外，在工程实践中还会通过并行化与混合精度等手段来加速反向传播，从而提高整体训练效率。"
  },
  {
    "instruction": "撰写神经网络的介绍，每段必须包含'特征提取'。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "特征提取",
          4
        ]
      ]
    ],
    "topic": "神经网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "神经网络是一类受生物神经系统启发的计算模型，通过多层节点的连接来完成复杂任务，其核心优势在于能够自动进行特征提取，从而减少对人工规则和先验知识的依赖。\n\n在训练过程中，神经网络通过反向传播算法不断调整参数，不同层次逐步学习从低级到高级的表示，这种层级化学习方式使得特征提取更加高效且具有鲁棒性。\n\n在图像识别、语音识别和自然语言处理等领域，神经网络展现了卓越性能，正是因为其强大的特征提取能力能够捕捉数据中的关键模式并支持准确的预测与决策。"
  },
  {
    "instruction": "以幽默风趣的方式介绍图像分割。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "图像分割",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "想象一下，图像分割就像给一张照片“分班”。\n\n一张图片原本是个热闹的大教室：猫、狗、桌子、天空，全都挤在一起。图像分割老师一进来，敲敲黑板说：“来来来，猫坐一排，狗坐一排，背景去后排，谁都别乱坐！”于是，像素们乖乖站队，每个像素都被贴上标签：这是猫的，这是地板的，那是天空的。\n\n和“只认脸”的图像分类不同，图像分割是个细节控——不仅要知道“有一只猫”，还要精确到“猫的胡须从第123个像素开始”。可以说，它追求的是**像素级的秩序感**。\n\n所以，下次你看到手机能自动抠图、医学影像能精准圈出肿瘤、自动驾驶能分清“这是路不是墙”，别忘了——背后正是图像分割在默默加班，把世界一块一块地分清楚。"
  },
  {
    "instruction": "写一篇关于目标检测的短文，必须以问句结尾。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "目标检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "目标检测是计算机视觉中的核心任务之一，旨在在图像或视频中同时定位并识别多个目标，常见方法包括基于两阶段的Faster R-CNN以及端到端的YOLO和SSD，它们在自动驾驶、安防监控和医疗影像等领域发挥着重要作用，随着深度学习和算力的提升，模型在精度与速度上不断突破，但在复杂场景、遮挡和小目标识别方面仍面临挑战，未来目标检测将如何在准确性与效率之间取得更好的平衡？"
  },
  {
    "instruction": "在描述模型量化时，请确保'算法'这个词出现至少3次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "算法",
          3
        ]
      ]
    ],
    "topic": "模型量化",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "模型量化是一种通过降低数值精度来压缩模型并加速推理的技术。其核心思想是将浮点权重和激活映射为低比特表示，不同算法在精度与效率之间做出权衡。常见的算法包括对称量化、非对称量化以及感知训练的量化算法，它们通过统计分布来确定量化参数。选择合适的算法可以在保持准确率的同时显著减少计算和存储开销。"
  },
  {
    "instruction": "描述边缘计算，开头必须是'众所周知'。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "边缘计算",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "众所周知，边缘计算是一种将计算、存储和数据处理能力从集中式云中心下沉到网络边缘、靠近数据源或用户侧的技术架构，通过在本地或近端设备上进行实时处理，能够显著降低时延、节省带宽、提升系统可靠性与隐私保护水平，广泛应用于物联网、智能制造、自动驾驶、智慧城市等对实时性和稳定性要求较高的场景。"
  },
  {
    "instruction": "介绍生成对抗网络，但不要使用'非常'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "非常"
      ]
    ],
    "topic": "生成对抗网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "生成对抗网络（Generative Adversarial Network，简称 GAN）是一类用于生成数据的深度学习模型，由伊恩·古德费洛等人在2014年提出。它的核心思想源于博弈论，通过两个神经网络之间的对抗学习来提升生成效果。\n\nGAN 由生成器（Generator）和判别器（Discriminator）组成。生成器的任务是从随机噪声中生成尽量逼真的数据，例如图像、音频或文本；判别器则负责判断输入数据是真实样本还是生成器伪造的样本。在训练过程中，生成器努力“欺骗”判别器，而判别器不断提高识别能力，二者相互促进、共同进化。\n\n随着训练的进行，生成器生成的数据质量会逐步提高，最终可能达到以假乱真的程度。由于这一特性，生成对抗网络在图像生成、图像修复、风格迁移、数据增强等领域得到了广泛应用，同时也推动了人工智能在创意生成方向的发展。"
  },
  {
    "instruction": "介绍计算机视觉，但不要使用'非常'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "非常"
      ]
    ],
    "topic": "计算机视觉",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "计算机视觉是人工智能的重要分支，研究如何让计算机从图像和视频中获取信息并理解内容。它通过算法和模型模拟人类视觉感知过程，完成目标检测、图像分类、语义分割、姿态估计等任务。近年来，随着深度学习、算力提升和数据规模增长，计算机视觉在自动驾驶、医学影像、安防监控、工业质检、增强现实等领域得到广泛应用。尽管取得了显著进展，计算机视觉仍面临光照变化、遮挡、数据偏差与泛化能力等挑战，相关研究仍在持续推进。"
  },
  {
    "instruction": "请用不少于150个字描述注意力机制。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        150
      ]
    ],
    "topic": "注意力机制",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "注意力机制是一种在机器学习和深度学习中广泛使用的重要思想，其核心目标是让模型在处理大量信息时，能够根据当前任务的需求，自动为不同输入部分分配不同的重要性权重。与传统模型对所有信息一视同仁不同，注意力机制通过计算相关性或相似度，使模型更加关注与当前输出最相关的特征、位置或时间步。在自然语言处理领域，注意力机制可以有效捕捉长距离依赖关系，缓解信息遗忘问题；在计算机视觉中，它能够帮助模型聚焦于关键区域；在语音和多模态任务中同样表现出色。自注意力、多头注意力等结构的提出，进一步提升了模型的表达能力，并推动了Transformer等先进模型的发展。同时，注意力机制还在一定程度上增强了模型的可解释性，使人们能够理解模型关注的信息来源。"
  },
  {
    "instruction": "请用JSON格式输出计算机视觉的关键信息。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "计算机视觉",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "```json\n{\n  \"定义\": \"计算机视觉是让计算机从图像和视频中获取、理解和推理信息的人工智能领域。\",\n  \"核心任务\": [\n    \"图像分类\",\n    \"目标检测\",\n    \"语义分割\",\n    \"实例分割\",\n    \"目标跟踪\",\n    \"姿态估计\",\n    \"人脸识别\",\n    \"光学字符识别\"\n  ],\n  \"关键技术\": [\n    \"卷积神经网络（CNN）\",\n    \"视觉Transformer（ViT）\",\n    \"自监督学习\",\n    \"多模态学习\",\n    \"迁移学习\",\n    \"数据增强\"\n  ],\n  \"常用模型\": [\n    \"ResNet\",\n    \"YOLO\",\n    \"Faster R-CNN\",\n    \"Mask R-CNN\",\n    \"Vision Transformer\",\n    \"CLIP\"\n  ],\n  \"数据集\": [\n    \"ImageNet\",\n    \"COCO\",\n    \"Pascal VOC\",\n    \"MNIST\",\n    \"CIFAR-10/100\"\n  ],\n  \"评估指标\": [\n    \"准确率（Accuracy）\",\n    \"精确率（Precision）\",\n    \"召回率（Recall）\",\n    \"F1值\",\n    \"mAP\",\n    \"IoU\"\n  ],\n  \"应用领域\": [\n    \"自动驾驶\",\n    \"安防监控\",\n    \"医疗影像\",\n    \"工业质检\",\n    \"增强现实与虚拟现实\",\n    \"零售与物流\"\n  ],\n  \"主要挑战\": [\n    \"数据标注成本高\",\n    \"复杂场景泛化能力不足\",\n    \"实时性与算力限制\",\n    \"隐私与伦理问题\"\n  ]\n}\n```"
  },
  {
    "instruction": "用三段式结构（引言、正文、结论）介绍强化学习。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "强化学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**引言**  \n强化学习是机器学习的重要分支，研究智能体如何通过与环境的交互来学习最优行为策略。与监督学习依赖标注数据不同，强化学习强调“试错”和“反馈”，智能体通过不断行动并接收奖励或惩罚，逐步提升决策能力。\n\n**正文**  \n在强化学习框架中，核心要素包括智能体、环境、状态、动作和奖励。智能体在某一状态下选择动作，环境随之发生变化并返回新的状态和奖励。学习的目标通常是最大化长期累计奖励，这往往通过策略（policy）来实现。常见方法包括基于价值的算法（如 Q-learning）、基于策略的算法，以及结合二者的 Actor-Critic 方法。强化学习已广泛应用于游戏对弈、机器人控制、自动驾驶和资源调度等领域。\n\n**结论**  \n总体来看，强化学习为解决序列决策和复杂控制问题提供了强有力的工具。尽管在样本效率、稳定性和安全性方面仍面临挑战，但随着算法改进和计算能力提升，强化学习正不断推动人工智能向更高水平的自主决策与智能行为发展。"
  },
  {
    "instruction": "写一段关于生成对抗网络的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "非常"
      ]
    ],
    "topic": "生成对抗网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "生成对抗网络是一种机器学习模型，由生成器和判别器组成。生成器根据输入噪声产生样本，判别器根据样本给出判断。训练时，两个模块通过博弈更新参数，目标是使生成器的输出符合数据分布。该方法已用于图像、语音和文本任务。"
  },
  {
    "instruction": "写一段关于代码生成的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "非常"
      ]
    ],
    "topic": "代码生成",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "代码生成是软件开发中的一个过程。系统根据规则和模板生成源代码。开发者提供输入，工具产生输出。流程包括需求表达、模型建立、代码产出和集成。生成结果进入测试和维护环节。该过程改变了工作方式。"
  },
  {
    "instruction": "用通俗易懂的语言向初学者解释联邦学习。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "联邦学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "联邦学习可以把它想成“大家一起学习，但不把作业交出来”。\n\n举个简单的例子：假设很多人的手机都在学习如何更好地预测下一个输入的字。传统做法是把大家的输入数据都传到一个服务器上统一学习，但这会有隐私风险。**联邦学习的做法不一样**：每部手机只在本地用自己的数据学习一点点，然后把“学到的经验总结”（比如模型参数的变化）发给服务器，而不是把原始数据发出去。\n\n服务器做的事情也很简单：它把来自很多设备的这些“经验总结”合在一起，得到一个更聪明的模型，再把这个模型发回给大家。这样一来，**数据始终留在本地，隐私更安全**，但模型又能利用大家的共同经验不断变好。\n\n一句话总结：**联邦学习就是在不共享原始数据的前提下，让多个设备或机构一起训练一个更好的模型**，既学得好，又更保护隐私。"
  },
  {
    "instruction": "用简洁的技术文档风格说明计算机视觉。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "计算机视觉",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**计算机视觉（Computer Vision）概述**\n\n**定义**  \n计算机视觉是研究如何使计算机从图像或视频中获取、理解和利用信息的技术领域。\n\n**核心任务**  \n- 图像分类  \n- 目标检测与跟踪  \n- 语义/实例分割  \n- 姿态估计与三维重建  \n- 行为与场景理解\n\n**典型流程**  \n1. 数据采集（图像/视频）  \n2. 预处理（去噪、归一化、增强）  \n3. 特征表示（传统特征或深度特征）  \n4. 模型推理（训练/推断）  \n5. 结果评估与部署\n\n**关键技术**  \n- 卷积神经网络（CNN）  \n- Transformer（ViT）  \n- 多视几何与SLAM  \n- 自监督/迁移学习\n\n**应用领域**  \n- 自动驾驶  \n- 工业检测  \n- 医学影像  \n- 安防监控  \n- 人机交互\n\n**常用指标**  \n- 准确率、召回率、F1  \n- mAP、IoU  \n- FPS、延迟"
  },
  {
    "instruction": "请用不少于200个字描述机器翻译。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        200
      ]
    ],
    "topic": "机器翻译",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "机器翻译是指利用计算机和人工智能技术，将一种自然语言自动转换为另一种自然语言的过程。它起源于二十世纪中期，随着计算能力、语料资源以及算法水平的不断提升而逐步发展。早期的机器翻译主要依赖人工编写的规则和词典，通过语法匹配来完成翻译，但在复杂句式和语义理解方面效果有限。后来出现了基于统计的机器翻译方法，系统通过分析大量双语语料，学习词语和句子之间的概率关系，从而提高了翻译的实用性。\n\n近年来，神经网络和深度学习技术的广泛应用，使机器翻译取得了突破性进展。神经机器翻译能够在更大范围内理解上下文信息，生成更加自然、连贯的译文，并支持多语言之间的直接互译。如今，机器翻译已被广泛应用于跨语言交流、国际商务、学术研究和日常生活中，如在线翻译工具和实时语音翻译。然而，它仍然面临文化差异、专业术语理解以及低资源语言支持不足等挑战，未来仍有持续改进和发展的空间。"
  },
  {
    "instruction": "用正式的学术语言描述图像分割。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "图像分割",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "图像分割（image segmentation）是计算机视觉与图像处理领域中的一项基础性研究任务，其目标是依据像素在灰度、颜色、纹理或空间结构等特征上的相似性与差异性，将图像划分为若干在语义或统计意义上具有一致性的区域，从而实现对图像内容的结构化表示。该过程通常被建模为像素级或区域级的标注问题，常用方法包括基于阈值、区域生长、边缘检测的传统技术，以及以卷积神经网络为代表的深度学习方法；其性能通常通过准确率、召回率、交并比等指标进行评估，并在医学影像分析、遥感解译与自动驾驶等领域具有重要应用价值。"
  },
  {
    "instruction": "请用JSON格式输出强化学习的关键信息。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "强化学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "```json\n{\n  \"概念\": \"强化学习是一种通过与环境交互并根据奖励信号来学习最优策略的机器学习方法。\",\n  \"核心要素\": {\n    \"智能体\": \"执行动作并学习策略的主体\",\n    \"环境\": \"智能体所处并与之交互的外部系统\",\n    \"状态\": \"对环境当前情况的描述\",\n    \"动作\": \"智能体在某一状态下可采取的行为\",\n    \"奖励\": \"环境对智能体行为的反馈信号\",\n    \"策略\": \"从状态到动作的映射规则\",\n    \"价值函数\": \"评估状态或状态-动作对长期回报的期望\"\n  },\n  \"学习目标\": \"最大化长期累积奖励（回报）\",\n  \"常见算法\": [\n    \"Q-learning\",\n    \"SARSA\",\n    \"策略梯度\",\n    \"Deep Q-Network (DQN)\",\n    \"Actor-Critic\"\n  ],\n  \"特点\": [\n    \"试错学习\",\n    \"延迟奖励\",\n    \"无需监督标签\"\n  ],\n  \"应用领域\": [\n    \"游戏与博弈\",\n    \"机器人控制\",\n    \"自动驾驶\",\n    \"资源调度\",\n    \"推荐系统\"\n  ],\n  \"优势\": [\n    \"适合序列决策问题\",\n    \"可在复杂环境中学习最优行为\"\n  ],\n  \"挑战\": [\n    \"训练不稳定\",\n    \"样本效率低\",\n    \"对环境建模和奖励设计敏感\"\n  ]\n}\n```"
  },
  {
    "instruction": "请用不少于150个字描述生成对抗网络。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        150
      ]
    ],
    "topic": "生成对抗网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "生成对抗网络（Generative Adversarial Network，简称GAN）是一种重要的深度学习模型，由生成器和判别器两个神经网络组成。生成器的目标是从随机噪声中生成尽可能逼真的样本，而判别器则负责判断输入数据是真实数据还是生成器伪造的数据。二者在训练过程中形成一种对抗博弈关系，生成器不断提升造假能力，判别器不断提高识别能力。通过这种相互竞争的过程，生成器最终可以学习到真实数据的分布特征。GAN在图像生成、图像修复、风格迁移、数据增强等领域有广泛应用，但也存在训练不稳定、模式崩溃等挑战，因此相关改进模型仍是研究热点。"
  },
  {
    "instruction": "请用JSON格式输出联邦学习的关键信息。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "联邦学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "{\n  \"定义\": \"联邦学习是一种分布式机器学习范式，在数据不出本地的前提下，由多方协同训练共享模型。\",\n  \"核心目标\": [\n    \"保护数据隐私\",\n    \"打破数据孤岛\",\n    \"提升模型泛化能力\"\n  ],\n  \"基本架构\": {\n    \"参与方\": \"多个数据持有方（客户端）\",\n    \"协调方\": \"中心服务器或去中心化协调机制\",\n    \"通信方式\": \"周期性交换模型参数或梯度\"\n  },\n  \"典型流程\": [\n    \"初始化全局模型\",\n    \"下发模型到各客户端\",\n    \"客户端本地训练模型\",\n    \"上传模型更新\",\n    \"聚合生成新全局模型\"\n  ],\n  \"常见算法\": [\n    \"FedAvg\",\n    \"FedSGD\",\n    \"FedProx\"\n  ],\n  \"隐私与安全技术\": [\n    \"安全多方计算\",\n    \"差分隐私\",\n    \"同态加密\"\n  ],\n  \"优势\": [\n    \"数据不出本地\",\n    \"符合法规要求\",\n    \"可利用分散数据资源\"\n  ],\n  \"挑战\": [\n    \"通信成本高\",\n    \"数据分布不均（Non-IID）\",\n    \"系统与安全复杂性\"\n  ],\n  \"应用场景\": [\n    \"移动设备个性化建模\",\n    \"医疗健康数据分析\",\n    \"金融风控\"\n  ],\n  \"评估指标\": [\n    \"模型精度\",\n    \"通信开销\",\n    \"训练收敛速度\",\n    \"隐私保护强度\"\n  ]\n}"
  },
  {
    "instruction": "请写4段关于联邦学习的内容，每段以数字编号开头。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "联邦学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. 联邦学习是一种分布式机器学习范式，其核心思想是在不集中原始数据的前提下进行模型训练。各参与方将数据保留在本地，只共享模型参数或梯度，从而在提升模型性能的同时保护数据隐私。\n\n2. 在联邦学习过程中，通常由一个中心服务器负责协调训练，各客户端在本地训练模型后将更新结果上传。服务器对这些更新进行聚合，形成新的全局模型，再下发给各客户端进行下一轮训练。\n\n3. 联邦学习在隐私保护和合规性方面具有显著优势，尤其适用于医疗、金融和智能终端等对数据安全要求较高的场景。通过减少数据集中化风险，它能够有效降低数据泄露和滥用的可能性。\n\n4. 尽管联邦学习前景广阔，但仍面临通信开销大、数据分布不均以及系统异构性等挑战。为此，研究者正在探索更高效的聚合算法、压缩技术和鲁棒机制，以提升联邦学习在真实环境中的可用性。"
  },
  {
    "instruction": "描述生成对抗网络，开头必须是'在当今时代'。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "生成对抗网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "在当今时代，生成对抗网络（Generative Adversarial Network，简称GAN）是一种重要的深度学习模型，由生成器和判别器两部分组成。生成器负责从随机噪声中生成逼真的数据样本，而判别器则试图区分输入数据是真实数据还是生成器伪造的数据。二者在对抗训练中相互博弈、共同进化，最终使生成器能够产生高度逼真的结果。由于这一机制，GAN在图像生成、风格迁移、数据增强以及创意内容生成等领域展现出强大的应用潜力。"
  },
  {
    "instruction": "写一段关于机器翻译的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "很"
      ]
    ],
    "topic": "机器翻译",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "机器翻译使用模型处理文本，将语言A内容转换为语言B内容。系统接收输入，进行分词，建模，生成输出，并向用户提供结果。人们在交流，学习，发布信息时使用这种技术。"
  },
  {
    "instruction": "请用JSON格式输出计算机视觉的关键信息。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "计算机视觉",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "```json\n{\n  \"领域名称\": \"计算机视觉\",\n  \"定义\": \"计算机视觉是研究如何使计算机从图像或视频中获取、理解和分析信息的人工智能领域。\",\n  \"核心任务\": [\n    \"图像分类\",\n    \"目标检测\",\n    \"语义分割\",\n    \"实例分割\",\n    \"人脸识别\",\n    \"姿态估计\",\n    \"视频理解\"\n  ],\n  \"关键技术\": [\n    \"卷积神经网络（CNN）\",\n    \"深度学习\",\n    \"特征提取与匹配\",\n    \"三维视觉\",\n    \"多模态学习\"\n  ],\n  \"常用算法与模型\": [\n    \"ResNet\",\n    \"VGG\",\n    \"YOLO\",\n    \"Faster R-CNN\",\n    \"Vision Transformer（ViT）\"\n  ],\n  \"主要应用场景\": [\n    \"自动驾驶\",\n    \"医学影像分析\",\n    \"安防监控\",\n    \"工业质检\",\n    \"增强现实与虚拟现实\"\n  ],\n  \"常用工具与框架\": [\n    \"OpenCV\",\n    \"PyTorch\",\n    \"TensorFlow\",\n    \"Detectron2\"\n  ],\n  \"主要挑战\": [\n    \"数据标注成本高\",\n    \"模型泛化能力有限\",\n    \"对算力资源要求高\",\n    \"复杂场景理解困难\"\n  ],\n  \"发展趋势\": [\n    \"大模型与视觉基础模型\",\n    \"多模态融合\",\n    \"自监督与弱监督学习\",\n    \"边缘端视觉计算\"\n  ]\n}\n```"
  },
  {
    "instruction": "请用不少于250个字描述文本分类。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        250
      ]
    ],
    "topic": "文本分类",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "文本分类是自然语言处理领域中的一项基础而重要的任务，指的是根据文本所包含的内容、语义特征或实际应用需求，将文本自动划分到预先定义好的类别中。常见的分类对象包括新闻报道、学术论文、电子邮件、社交媒体评论、问卷反馈以及客服对话等，而分类标签可能是主题类别、情感倾向、用户意图、垃圾信息与否，或风险与合规等级。\n\n从技术角度来看，文本分类通常需要经历数据收集、文本预处理、特征表示、模型训练和结果评估等步骤。早期方法多依赖人工设计特征，如词频、TF-IDF、关键词和规则匹配，并结合朴素贝叶斯、支持向量机等传统机器学习算法。随着深度学习的发展，词向量、循环神经网络、卷积神经网络以及基于注意力机制的预训练语言模型被广泛应用，使模型能够更好地理解上下文语义和隐含信息。\n\n文本分类在实际应用中具有很高的价值，例如用于舆情分析、内容审核、智能推荐、搜索引擎优化和智能客服系统等。但同时也面临着类别不平衡、文本歧义、多语言场景以及领域迁移等挑战。因此，如何在保证准确率的同时提升模型的泛化能力和可解释性，仍然是文本分类研究和应用中的重要方向。"
  },
  {
    "instruction": "请用列表形式列出目标检测的4个要点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "列表"
      ]
    ],
    "topic": "目标检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. **特征提取**：从输入图像中提取多尺度、具有判别力的视觉特征。  \n2. **候选区域生成**：产生可能包含目标的候选框（如锚框或候选区域）。  \n3. **分类与边界框回归**：判断候选框内目标的类别，并精确回归其位置和大小。  \n4. **后处理**：通过非极大值抑制（NMS）等方法去除冗余框，得到最终检测结果。"
  },
  {
    "instruction": "在描述语音识别时，请确保'深度学习'这个词出现至少2次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "深度学习",
          2
        ]
      ]
    ],
    "topic": "语音识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "语音识别是将人类语音信号转化为可理解文本的技术，其核心在于对声音特征的建模与理解。近年来，随着计算能力的提升，深度学习在语音识别中发挥了关键作用，通过神经网络自动学习语音中的时序和语义特征，大幅提升了识别准确率。基于深度学习的方法还能适应不同口音和噪声环境，使现代语音助手和实时转写系统更加可靠。"
  },
  {
    "instruction": "描述语音识别的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "特别"
      ]
    ],
    "topic": "语音识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "语音识别的特点包括：\n\n- 以语音信号为输入，将声音转换为文本或指令。  \n- 结合声学模型、语言模型和解码算法完成识别过程。  \n- 支持连续语音与离散指令两种交互方式。  \n- 能处理不同说话人、口音和语速带来的差异。  \n- 可在联网或本地环境中运行，适用于多种设备。  \n- 需要对背景噪声进行处理，以提升识别稳定性。  \n- 可通过数据训练与更新，扩展词汇和应用场景。"
  },
  {
    "instruction": "请写5段关于提示工程的内容，每段以数字编号开头。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "提示工程",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. 提示工程是指通过精心设计输入提示，引导大语言模型产生更准确、有用和可控输出的一门实践，它强调人机交互中语言表达的重要性。  \n2. 提示工程的核心原则包括明确目标、提供足够上下文、设定角色和约束条件，从而减少模型歧义并提高响应一致性。  \n3. 常见的提示工程技巧有示例驱动（Few-shot）、逐步推理（Chain-of-Thought）、结构化输出要求以及反向提示等，用于应对不同复杂度的任务。  \n4. 提示工程是一个持续迭代的过程，需要通过测试、评估和调整提示来优化效果，并根据模型更新和任务变化不断改进。  \n5. 在实际应用中，提示工程还需要关注伦理与安全问题，避免引导模型生成有害内容，同时结合自动化工具提升提示设计的效率与可复用性。"
  },
  {
    "instruction": "写一篇关于注意力机制的短文，必须以问句结尾。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "注意力机制",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "注意力机制是一种模拟人类认知过程的重要方法，它通过为不同信息分配不同权重，使模型能够更加关注关键部分，从而提升处理复杂数据的能力。在自然语言处理、计算机视觉等领域，注意力机制帮助模型在海量信息中聚焦最有价值的内容，不仅提高了准确性，也增强了可解释性，那么注意力机制是否会在未来成为所有智能系统的核心结构呢？"
  },
  {
    "instruction": "在描述目标检测时，请确保'损失函数'这个词出现至少4次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "损失函数",
          4
        ]
      ]
    ],
    "topic": "目标检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "在目标检测任务中，模型需要同时预测目标的位置和类别，因此损失函数的设计至关重要。常见方法会将分类损失、定位回归损失和置信度损失组合成一个整体损失函数。为了平衡不同子任务，研究者会对损失函数中的各部分加权，使损失函数在训练早期和后期都能稳定收敛。例如在 YOLO 或 Faster R-CNN 中，改进损失函数可以显著提升小目标检测效果，而合适的损失函数也有助于缓解正负样本不平衡问题。"
  },
  {
    "instruction": "写一段关于分布式训练的文字，必须包含以下关键词：模型, 优化, 参数。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "损失函数",
          3
        ]
      ]
    ],
    "topic": "分布式训练",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "分布式训练通过将计算任务分散到多台设备上协同完成，可以显著提升大规模模型的训练效率，在这一过程中，各节点需要同步参数以保证结果一致性，同时借助高效的通信机制和并行策略实现整体优化，从而在保证模型性能的前提下缩短训练时间并提高资源利用率。"
  },
  {
    "instruction": "描述Transformer架构的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "极其"
      ]
    ],
    "topic": "Transformer架构",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "Transformer架构的主要特点包括：\n\n- 基于自注意力机制，建模序列中不同位置之间的依赖关系。  \n- 采用多头注意力结构，在多个子空间中并行处理信息。  \n- 引入位置编码，为模型提供序列顺序信息。  \n- 使用编码器与解码器的堆叠结构，整体设计具有模块化特征。  \n- 计算过程支持并行执行，降低对递归和卷积结构的依赖。  \n- 通过残差连接和层归一化机制，维持训练过程的稳定性。  \n- 可应用于多种序列建模与序列到序列任务。"
  },
  {
    "instruction": "用Markdown格式撰写关于情感分析的说明，必须包含标题和分点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "情感分析",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "```markdown\n# 情感分析简介\n\n- **定义**：情感分析（Sentiment Analysis）是自然语言处理中的一个重要任务，用于判断文本中表达的情绪倾向，如正面、负面或中性。  \n- **主要方法**：常见方法包括基于词典的方法、传统机器学习方法以及基于深度学习的模型。  \n- **应用场景**：广泛应用于舆情监测、产品评价分析、社交媒体分析和客户反馈处理等领域。  \n- **面临挑战**：情感表达的多样性、讽刺与隐喻的识别、多语言与领域适应性等问题仍然具有挑战性。\n```"
  },
  {
    "instruction": "用恰好50个字概括大语言模型的核心要点。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "exact_length",
        50
      ]
    ],
    "topic": "大语言模型",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "大语言模型以海量数据训练通过神经网络学习语言规律具备理解生成推理能力应用广泛但需安全治理持续演进发展"
  },
  {
    "instruction": "用正式的学术语言描述文本分类。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "文本分类",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "文本分类（text classification）是自然语言处理领域中的一项基础性研究任务，其目标是依据预先定义的类别体系，自动地将文本实例映射到一个或多个语义类别。该过程通常涉及对文本数据进行表示学习与特征建模，通过统计方法、传统机器学习算法或深度学习模型，捕捉文本在词汇、句法、语义及篇章层面的关键信息，从而实现对文本语义属性的判别。文本分类在理论上涉及模式识别、信息论与语言学等多学科交叉，在实践中被广泛应用于情感分析、主题标注、垃圾信息检测、舆情监测以及信息检索等场景，是推动文本信息自动化理解与处理的重要技术支撑。"
  },
  {
    "instruction": "写一段关于模型压缩的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "特别"
      ]
    ],
    "topic": "模型压缩",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "模型压缩研究通过剪枝、量化、蒸馏、秩分解等方法，对参数与计算进行处理。过程包括删除连接、替换表示、传递知识、重构矩阵。目标是在推理与存储场景中，控制参数数量与计算量，维持任务指标。应用涉及端侧部署、服务推理、训练流程整合。"
  },
  {
    "instruction": "描述提示工程，开头必须是'随着技术发展'。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "提示工程",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "随着技术发展，提示工程逐渐成为人机交互和人工智能应用中的重要方法。它指的是通过精心设计输入给模型的提示语，引导模型更准确、高效地生成期望的输出。提示工程不仅关注提问的内容，还包括表达方式、结构、上下文信息以及约束条件的设置。通过合理的提示设计，用户可以在无需修改模型本身的情况下，显著提升模型在理解问题、推理分析、内容生成等方面的表现，从而让人工智能更好地服务于实际需求。"
  },
  {
    "instruction": "用正式的学术语言描述生成对抗网络。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "生成对抗网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "生成对抗网络（Generative Adversarial Networks，GAN）是一类基于博弈论思想的深度生成模型，由生成器（Generator）与判别器（Discriminator）两部分构成。生成器旨在从随机噪声中学习潜在数据分布并生成逼真的样本，而判别器则试图区分真实数据与生成数据。二者通过最小—最大对抗目标函数进行联合训练，在相互博弈的过程中不断提升各自性能，最终使生成样本在统计意义上逼近真实数据分布。  \n作为一种重要的无监督与半监督学习框架，生成对抗网络在图像生成、数据增强、表示学习等领域展现出显著优势，但同时也面临训练不稳定、模式崩溃及评价指标不足等理论与实践挑战。"
  },
  {
    "instruction": "写一篇关于深度学习的短文，必须以问句结尾。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "深度学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "深度学习是人工智能领域的重要分支，它通过多层神经网络从海量数据中自动学习特征，在图像识别、语音处理和自然语言理解等方面取得了突破性进展。随着算力提升和数据规模扩大，模型结构不断演化，从卷积网络到Transformer，推动了技术的快速迭代。然而，深度学习也面临可解释性不足、能耗高和数据偏见等挑战，在未来的发展中，我们该如何在性能提升与可靠性、伦理之间取得平衡呢？"
  },
  {
    "instruction": "在描述代码生成时，请确保'推理'这个词出现至少3次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "推理",
          3
        ]
      ]
    ],
    "topic": "代码生成",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "代码生成通常从理解需求开始，系统会进行语义分析和约束检查，通过推理将自然语言目标映射为可执行结构。随后在设计阶段，模型利用规则与经验进行推理，选择合适的算法、数据结构和接口。生成代码后，还会进行静态分析与测试，用推理来发现边界情况和潜在缺陷。最后在优化阶段，持续的推理帮助改进性能、可读性与安全性。"
  },
  {
    "instruction": "以幽默风趣的方式介绍语音识别。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "语音识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "语音识别就像一位24小时待命的“听写小秘书”：你一张嘴，它就竖起耳朵，飞快把“哎呀我刚才说啥来着”变成清清楚楚的文字。它不喝咖啡、不打瞌睡，还能听懂普通话、方言，甚至你含着饼干说话的勇气。\n\n简单说，语音识别就是让机器学会“听人话”。从打字解放你的手指，到让手机、音箱、汽车乖乖听话，它的终极目标只有一个——当你懒得动手时，世界依然能乖乖运转。只要别对它大喊“你怎么还听不懂啊！”，它其实已经很努力了。"
  },
  {
    "instruction": "写一篇关于自监督学习的短文，必须以问句结尾。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "自监督学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "自监督学习是一种利用数据自身结构生成监督信号的机器学习范式，它通过预文本任务在无需人工标注的情况下学习有用表示。该方法在计算机视觉与自然语言处理中表现突出，能够显著降低标注成本并提升下游任务的泛化能力。随着大模型的发展，自监督学习是否会成为通用智能系统的核心训练方式？"
  },
  {
    "instruction": "在描述深度学习时，请确保'训练'这个词出现至少4次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "训练",
          4
        ]
      ]
    ],
    "topic": "深度学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "深度学习是一种基于多层神经网络的方法，其核心在于通过大量数据进行训练。模型在训练过程中不断调整参数，以最小化损失函数。有效的训练需要高质量的数据、合适的网络结构以及充足的计算资源。反复的训练可以提升模型的泛化能力，而不充分的训练则可能导致欠拟合。随着算法和硬件的发展，训练效率不断提高，使得深度学习在计算机视觉、语音识别和自然语言处理等领域取得了显著进展。"
  },
  {
    "instruction": "写一段关于注意力机制的文字，必须包含以下关键词：人工智能, 深度学习, 算法。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "算法",
          2
        ]
      ]
    ],
    "topic": "注意力机制",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "注意力机制是人工智能领域中的一项关键技术，它通过在深度学习模型中动态分配权重，使模型能够聚焦于对当前任务最重要的信息，从而提升表示能力和预测效果；这种机制本质上是一类高效的算法设计思想，已被广泛应用于自然语言处理、计算机视觉等方向，并显著推动了复杂模型在实际场景中的性能与可解释性。"
  },
  {
    "instruction": "介绍计算机视觉，但不要使用'特别'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "特别"
      ]
    ],
    "topic": "计算机视觉",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "计算机视觉是人工智能的重要分支，研究如何让计算机从图像和视频中获取信息、理解内容并做出判断。它融合了计算机科学、数学、统计学以及认知科学等领域的方法，通过算法让机器具备“看”和“理解”的能力。\n\n在实际应用中，计算机视觉被广泛用于人脸识别、自动驾驶、医学影像分析、工业检测和智能安防等场景。随着深度学习的发展，计算机在目标识别、场景理解和行为分析方面的表现不断提升，正在推动各行各业向更加智能和高效的方向发展。"
  },
  {
    "instruction": "写一段关于计算机视觉的文字，必须包含以下关键词：算法, 深度学习, 反向传播。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "推理",
          2
        ]
      ]
    ],
    "topic": "计算机视觉",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "计算机视觉是人工智能的重要分支，致力于让机器理解和分析图像与视频内容，其核心依赖于高效的算法设计与数据驱动的方法，其中深度学习通过构建多层神经网络自动提取特征，显著提升了识别与理解能力，而模型训练过程中常用的反向传播机制则用于不断调整参数、减少误差，从而使系统在复杂视觉任务中表现得更加准确和稳定。"
  },
  {
    "instruction": "用恰好150个字概括知识蒸馏的核心要点。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "exact_length",
        150
      ]
    ],
    "topic": "知识蒸馏",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "知识蒸馏是将大型模型的知识迁移到小模型的训练方法通过教师模型输出的软目标中间表示或特征分布指导学生模型学习以在保持性能的同时降低参数量计算量和延迟提升部署效率其关键在于温度系数损失函数设计数据选择与训练策略平衡准确性泛化性与稳定性适用于模型压缩加速与边缘应用场景并促进知识复用与迁移能力提升发展潜力大"
  },
  {
    "instruction": "用恰好150个字概括生成对抗网络的核心要点。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "exact_length",
        150
      ]
    ],
    "topic": "生成对抗网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "生成对抗网络由生成器与判别器组成通过博弈训练生成器学习数据分布判别器提升区分真伪能力二者交替优化在极小极大目标下收敛实现以假乱真的样本生成其优势在于无监督学习表达强大但训练不稳定易崩溃需平衡网络结构损失函数与训练策略以获得高质量结果并推动图像语音文本等领域创新应用发展研究实践持续深化价值潜力显著提升"
  },
  {
    "instruction": "写一段关于计算机视觉的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "非常"
      ]
    ],
    "topic": "计算机视觉",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "计算机视觉是计算机科学的一个研究方向，关注从图像和视频中获取信息。领域通过模型训练和数据处理，实现识别、分析、重建等任务，并在医疗、交通、制造、农业场景中应用。"
  },
  {
    "instruction": "在描述异常检测时，请确保'训练'这个词出现至少2次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "训练",
          2
        ]
      ]
    ],
    "topic": "异常检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "异常检测是一种用于识别数据中偏离正常模式行为的技术，常见于工业监控、金融风控和网络安全等领域。通常需要先收集大量正常样本进行模型训练，通过训练学习数据的统计特征或表示方式，在训练完成后再对新数据进行评估；当新数据与训练阶段学到的模式差异较大时，就会被判定为异常。"
  },
  {
    "instruction": "用正式的学术语言描述机器翻译。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "机器翻译",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "机器翻译（Machine Translation, MT）是计算语言学与人工智能领域的重要研究方向，旨在利用计算模型实现不同自然语言之间的自动转换。其核心目标是在保持语义等价、语法正确和语用适切的前提下，将源语言文本映射为目标语言表达。机器翻译方法经历了从基于规则的方法、基于统计的方法到基于神经网络的方法的发展演进，其中神经机器翻译通过端到端的深度学习模型显著提升了翻译质量。尽管如此，机器翻译在处理语义歧义、文化差异及复杂语境等方面仍面临挑战，其性能通常通过自动评价指标与人工评测相结合的方式进行衡量。"
  },
  {
    "instruction": "请用不少于300个字描述联邦学习。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        300
      ]
    ],
    "topic": "联邦学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "联邦学习是一种面向隐私保护和数据安全的分布式机器学习范式，它的核心思想是在数据不出本地的前提下完成模型的协同训练。与传统的集中式机器学习不同，联邦学习不需要将各参与方的数据集中上传到同一个服务器，而是让数据始终保留在本地设备或机构内部，从而在源头上降低数据泄露和滥用的风险。这种思想特别适合当前数据分散、隐私法规日益严格的应用环境。\n\n在联邦学习的基本流程中，中央服务器首先初始化一个全局模型，并将模型参数下发给各个参与方。每个参与方利用自身本地的数据对模型进行训练，只计算参数更新或梯度信息，而不暴露原始数据。随后，各参与方将本地训练得到的更新结果上传到服务器，服务器通过安全聚合等机制对这些更新进行整合，形成新的全局模型。这个过程会反复进行，直到模型收敛或达到预定的性能指标。\n\n联邦学习的优势不仅体现在隐私保护方面，还体现在数据利用效率和跨机构协作能力上。许多场景中，单个机构的数据规模有限或分布存在偏差，而通过联邦学习，可以在不打破数据壁垒的情况下实现“数据可用不可见”，从而训练出泛化能力更强的模型。此外，联邦学习还能减少大规模数据传输带来的通信成本和合规风险，在一定程度上提升系统的整体安全性。\n\n当然，联邦学习也面临着诸多挑战，例如参与方数据分布不均衡、通信效率低下、系统异构性强以及潜在的模型反推攻击等问题。为此，研究者提出了差分隐私、安全多方计算、同态加密等技术来增强联邦学习的安全性和鲁棒性。目前，联邦学习已在移动输入法、智能推荐、金融风控、医疗健康等领域得到应用，并被认为是未来隐私计算和可信人工智能的重要发展方向之一。"
  },
  {
    "instruction": "描述联邦学习，开头必须是'随着技术发展'。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "联邦学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "随着技术发展，联邦学习作为一种分布式机器学习范式逐渐受到关注。它的核心思想是在数据不出本地的前提下，通过在各参与方本地训练模型并仅共享模型参数或梯度，实现多方协同建模。这种方式既能充分利用分散在不同设备或机构中的数据价值，又能有效保护数据隐私、降低合规风险，因此在金融、医疗、智能终端等对隐私敏感的场景中具有重要应用前景。"
  },
  {
    "instruction": "用通俗易懂的语言向初学者解释RAG检索增强。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "RAG检索增强",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "RAG（检索增强生成，Retrieval-Augmented Generation）可以把它理解成：**让大模型“先查资料，再回答问题”**。\n\n用一个生活中的例子来解释：  \n你要回答一个不太熟的问题，如果**只靠记忆**，可能会答错；但如果你**先翻书、查资料**，再根据资料来回答，就会更准确。RAG做的就是这件事。\n\n具体来说，RAG分三步：\n\n1. **检索（查资料）**  \n   当你提问时，系统先从一堆资料（比如文档、数据库、网页）里，找出和问题最相关的内容。\n\n2. **增强（把资料给模型）**  \n   把刚刚找到的相关内容，一起“喂”给大模型，相当于给它一份参考材料。\n\n3. **生成（回答问题）**  \n   大模型根据你的问题 + 这些参考资料，生成一个更准确、更贴合事实的回答。\n\n为什么要用RAG？\n\n- **减少胡编乱造**：模型不只靠“记忆”，而是有真实资料支撑  \n- **能用最新数据**：不用重新训练模型，换资料就行  \n- **适合企业场景**：比如用公司内部文档来回答问题\n\n一句话总结：  \n**RAG就是给大模型配了一个“随时能查资料的助手”，让它回答得更准、更靠谱。**"
  },
  {
    "instruction": "写一段关于知识蒸馏的文字，字数要求在150到300之间。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        150
      ],
      [
        "max_length",
        300
      ]
    ],
    "topic": "知识蒸馏",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "知识蒸馏是一种模型压缩与迁移学习方法，核心思想是利用性能较强的教师模型，将其学到的知识传递给结构更简单的学生模型。教师模型不仅提供硬标签，还通过软标签、特征表示或中间层信息，指导学生模型学习更平滑、更具泛化性的决策边界。通过知识蒸馏，可以在保持较高精度的同时显著降低模型规模与计算成本，使深度学习模型更易于在移动端和边缘设备上部署。"
  },
  {
    "instruction": "请写4段关于自然语言处理的内容，每段以数字编号开头。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "自然语言处理",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. 自然语言处理（NLP）是人工智能的重要分支，研究计算机如何理解、生成和处理人类语言。它融合了语言学、计算机科学和统计学，使机器能够从文本和语音中获取意义。\n\n2. 在实际应用中，自然语言处理被广泛用于搜索引擎、机器翻译、语音助手和文本推荐系统。通过分词、句法分析和语义理解等技术，系统可以更准确地理解用户意图。\n\n3. 随着深度学习的发展，基于神经网络的模型显著提升了自然语言处理的效果，如预训练语言模型在问答、摘要和对话任务中表现突出。这些模型能够从大规模语料中学习复杂的语言规律。\n\n4. 未来，自然语言处理将更加注重多模态理解、低资源语言支持以及安全与可解释性。随着技术进步，人机交流将变得更加自然、高效和可信。"
  },
  {
    "instruction": "请用列表形式列出知识蒸馏的3个要点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "列表"
      ]
    ],
    "topic": "知识蒸馏",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. **教师-学生模型框架**：通过性能更强的教师模型指导较小的学生模型学习。  \n2. **软标签与温度参数**：利用教师模型输出的软标签（通常引入温度参数）传递更丰富的类别间信息。  \n3. **蒸馏损失函数设计**：将真实标签损失与蒸馏损失相结合，在保证精度的同时实现模型压缩与加速。"
  },
  {
    "instruction": "请用JSON格式输出注意力机制的关键信息。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "注意力机制",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "```json\n{\n  \"概念\": \"注意力机制是一种让模型在处理信息时动态关注最相关部分的方法，用于提升表示能力和建模效果。\",\n  \"核心组成\": {\n    \"Query\": \"查询向量，用于表示当前需要关注的信息\",\n    \"Key\": \"键向量，用于与Query计算相关性\",\n    \"Value\": \"值向量，包含实际被加权聚合的信息\"\n  },\n  \"计算流程\": [\n    \"计算Query与Key的相似度得分\",\n    \"对相似度得分进行归一化（通常使用Softmax）\",\n    \"使用归一化权重对Value进行加权求和\"\n  ],\n  \"数学公式\": {\n    \"相似度计算\": \"score(Q, K) = Q · K 或 Q · K / sqrt(d)\",\n    \"注意力输出\": \"Attention(Q, K, V) = softmax(score(Q, K)) · V\"\n  },\n  \"常见类型\": [\n    \"点积注意力\",\n    \"缩放点积注意力\",\n    \"加性注意力\",\n    \"自注意力（Self-Attention）\",\n    \"多头注意力（Multi-Head Attention）\"\n  ],\n  \"优点\": [\n    \"能够捕捉长距离依赖关系\",\n    \"提升模型对关键信息的关注能力\",\n    \"具有较好的可解释性\"\n  ],\n  \"应用场景\": [\n    \"机器翻译\",\n    \"文本摘要\",\n    \"语音识别\",\n    \"计算机视觉\",\n    \"多模态学习\"\n  ]\n}\n```"
  },
  {
    "instruction": "用Markdown格式撰写关于深度学习的说明，必须包含标题和分点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "深度学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "```markdown\n# 深度学习简介\n\n深度学习（Deep Learning）是机器学习的一个重要分支，主要基于人工神经网络模型。\n\n## 核心特点\n- **多层结构**：通过多层神经网络自动提取数据特征  \n- **端到端学习**：从原始数据直接学习到输出结果  \n- **大数据驱动**：通常需要大量数据进行训练  \n- **强表达能力**：在图像、语音和自然语言处理等领域表现突出  \n\n## 常见应用\n- 图像识别与计算机视觉  \n- 语音识别与语音合成  \n- 自然语言处理与机器翻译  \n- 推荐系统与智能决策  \n```"
  },
  {
    "instruction": "用简洁的技术文档风格说明模型量化。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "模型量化",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**模型量化（Model Quantization）**\n\n**定义**  \n模型量化是将神经网络中的高精度数值（如 FP32）转换为低精度表示（如 INT8/INT4），以降低计算与存储成本的技术。\n\n**目的**  \n- 减少模型体积  \n- 提升推理速度  \n- 降低功耗与硬件成本  \n- 便于边缘/移动端部署\n\n**常见类型**  \n- **后训练量化（PTQ）**：在训练完成后量化，实施简单，精度可能下降。  \n- **量化感知训练（QAT）**：训练时模拟量化误差，精度更高。  \n- **动态量化**：权重预量化，激活在推理时量化。  \n- **静态量化**：权重和激活均预先量化，需要校准数据。\n\n**量化粒度**  \n- **逐层（Per-Tensor）**  \n- **逐通道（Per-Channel）**（精度更优，常用于权重）\n\n**基本流程**  \n1. 选择量化策略与位宽  \n2. 校准/训练（PTQ 或 QAT）  \n3. 量化与模型导出  \n4. 部署与验证\n\n**优缺点**  \n- **优点**：高效、低延迟、低资源占用  \n- **缺点**：可能引入精度损失，对算子/硬件有依赖\n\n**适用场景**  \n- 实时推理  \n- 边缘计算与移动设备  \n- 大规模在线服务"
  },
  {
    "instruction": "介绍注意力机制，但不要使用'特别'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "特别"
      ]
    ],
    "topic": "注意力机制",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "注意力机制是一种用于提升模型信息处理能力的技术，核心思想是在处理输入时，根据任务需求为不同信息分配不同权重。这样，模型可以更关注与当前目标相关的部分，而不是对所有信息一视同仁。\n\n在实现上，注意力机制常通过“查询（Query）—键（Key）—值（Value）”的结构来计算相关性：用查询与各个键进行匹配，得到权重，再对对应的值进行加权求和，从而形成更有针对性的表示。这种方式让模型在长序列中依然能够捕捉重要依赖关系。\n\n注意力机制在自然语言处理、计算机视觉和语音识别等领域都有广泛应用，例如机器翻译、文本摘要和图像理解。它的引入显著提升了模型的表达能力与效果，也是 Transformer 等现代模型的核心组成部分。"
  },
  {
    "instruction": "请用JSON格式输出自监督学习的关键信息。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "自监督学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "```json\n{\n  \"定义\": \"自监督学习是一种无需人工标注数据、通过从数据自身构造监督信号来训练模型的学习范式。\",\n  \"核心思想\": \"利用数据内部结构或关系自动生成伪标签，通过预训练获得通用表征能力。\",\n  \"常见方法\": [\n    \"对比学习\",\n    \"掩码预测\",\n    \"自编码器\",\n    \"预测未来或缺失部分\"\n  ],\n  \"代表任务\": [\n    \"图像旋转预测\",\n    \"遮挡词/像素恢复\",\n    \"正负样本对比\",\n    \"时序顺序预测\"\n  ],\n  \"常用损失函数\": [\n    \"对比损失（Contrastive Loss）\",\n    \"InfoNCE\",\n    \"重构损失（MSE、交叉熵）\"\n  ],\n  \"优点\": [\n    \"减少对人工标注的依赖\",\n    \"充分利用海量无标注数据\",\n    \"提升下游任务性能\"\n  ],\n  \"挑战\": [\n    \"预训练任务设计困难\",\n    \"负样本选择敏感\",\n    \"计算资源消耗较大\"\n  ],\n  \"应用领域\": [\n    \"计算机视觉\",\n    \"自然语言处理\",\n    \"语音处理\",\n    \"推荐系统\"\n  ],\n  \"典型模型\": [\n    \"BERT\",\n    \"SimCLR\",\n    \"MoCo\",\n    \"BYOL\",\n    \"MAE\"\n  ]\n}\n```"
  },
  {
    "instruction": "在描述自然语言处理时，请确保'人工智能'这个词出现至少2次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "人工智能",
          2
        ]
      ]
    ],
    "topic": "自然语言处理",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "自然语言处理（NLP）是人工智能领域的重要分支，旨在让计算机理解、生成和处理人类语言。通过结合语言学、统计学和机器学习方法，人工智能系统可以完成文本分析、机器翻译、情感分析和对话系统等任务。随着深度学习的发展，人工智能在自然语言处理中的应用不断深化，使人机交互更加自然、高效。"
  },
  {
    "instruction": "撰写大语言模型的介绍，每段必须包含'算法'。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "算法",
          3
        ]
      ]
    ],
    "topic": "大语言模型",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "大语言模型是一类基于深度学习的人工智能系统，能够理解和生成自然语言文本。它们通常通过大规模语料进行训练，学习语言中的统计规律与语义结构，其核心在于高效的神经网络算法，使模型能够在不同任务中表现出较强的泛化能力。\n\n在技术层面，大语言模型多采用变换器架构，通过注意力机制来捕捉长距离依赖关系。这种设计依赖复杂而精密的算法，对输入文本进行编码、加权和解码，从而实现对上下文含义的准确建模。\n\n在应用方面，大语言模型已广泛用于对话系统、文本生成、代码辅助等场景。随着计算能力和数据规模的提升，相关算法也在不断演进，使模型在准确性、安全性和可控性方面持续改进。"
  },
  {
    "instruction": "用通俗易懂的语言向初学者解释联邦学习。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "联邦学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "联邦学习可以理解为一种**“大家一起学习，但不把作业交出来”**的学习方式。\n\n先从一个生活例子说起：  \n假设有很多手机在用同一个输入法。每个手机都会记录你常用的词，但这些数据都很私密，不能直接上传到服务器。那怎么让输入法整体变得更聪明呢？\n\n**联邦学习的做法是：**\n\n1. **数据不出门**  \n   每台手机的数据都留在本地，不上传原始数据，比如你的聊天内容、输入习惯。\n\n2. **在本地学习**  \n   服务器先发一个“基础模型”给每台手机。手机用自己的数据在本地训练这个模型，让它变得更适合自己。\n\n3. **只交“学习结果”**  \n   手机不会上传数据本身，而是把“模型学到了什么变化”（比如参数的改动）发回服务器。\n\n4. **集中变聪明**  \n   服务器把来自很多手机的这些改动汇总，更新出一个更聪明的整体模型。\n\n5. **再发回去**  \n   更新后的模型再发给所有手机，进入下一轮学习。\n\n**为什么要用联邦学习？**\n\n- ✅ **保护隐私**：数据不集中上传，更安全  \n- ✅ **利用分散数据**：很多数据本来就存在不同设备上  \n- ✅ **效果更好**：大家一起学，模型进步更快\n\n**一句话总结：**  \n联邦学习就是——**数据留在本地，模型一起进步**。"
  },
  {
    "instruction": "描述计算机视觉的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "非常"
      ]
    ],
    "topic": "计算机视觉",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "计算机视觉的特点包括：\n\n- 以图像和视频为主要数据来源，结合几何、统计与学习方法，完成感知与理解任务。  \n- 支持目标检测、识别、分割、跟踪与三维重建等功能。  \n- 对光照、视角、遮挡和噪声变化具备鲁棒性。  \n- 依赖规模化数据与计算资源，常与深度学习模型结合。  \n- 可在医疗、工业、交通、安防和机器人等领域落地应用。  \n- 强调实时性、准确性与可解释性之间的权衡。"
  },
  {
    "instruction": "以问答形式介绍注意力机制，包含至少5个问题和回答。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "注意力机制",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**问1：什么是注意力机制（Attention Mechanism）？**  \n**答：**注意力机制是一种让模型在处理信息时，能够“关注”更重要部分的技术。它通过为不同输入分配不同权重，使模型重点利用关键信息，而不是平均对待所有输入。\n\n**问2：为什么需要注意力机制？**  \n**答：**在处理长序列或复杂数据时（如长句子、图像），传统模型容易丢失关键信息。注意力机制可以缓解这一问题，让模型动态聚焦于与当前任务最相关的部分，从而提升性能。\n\n**问3：注意力机制的基本原理是什么？**  \n**答：**其核心是计算“相关性”。模型会根据查询（Query）与键（Key）的匹配程度，得到权重，再用这些权重对值（Value）进行加权求和，最终得到更有信息量的表示。\n\n**问4：常见的注意力机制有哪些类型？**  \n**答：**常见类型包括软注意力（Soft Attention）、硬注意力（Hard Attention）、自注意力（Self-Attention）以及多头注意力（Multi-Head Attention）等。\n\n**问5：什么是自注意力机制？**  \n**答：**自注意力是一种特殊的注意力机制，查询、键和值都来自同一序列。它可以建模序列内部各元素之间的依赖关系，是Transformer模型的核心组件。\n\n**问6：注意力机制主要应用在哪些领域？**  \n**答：**注意力机制广泛应用于自然语言处理（如机器翻译、文本摘要）、计算机视觉（如图像识别）、语音识别以及多模态学习等领域。"
  },
  {
    "instruction": "用正式的学术语言描述自监督学习。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "自监督学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "自监督学习（Self-Supervised Learning）是一种介于无监督学习与监督学习之间的机器学习范式，其核心思想是在不依赖人工标注数据的前提下，从原始数据本身构造监督信号以学习具有判别性和泛化能力的表示。该方法通常通过设计预训练任务（pretext tasks），如对比学习、掩码预测或数据重构等，使模型在大规模未标注数据上捕获潜在的结构性特征与统计规律。随后，所学习到的表示可迁移至下游任务，并通过少量标注数据进行微调，从而显著降低对人工标注成本的依赖。近年来，自监督学习在计算机视觉、自然语言处理和语音处理等领域展现出卓越性能，被广泛视为构建通用表征学习模型的重要途径。"
  },
  {
    "instruction": "写一段关于语音识别的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "很"
      ]
    ],
    "topic": "语音识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "语音识别是计算领域中的一种技术。系统通过采集语音信号，进行分析与处理，将语音转写为文本，并支持搜索、记录和交互等功能。该过程依赖模型、数据和算法的训练与更新。"
  },
  {
    "instruction": "撰写代码生成的介绍，每段必须包含'参数'。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "参数",
          2
        ]
      ]
    ],
    "topic": "代码生成",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "代码生成是指通过工具或模型根据需求描述自动产出可运行代码的过程，它能够提升开发效率、减少重复劳动，并降低出错率。在这一过程中，输入的参数起到关键作用，不同的参数配置会直接影响生成代码的结构、风格与功能覆盖范围。\n\n在实际应用中，代码生成系统通常允许开发者灵活调整参数，例如目标语言、框架约束、性能偏好或安全要求等，通过合理设置这些参数，可以让生成结果更贴合业务场景，从而更好地服务于软件工程实践。"
  },
  {
    "instruction": "在描述自然语言处理时，请确保'梯度下降'这个词出现至少3次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "梯度下降",
          3
        ]
      ]
    ],
    "topic": "自然语言处理",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "自然语言处理（NLP）是研究计算机如何理解、生成和推理人类语言的技术领域，广泛应用于机器翻译、情感分析和对话系统。在现代NLP中，深度学习模型通过大规模语料进行训练，常用的优化方法是梯度下降，以最小化损失函数并提升模型性能。无论是词向量学习、序列到序列模型，还是大型语言模型的微调过程，梯度下降都扮演着核心角色；在实际训练中，还会使用随机梯度下降等变体来提高效率和稳定性。可以说，从参数更新到模型收敛，梯度下降贯穿了自然语言处理模型训练的全过程。"
  },
  {
    "instruction": "写一段关于模型压缩的文字，必须包含以下关键词：推理, 反向传播, 深度学习。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "参数",
          2
        ]
      ]
    ],
    "topic": "模型压缩",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "模型压缩是深度学习领域中的一项重要技术，旨在在尽量保持模型性能的前提下降低参数规模和计算复杂度，从而提升模型在实际应用中的效率。在训练阶段，模型仍然依赖反向传播来更新参数，但可以通过剪枝、量化或知识蒸馏等方法减少冗余信息；而在部署阶段，经过压缩的模型能够以更低的资源消耗完成推理任务，使其更适合在边缘设备或低功耗场景中运行。"
  },
  {
    "instruction": "用Markdown格式撰写关于边缘计算的说明，必须包含标题和分点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "边缘计算",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "```markdown\n# 边缘计算（Edge Computing）说明\n\n- **定义**：边缘计算是一种将数据处理和计算能力从集中式云端下沉到网络边缘（靠近数据产生源头）的计算架构。\n- **核心特点**：\n  - 低时延：数据在本地或近端处理，减少网络传输延迟。\n  - 高效率：降低对中心云的带宽依赖，提升整体系统响应速度。\n  - 高可靠性：即使网络不稳定，边缘节点也可独立运行。\n- **典型应用场景**：\n  - 物联网（IoT）设备的数据实时处理\n  - 自动驾驶与智能交通系统\n  - 工业互联网与智能制造\n  - 视频监控与智能安防\n- **与云计算的关系**：\n  - 边缘计算并非取代云计算，而是与云计算协同工作，各自承担不同层级的计算任务。\n- **主要优势**：\n  - 提升用户体验\n  - 加强数据隐私与安全\n  - 支持实时性要求高的应用\n```"
  },
  {
    "instruction": "介绍分布式训练，但不要使用'特别'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "特别"
      ]
    ],
    "topic": "分布式训练",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "分布式训练是指在多台计算设备（如多台服务器或多块 GPU）上协同完成同一个模型的训练过程。随着数据规模和模型参数量不断增长，单机计算在时间和资源上往往难以满足需求，于是通过分布式方式将计算压力分摊到多个节点上，从而加快训练速度并提升可扩展性。\n\n在实现方式上，分布式训练常见的思路包括数据并行、模型并行和流水线并行。数据并行是将训练数据划分给不同节点，每个节点运行相同的模型，再通过通信机制同步参数；模型并行则是把模型结构拆分到不同设备上，适合超大模型；流水线并行结合了时间维度上的调度，使不同阶段同时运行。参数同步通常依赖参数服务器或 AllReduce 等通信策略。\n\n分布式训练在提升效率的同时，也带来了通信开销、系统复杂度和一致性控制等挑战。因此，在实际应用中需要在硬件条件、网络带宽和算法设计之间进行权衡，以获得稳定而高效的训练效果。"
  },
  {
    "instruction": "写一段关于模型压缩的文字，必须包含以下关键词：推理, 深度学习, 参数。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "梯度下降",
          3
        ]
      ]
    ],
    "topic": "模型压缩",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "模型压缩是指在保证模型性能尽量不下降的前提下，通过剪枝、量化、蒸馏等方法减少模型规模的技术，在深度学习中尤为重要，因为它可以显著降低模型的参数数量，从而提升推理速度、减少内存和能耗，使模型更适合在移动端和边缘设备上部署。"
  },
  {
    "instruction": "在描述机器学习时，请确保'反向传播'这个词出现至少3次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "反向传播",
          3
        ]
      ]
    ],
    "topic": "机器学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "机器学习是一种让计算机从数据中学习规律的方法，常见于监督学习和深度学习。在训练神经网络时，反向传播用于计算损失函数对参数的梯度，从而更新权重。通过反向传播，模型能够逐层调整参数；配合优化算法，反向传播使模型不断降低误差并提升性能。"
  },
  {
    "instruction": "请用列表形式列出代码生成的5个要点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "列表"
      ]
    ],
    "topic": "代码生成",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. 明确需求与输入输出：充分理解业务逻辑、接口规范和边界条件。  \n2. 正确性与可靠性：保证逻辑正确，处理异常与错误情况。  \n3. 可读性与可维护性：代码结构清晰，命名规范，注释必要。  \n4. 性能与效率：选择合适的算法与数据结构，避免不必要的开销。  \n5. 安全与测试：防范常见安全风险，并配合单元测试与验证。"
  },
  {
    "instruction": "介绍深度学习，但不要使用'特别'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "特别"
      ]
    ],
    "topic": "深度学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "深度学习是机器学习的重要分支，核心是由多层神经网络构成的模型。它通过从大量数据中自动学习特征表示，逐层提取更抽象的信息，常见结构包括卷积神经网络、循环神经网络和变换器。模型通常利用反向传播和梯度下降进行训练，以最小化预测误差。\n\n在图像识别、语音识别、自然语言处理和推荐系统等领域，深度学习取得了显著进展。其优势在于端到端学习和强大的表示能力，但也面临数据需求高、计算资源消耗大以及可解释性不足等挑战。随着算法、算力和数据的持续发展，深度学习正推动人工智能不断演进。"
  },
  {
    "instruction": "以幽默风趣的方式介绍分布式训练。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "分布式训练",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "想象一下，你要教一只“大模型”读书，但这本书厚到能当健身器材。如果只让一个人（单机训练）来读，读到一半人就先“过拟合”累趴了。这时候，**分布式训练**登场了——它就像把这本书撕成几份，叫上一群学霸朋友一起读。\n\n在分布式训练里，每个小伙伴都是一块“算力砖”。有人负责读前言，有人负责啃公式，还有人专攻附录。大家一边读一边记笔记（算梯度），然后通过电话、微信、甚至“心灵感应”（网络通信）把笔记汇总到班长那里（参数服务器或 AllReduce）。班长一合并，嘿，全班的知识水平瞬间同步提升！\n\n当然，集体学习也有小插曲：  \n- 有人网络卡了，像“老师我这页没加载出来”；  \n- 有人算得太快，等别人时开始摸鱼；  \n- 还有人不小心算错，被大家温柔地“梯度纠正”。\n\n但总体来说，分布式训练的魅力就在于：**人多力量大，模型学得快**。当单机还在慢悠悠翻页时，分布式训练已经把书读完，顺便还写了篇读后感。用一句话总结：  \n> 不怕模型大，就怕朋友少。朋友一多，训练起飞！"
  },
  {
    "instruction": "用通俗易懂的语言向初学者解释神经网络。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "神经网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "神经网络可以把它想象成一个**会学习的“自动算题机”**，灵感来自人脑，但其实没那么神秘。\n\n**1. 它在做什么？**  \n神经网络的任务是：**把输入变成有用的输出**。  \n比如：  \n- 输入一张图片 → 输出“这是猫还是狗”  \n- 输入一句话 → 输出翻译后的句子  \n- 输入历史数据 → 输出对未来的预测  \n\n**2. 它是怎么组成的？**  \n神经网络由很多“神经元”组成，你可以把每个神经元理解为一个**小计算器**。  \n这些神经元通常分成三层：  \n- **输入层**：接收信息（比如图片的像素、数字、文字）  \n- **隐藏层**：中间的处理过程，负责提取特征（这是最“聪明”的部分）  \n- **输出层**：给出结果（比如答案或预测）\n\n**3. 神经元在干嘛？**  \n每个神经元会做三件事：  \n1. 接收输入  \n2. 给不同输入分配不同的重要程度（叫“权重”）  \n3. 算出一个结果，再传给下一个神经元  \n\n就像做决定时，你会更重视某些信息，忽略另一些。\n\n**4. 它是怎么“学会”的？**  \n神经网络通过**反复练习**来学习：  \n- 先随便给个答案  \n- 把答案和正确答案对比，看看错了多少  \n- 调整内部的“权重”，让下次错得更少  \n\n这个过程重复成千上万次，就像做题订正，**越练越准**。\n\n**5. 一句话总结**  \n> 神经网络就是一个由很多小计算器组成的系统，通过大量练习，学会把输入变成正确输出。  \n\n不需要懂太多数学，也能理解它的核心思想：**不断试错 + 不断改进 = 学会**。"
  },
  {
    "instruction": "撰写提示工程的介绍，每段必须包含'反向传播'。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "反向传播",
          4
        ]
      ]
    ],
    "topic": "提示工程",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "提示工程是指通过精心设计和优化提示语，引导大语言模型产生更高质量、更可控输出的方法论。它关注指令结构、上下文设置与示例选择等要素，虽然不直接修改模型参数，但其思路常被类比为在推理阶段进行“软调参”，这一过程常被拿来与训练阶段的反向传播作对比。\n\n在实践中，提示工程强调迭代试验与效果评估，通过不断调整提示来逼近目标输出。与依赖梯度更新的反向传播不同，提示工程是在不训练模型的前提下实现性能提升，但二者都体现了“误差—调整—改进”的优化思想，因此理解反向传播有助于更好把握提示工程的底层逻辑。"
  },
  {
    "instruction": "写一段关于分布式训练的文字，必须包含以下关键词：优化, 梯度下降, 特征提取。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "模型",
          3
        ]
      ]
    ],
    "topic": "分布式训练",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "分布式训练通过将模型和数据划分到多台计算节点上并行计算，大幅提升了大规模模型的训练效率，在这一过程中，各节点协同完成特征提取并汇总结果，通过通信机制共享参数和梯度，从而在全局范围内进行优化，而基于梯度下降的方法则用于不断更新模型参数，使模型在多轮迭代中逐步收敛并获得更好的性能。"
  },
  {
    "instruction": "写一段关于边缘计算的文字，字数要求在300到450之间。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        300
      ],
      [
        "max_length",
        450
      ]
    ],
    "topic": "边缘计算",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "边缘计算是将数据处理和分析能力从传统的云中心下沉到网络边缘的一种计算模式，通常部署在靠近"
  },
  {
    "instruction": "请写5段关于代码生成的内容，每段以数字编号开头。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "代码生成",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. 代码生成是指通过工具或模型自动产出源代码的过程，它可以根据需求描述、模板或示例生成可执行的程序，从而减少人工编码的工作量。  \n2. 在软件工程中，代码生成常用于样板代码、接口定义和配置文件的创建，有助于提升开发效率并降低人为错误的概率。  \n3. 随着人工智能的发展，基于大模型的代码生成能够理解自然语言需求，并生成多种编程语言的代码，拓宽了开发者的工作方式。  \n4. 代码生成并非完全替代人工开发，它更适合作为辅助工具，需要开发者进行审查、调试和优化，以保证代码质量和安全性。  \n5. 未来，代码生成可能与低代码平台、自动化测试和持续集成深度结合，推动软件开发向更高层次的自动化与智能化发展。"
  },
  {
    "instruction": "描述异常检测，开头必须是'在当今时代'。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "异常检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "在当今时代，异常检测是数据分析与智能系统中的关键技术，旨在从大量数据中识别出与正常模式显著不同的行为或事件。它广泛应用于金融风控、网络安全、工业监测、医疗健康等领域，通过统计方法、机器学习或深度学习模型，自动发现潜在风险、故障或欺诈行为，从而帮助组织及时预警、降低损失并提升系统的可靠性与决策效率。"
  },
  {
    "instruction": "请写5段关于代码生成的内容，每段以数字编号开头。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "代码生成",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. 代码生成是指通过工具或算法自动产生源代码的过程，它能够根据模板、模型或自然语言描述生成可执行的程序，从而减少人工编写代码的时间和错误率。\n\n2. 在软件工程中，代码生成常用于重复性高的场景，例如数据访问层、接口定义或配置文件的创建，这使得开发者可以将精力更多地放在业务逻辑和系统设计上。\n\n3. 随着人工智能的发展，基于大模型的代码生成逐渐成熟，这类系统能够理解上下文、遵循编程规范，并生成接近人类水平的代码，甚至还能解释和重构已有代码。\n\n4. 代码生成也面临一些挑战，例如生成代码的可维护性、性能优化以及安全性问题，如果缺乏有效的审查机制，自动生成的代码可能引入隐蔽的缺陷。\n\n5. 总体来看，代码生成正在成为现代开发流程中的重要组成部分，合理地将其与人工开发结合，可以显著提升开发效率并推动软件工程方法的持续演进。"
  },
  {
    "instruction": "请用JSON格式输出自监督学习的关键信息。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "自监督学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "```json\n{\n  \"名称\": \"自监督学习\",\n  \"定义\": \"一种无需人工标注，通过从数据自身构造监督信号来学习表示的机器学习方法。\",\n  \"核心思想\": \"利用数据内在结构或预设任务生成伪标签，从而进行监督式训练。\",\n  \"主要特点\": [\n    \"无需人工标注\",\n    \"可利用大规模无标签数据\",\n    \"学习通用特征表示\"\n  ],\n  \"常见方法\": [\n    \"对比学习\",\n    \"预训练-微调\",\n    \"预测缺失信息\",\n    \"生成式建模\"\n  ],\n  \"典型任务\": [\n    \"图像旋转预测\",\n    \"遮挡区域恢复\",\n    \"上下文预测\",\n    \"时间顺序预测\"\n  ],\n  \"优势\": [\n    \"降低标注成本\",\n    \"提升模型泛化能力\",\n    \"适用于多种模态数据\"\n  ],\n  \"挑战\": [\n    \"预任务设计困难\",\n    \"训练不稳定\",\n    \"评估标准不统一\"\n  ],\n  \"应用领域\": [\n    \"计算机视觉\",\n    \"自然语言处理\",\n    \"语音处理\",\n    \"推荐系统\"\n  ],\n  \"代表模型\": [\n    \"SimCLR\",\n    \"MoCo\",\n    \"BYOL\",\n    \"BERT\"\n  ]\n}\n```"
  },
  {
    "instruction": "用正式的学术语言描述边缘计算。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "边缘计算",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "边缘计算（Edge Computing）是一种分布式计算范式，其核心思想是将数据处理、存储与分析能力从集中式云计算中心下沉至网络边缘，即靠近数据产生源或终端设备的位置。通过在边缘节点（如网关、基站、路由器或智能终端）执行计算任务，边缘计算能够有效降低数据传输时延，减少对核心网络带宽的依赖，并提升系统的实时性与可靠性。该范式在满足低时延、高并发和高可靠性需求方面具有显著优势，因而在物联网、智能制造、自动驾驶、智慧城市等应用场景中展现出重要价值。同时，边缘计算也面临着资源受限、系统异构性强、安全与隐私保护复杂等挑战，亟需在体系结构设计、资源调度、协同计算与安全机制等方面开展深入研究。"
  },
  {
    "instruction": "以幽默风趣的方式介绍计算机视觉。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "计算机视觉",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "如果你觉得计算机只是冷冰冰地算数，那计算机视觉一定会让它“开眼看世界”。简单说，计算机视觉就是教电脑学会“看”。只不过它没有眼睛，只能靠摄像头；没有大脑，只能靠算法；而且还特别较真——你看的是“猫咪好可爱”，它看到的是“像素矩阵加概率分布”。\n\n在计算机视觉的世界里，一张照片不是照片，而是一堆数字；一只狗不是狗，而是“疑似毛茸茸目标，置信度87%”。我们人类看红绿灯是为了过马路，计算机看红绿灯是为了不把你送进修车厂。于是工程师们天天在教电脑：这是人脸，不是月饼；这是斑马线，不是真的斑马。\n\n总之，计算机视觉就是一门让机器努力“装作自己看得懂世界”的技术。虽然它偶尔会把猫认成面包，把你认成陌生人，但别笑——至少它一直在努力睁大“电子眼”，争取有一天，比你还会找东西。"
  },
  {
    "instruction": "写一段关于大语言模型的文字，必须包含以下关键词：算法, 梯度下降, 特征提取。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "推理",
          3
        ]
      ]
    ],
    "topic": "大语言模型",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "大语言模型是一类基于深度学习的人工智能系统，其核心依赖复杂的算法来处理和生成自然语言，在训练过程中通过梯度下降不断优化参数以最小化损失函数，同时模型会自动完成从海量文本中进行特征提取，从而学习语义、语法和上下文关系，最终实现高质量的语言理解与生成能力。"
  },
  {
    "instruction": "请用JSON格式输出文本分类的关键信息。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "文本分类",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "```json\n{\n  \"文本\": \"待分类的原始文本内容\",\n  \"预测类别\": \"类别名称\",\n  \"置信度\": 0.95,\n  \"关键词\": [\"关键词1\", \"关键词2\", \"关键词3\"],\n  \"分类说明\": \"简要说明分类依据\",\n  \"模型信息\": {\n    \"模型名称\": \"模型名称\",\n    \"版本\": \"版本号\"\n  },\n  \"时间戳\": \"2026-02-02T00:00:00Z\"\n}\n```"
  },
  {
    "instruction": "写一篇关于Transformer架构的短文，必须以问句结尾。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "Transformer架构",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "Transformer架构是一种以自注意力机制为核心的深度学习模型，最早被提出用于自然语言处理任务。它通过编码器和解码器堆叠结构，在不依赖循环或卷积的情况下高效建模序列中各位置之间的关系，从而实现高度并行计算和更强的长程依赖捕捉能力。多头注意力、位置编码以及前馈网络共同构成了其强大的表达能力，使其成为现代大模型的基础，那么Transformer为何能在如此多的领域中取得成功并持续演化呢？"
  },
  {
    "instruction": "用Markdown格式撰写关于机器翻译的说明，必须包含标题和分点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "机器翻译",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "# 机器翻译（Machine Translation）说明\n\n- **定义**：机器翻译是利用计算机算法将一种自然语言自动转换为另一种自然语言的技术。\n- **主要类型**：\n  - 规则驱动的机器翻译（RBMT）\n  - 统计机器翻译（SMT）\n  - 神经机器翻译（NMT）\n- **工作原理**：\n  - 分析源语言的语法与语义\n  - 建立跨语言对应关系\n  - 生成目标语言文本\n- **应用场景**：\n  - 跨语言交流与国际化内容发布\n  - 在线翻译工具与浏览器插件\n  - 多语言客服与文档本地化\n- **优点与局限**：\n  - 优点：速度快、成本低、可扩展性强\n  - 局限：对语境、文化差异和专业领域的理解仍有限"
  },
  {
    "instruction": "以问答形式介绍情感分析，包含至少5个问题和回答。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "情感分析",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**问1：什么是情感分析？**  \n**答：**情感分析（Sentiment Analysis）是自然语言处理中的一项技术，用于判断文本中所表达的情绪或态度，通常分为正面、负面和中性。\n\n**问2：情感分析主要应用在哪些场景？**  \n**答：**常见应用包括产品评论分析、社交媒体舆情监测、客户满意度调查、品牌声誉管理以及客服对话分析等。\n\n**问3：情感分析是如何工作的？**  \n**答：**情感分析通常通过对文本进行分词、特征提取，然后使用规则方法或机器学习/深度学习模型来判断情感倾向。\n\n**问4：情感分析有哪些常见方法？**  \n**答：**主要方法包括基于词典的方法、传统机器学习方法（如SVM、朴素贝叶斯）以及深度学习方法（如LSTM、Transformer模型）。\n\n**问5：情感分析面临哪些挑战？**  \n**答：**主要挑战包括讽刺和反讽的识别、多义词理解、领域差异、口语化表达以及多情绪混合的问题。\n\n**问6：情感分析的发展趋势是什么？**  \n**答：**未来情感分析将更加注重细粒度情感、多模态分析（结合文本、语音、图像）以及跨语言和跨领域的泛化能力。"
  },
  {
    "instruction": "描述目标检测，开头必须是'在当今时代'。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "目标检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "在当今时代，目标检测作为计算机视觉领域的核心技术之一，旨在从图像或视频中自动识别并定位特定目标。它通常通过深度学习模型对输入数据进行特征提取、分类与回归，从而同时给出目标的类别和边界框位置。目标检测被广泛应用于自动驾驶、安防监控、医疗影像、工业质检等场景，不仅提升了系统的感知能力，也推动了智能化应用的发展。"
  },
  {
    "instruction": "在描述图像分割时，请确保'推理'这个词出现至少2次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "推理",
          2
        ]
      ]
    ],
    "topic": "图像分割",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "图像分割是一种将图像划分为具有语义意义区域的计算机视觉技术，常用于医学影像、自动驾驶和遥感分析等场景。其核心流程通常包括特征提取、模型训练以及推理阶段，在推理过程中，模型根据已学到的特征对每个像素进行分类判断。高质量的图像分割不仅依赖于数据和模型结构，还依赖于高效稳定的推理策略，通过多次推理优化可以提升边界精度和整体鲁棒性。"
  },
  {
    "instruction": "请用不少于300个字描述命名实体识别。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        300
      ]
    ],
    "topic": "命名实体识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "命名实体识别（Named Entity Recognition，简称NER）是自然语言处理领域中的一项基础而关键的技术，其主要目标是从非结构化的文本中自动识别并抽取具有特定语义类别的实体信息。所谓命名实体，通常指的是现实世界中具有唯一指代意义的对象，例如人名、地名、机构名、时间、日期、货币、数量，以及在特定领域中具有重要意义的专业术语或概念。通过命名实体识别，计算机能够在一定程度上理解文本中“谁”“在什么地方”“做了什么事情”，从而为更高层次的语义分析提供支撑。\n\n从技术发展角度来看，命名实体识别经历了从基于规则的方法、统计学习方法到深度学习方法的演进。早期方法主要依赖人工编写的规则和词典，这类方法可解释性强，但维护成本高、泛化能力有限。随后，隐马尔可夫模型、条件随机场等统计模型被广泛应用，它们能够通过特征工程从标注数据中学习实体边界和类别。近年来，随着深度学习的发展，基于循环神经网络、卷积神经网络以及Transformer架构的模型成为主流，尤其是结合预训练语言模型的NER系统，在准确率和鲁棒性方面取得了显著提升。\n\n命名实体识别在实际应用中具有广泛价值。在信息抽取、知识图谱构建、搜索引擎、智能问答、舆情分析以及医疗、金融等垂直领域中，NER都是不可或缺的基础模块。例如，在新闻分析中识别人名和机构名，有助于快速理解事件主体；在医疗文本中识别疾病和药物实体，可以辅助临床决策和科研分析。总体而言，命名实体识别不仅是连接自然语言与结构化知识的重要桥梁，也是推动人工智能理解人类语言的重要组成部分。"
  },
  {
    "instruction": "请用列表形式列出联邦学习的4个要点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "列表"
      ]
    ],
    "topic": "联邦学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. **数据不出本地**：训练过程中原始数据保留在本地设备或机构，不集中上传，保护隐私。  \n2. **协同建模**：多方在不共享数据的情况下，通过共享模型参数或梯度共同训练模型。  \n3. **参数聚合机制**：中心服务器或去中心化机制对各方上传的参数进行聚合（如加权平均）。  \n4. **隐私与安全保障**：结合加密、差分隐私等技术，降低信息泄露和攻击风险。"
  },
  {
    "instruction": "在描述情感分析时，请确保'算法'这个词出现至少4次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "算法",
          4
        ]
      ]
    ],
    "topic": "情感分析",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "情感分析是一种从文本中识别主观情绪和态度的技术，常用于舆情监测和用户反馈理解。其核心在于选择合适的算法，对文本进行特征提取、情绪分类与强度判断。传统方法依赖规则和统计算法，而现代方法更多采用机器学习和深度学习算法，通过大规模数据训练提升准确率。无论采用哪种路线，算法的设计、评估与迭代都直接决定情感分析的效果。"
  },
  {
    "instruction": "以问答形式介绍异常检测，包含至少5个问题和回答。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "异常检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**问：什么是异常检测？**  \n答：异常检测是指从数据中识别出与大多数正常数据显著不同的样本或行为的过程，这些异常可能代表错误、风险、欺诈或潜在的重要事件。\n\n**问：异常检测常见的应用场景有哪些？**  \n答：异常检测广泛应用于金融欺诈检测、网络入侵检测、设备故障预警、医疗健康监测以及工业质量控制等领域。\n\n**问：异常检测的方法主要有哪些类型？**  \n答：常见方法包括基于统计的方法、基于距离或密度的方法（如KNN、LOF）、基于模型的方法（如孤立森林、One-Class SVM）以及基于深度学习的方法（如自编码器）。\n\n**问：异常检测与分类有什么区别？**  \n答：分类通常需要大量已标注的正常和异常样本，而异常检测往往在异常样本稀少或没有标签的情况下进行，更关注发现“不同寻常”的数据。\n\n**问：进行异常检测时面临哪些挑战？**  \n答：主要挑战包括异常样本稀缺、异常定义模糊、数据分布复杂以及误报和漏报之间的平衡问题。\n\n**问：如何评估异常检测模型的效果？**  \n答：常用评估方式包括使用精确率、召回率、F1值、ROC曲线等指标，在有标签数据的情况下也可结合实际业务影响进行评估。"
  },
  {
    "instruction": "描述命名实体识别的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "极其"
      ]
    ],
    "topic": "命名实体识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "- 任务目标：从文本中识别并分类人名、地名、组织机构等实体。  \n- 依赖上下文：同一词在不同语境下可能属于不同实体类别。  \n- 标注粒度：关注实体边界与类别标签的一致性。  \n- 方法多样：规则方法、统计学习、神经网络方法均可用于该任务。  \n- 领域敏感：不同领域的词汇与表达差异会影响识别效果。  \n- 数据需求：需要标注语料用于训练与评估。  \n- 应用广泛：信息抽取、问答系统、知识图谱构建等场景。"
  },
  {
    "instruction": "请用不少于200个字描述命名实体识别。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        200
      ]
    ],
    "topic": "命名实体识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "命名实体识别（Named Entity Recognition，简称NER）是自然语言处理领域中的一项基础而关键的技术，其目标是从非结构化的文本中自动识别并抽取具有特定意义的实体信息，例如人名、地名、组织机构名、时间、日期、数值以及专有名词等。通过对文本进行实体边界的定位和实体类别的判定，NER能够将杂乱的文本信息转化为结构化数据，为后续的语义理解和知识建模提供基础。\n\n在技术发展上，命名实体识别经历了从基于规则和词典的方法，到基于统计学习的方法，再到以深度学习为核心的方法演进过程。早期方法依赖人工编写规则，维护成本高、泛化能力弱；随后出现了隐马尔可夫模型、条件随机场等统计模型；近年来，基于神经网络和预训练语言模型的方法显著提升了识别效果。命名实体识别被广泛应用于信息检索、机器翻译、问答系统、知识图谱构建以及舆情分析等场景，但在歧义消解、跨领域迁移和低资源语言处理方面仍面临挑战。"
  },
  {
    "instruction": "描述迁移学习的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "特别"
      ]
    ],
    "topic": "迁移学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "迁移学习的特点包括：  \n- 利用已有任务或领域中的知识，应用到新的相关任务中。  \n- 在目标数据规模有限的情况下，仍可构建有效模型。  \n- 通过共享特征表示或模型参数，减少从零开始训练的成本。  \n- 能缩短模型训练时间，提高开发效率。  \n- 适用于跨任务、跨领域或跨数据分布的学习场景。"
  },
  {
    "instruction": "写一篇关于情感分析的短文，必须以问句结尾。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "情感分析",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "情感分析是一种利用自然语言处理与机器学习技术，从文本、语音或图像中识别和理解人类情绪与态度的方法，它被广泛应用于舆情监测、客户反馈分析、产品推荐和社会研究等领域，通过对大量数据的情绪倾向进行归纳，帮助企业和研究者做出更精准的决策，然而在处理讽刺、隐喻和跨文化语境时仍面临挑战，你认为情感分析的未来将走向何方？"
  },
  {
    "instruction": "写一段关于联邦学习的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "极其"
      ]
    ],
    "topic": "联邦学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "联邦学习是一种机器学习方法，模型在各参与方设备上训练，参数通过通信汇总到服务器，服务器执行更新，过程反复进行，数据留在原处，隐私风险降低，机构之间可以进行协作并推动应用落地。"
  },
  {
    "instruction": "写一段关于情感分析的文字，必须包含以下关键词：优化, 神经网络, 算法。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "人工智能",
          3
        ]
      ]
    ],
    "topic": "情感分析",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "情感分析是自然语言处理中的重要任务，旨在从文本中识别和理解人类情绪倾向，随着数据规模的增长，研究者不断通过模型结构和训练策略的优化来提升分析效果，其中以神经网络为核心的方法尤为突出，这类方法通常结合深度学习算法自动提取语义特征，从而在复杂语境下实现更准确、更稳定的情感判断。"
  },
  {
    "instruction": "写一段关于目标检测的文字，字数要求在200到400之间。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        200
      ],
      [
        "max_length",
        400
      ]
    ],
    "topic": "目标检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "目标检测是计算机视觉领域中的重要研究方向，旨在从图像或视频中同时完成目标的位置定位与类别识别。与图像分类只关注整体类别不同，目标检测需要在复杂背景下准确找出多个目标，并用边界框或其他形式进行标注。近年来，随着深度学习的发展，基于卷积神经网络的算法成为主流，如Faster R-CNN、YOLO和SSD等。这些方法在速度和精度之间不断权衡，推动了目标检测在自动驾驶、安防监控、医学影像和工业检测等场景中的广泛应用。未来，目标检测仍将朝着更高精度、更强鲁棒性和更低计算成本的方向发展。"
  },
  {
    "instruction": "请写5段关于计算机视觉的内容，每段以数字编号开头。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "计算机视觉",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. 计算机视觉是人工智能的重要分支，致力于让计算机从图像和视频中获取、理解并解释视觉信息，其目标是模拟和扩展人类视觉能力。\n\n2. 在技术层面，计算机视觉通常结合图像处理、几何建模和机器学习方法，其中卷积神经网络在目标识别、图像分类等任务中发挥了关键作用。\n\n3. 计算机视觉在实际应用中非常广泛，例如人脸识别、自动驾驶、医学影像分析和工业质检，都依赖高精度的视觉感知能力。\n\n4. 随着深度学习和算力的提升，计算机视觉系统在复杂场景下的鲁棒性和实时性不断增强，但在数据偏差和可解释性方面仍面临挑战。\n\n5. 未来，计算机视觉有望与自然语言处理、机器人技术进一步融合，推动智能系统在理解世界和自主决策方面取得更大突破。"
  },
  {
    "instruction": "描述模型压缩的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "非常"
      ]
    ],
    "topic": "模型压缩",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "模型压缩的特点包括：\n\n- 通过参数裁剪、量化、蒸馏等方法减少模型规模。  \n- 在保持任务功能的前提下降低存储与计算需求。  \n- 有助于提升推理速度并降低能耗。  \n- 便于模型在边缘设备或资源受限环境中部署。  \n- 可能带来精度变化，需要在性能与效率之间进行权衡。"
  },
  {
    "instruction": "写一段关于大语言模型的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "特别"
      ]
    ],
    "topic": "大语言模型",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "语言模型通过数据训练学习词与词之间的关系。该模型接收文本输入，生成文本输出，用于对话、写作、翻译与代码。训练过程包括参数更新与损失计算，推理过程根据上下文选择词。模型在应用中需要计算资源与数据治理，并涉及隐私与安全问题。"
  },
  {
    "instruction": "用恰好100个字概括神经网络的核心要点。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "exact_length",
        100
      ]
    ],
    "topic": "神经网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "神经网络以多层神经元连接为结构通过加权求和与非线性激活学习映射关系借助反向传播优化参数从数据中提取特征具备表示复杂模式与泛化预测能力支持端到端训练适用于视觉语音文本等任务并依赖大量数据与算力持续进化中"
  },
  {
    "instruction": "用Markdown格式撰写关于机器翻译的说明，必须包含标题和分点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "机器翻译",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "```markdown\n# 机器翻译简介\n\n机器翻译（Machine Translation, MT）是指利用计算机自动将一种自然语言转换为另一种自然语言的技术。\n\n## 基本原理\n- 基于规则的翻译：依赖人工编写的语法和词典规则\n- 统计机器翻译：通过大量双语语料学习翻译概率\n- 神经机器翻译：使用深度学习模型进行端到端翻译\n\n## 主要应用\n- 跨语言信息获取与交流\n- 多语言内容本地化\n- 在线翻译工具与智能助手\n\n## 优点与局限\n- 优点：\n  - 翻译速度快、成本低\n  - 可支持多种语言对\n- 局限：\n  - 对语境和文化理解有限\n  - 专业领域翻译准确性仍需提升\n\n## 发展趋势\n- 更大规模的预训练语言模型\n- 多模态翻译（结合文本、语音、图像）\n- 人机协同翻译提升质量\n```"
  },
  {
    "instruction": "以幽默风趣的方式介绍异常检测。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "异常检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "想象一下，异常检测就像小区里的“门卫大爷”。平时大家进进出出：上班、遛狗、买菜，门卫大爷眼睛都不抬一下。但有一天，凌晨三点，一个人穿着羽绒服、拖着冲浪板、还牵着一只羊走进来——大爷立刻警觉：“等等，你不对劲！”这，就是异常检测的灵魂：**在一堆“正常操作”里，揪出那个最不像话的家伙**。\n\n在数据世界里，正常数据就像每天稳定输出的心跳、网速、销量曲线；而异常呢？可能是心电图突然蹦迪、服务器流量半夜开派对、或者销量在没打广告的情况下原地起飞。异常检测的任务不是八卦“为啥这样”，而是先大喊一声：“老板！这里不太对！”\n\n所以你可以把异常检测当成一个不爱惹事、但眼神犀利的同事：平时安静分析数据，一旦发现“画风突变”，立刻举手报告。至于是不是真的出事，还是数据在开玩笑？那就是人类和专家系统接力登场的时刻了。总之，没有异常检测，数据世界就少了一个靠谱的“看门人”。"
  },
  {
    "instruction": "用Markdown格式撰写关于语音识别的说明，必须包含标题和分点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "语音识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "```markdown\n# 语音识别技术说明\n\n语音识别（Automatic Speech Recognition，ASR）是将人类语音转换为可处理文本的技术，广泛应用于人机交互领域。\n\n## 核心概念\n- **语音信号采集**：通过麦克风等设备获取音频数据  \n- **特征提取**：从语音中提取有代表性的声学特征  \n- **声学模型**：将语音特征映射为音素或字词概率  \n- **语言模型**：根据上下文预测最合理的词序列  \n\n## 主要应用\n- **智能语音助手**：如语音搜索、语音控制  \n- **实时转写**：会议记录、字幕生成  \n- **无障碍技术**：辅助听障或行动不便人群  \n\n## 技术挑战\n- **噪声干扰**：复杂环境下识别准确率下降  \n- **口音与方言**：语音多样性带来的建模难度  \n- **实时性要求**：需要在低延迟下完成识别  \n\n## 发展趋势\n- **深度学习模型**：提升识别准确率  \n- **端到端系统**：简化传统多模块流程  \n- **多语言支持**：实现跨语言语音识别\n```"
  },
  {
    "instruction": "在描述模型量化时，请确保'特征提取'这个词出现至少4次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "特征提取",
          4
        ]
      ]
    ],
    "topic": "模型量化",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "模型量化是将浮点模型压缩为低比特表示的过程，目的是在保持精度的同时降低计算与存储开销。在这一过程中，特征提取的质量直接影响量化后的性能，因为特征提取决定了中间表示的分布与动态范围。为了适配量化，常常需要在训练或校准阶段重新审视特征提取，通过统计激活值来选择合适的量化尺度。进一步地，端到端优化会把特征提取与量化感知训练结合起来，使特征提取对低比特噪声更加鲁棒。最终，当特征提取被良好设计并与量化策略协同，模型在边缘设备上的推理效率和准确率都能得到平衡。"
  },
  {
    "instruction": "请写4段关于Transformer架构的内容，每段以数字编号开头。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "Transformer架构",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. Transformer架构最早由Vaswani等人在论文《Attention Is All You Need》中提出，其核心思想是完全基于注意力机制来建模序列数据，摒弃了传统的循环神经网络和卷积结构，从而显著提升了并行计算能力和训练效率。  \n\n2. Transformer通常由编码器（Encoder）和解码器（Decoder）两部分组成，编码器负责将输入序列映射为高层语义表示，解码器则在这些表示的基础上逐步生成输出序列，二者通过注意力机制进行信息交互。  \n\n3. 在Transformer中，多头自注意力（Multi-Head Self-Attention）是关键组件，它允许模型在不同的子空间中同时关注序列的不同位置，从而捕捉长距离依赖关系并增强表达能力。  \n\n4. 由于Transformer本身不包含序列顺序信息，因此引入了位置编码（Positional Encoding）来表示词序，结合残差连接和层归一化等技术，使模型在机器翻译、文本生成和大规模预训练语言模型中取得了突破性成果。"
  },
  {
    "instruction": "用正式的学术语言描述自然语言处理。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "自然语言处理",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "自然语言处理（Natural Language Processing，NLP）是人工智能与计算语言学交叉领域中的一项核心研究方向，旨在研究如何使计算机能够理解、生成与处理人类自然语言。该领域综合运用语言学理论、统计学方法以及机器学习与深度学习技术，对语言的语法结构、语义表示、语用信息与篇章关系进行建模与分析，从而实现对文本与语音数据的自动化处理。\n\n从研究内容上看，自然语言处理涵盖词法分析、句法分析、语义理解、情感分析、信息抽取、机器翻译、自动问答与文本生成等多个子领域。其目标不仅在于提高语言处理任务的准确性与鲁棒性，还在于推动计算系统对人类语言与认知机制的深层次理解。随着大规模语料资源与预训练模型的发展，自然语言处理在理论研究与实际应用层面均取得了显著进展，并在信息检索、智能交互与知识发现等领域展现出重要的学术价值与社会意义。"
  },
  {
    "instruction": "请用不少于100个字描述情感分析。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        100
      ]
    ],
    "topic": "情感分析",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "情感分析是一种利用自然语言处理、机器学习和统计方法，对文本中所表达的情绪、态度和主观倾向进行识别与判断的技术。它通常将情感划分为正面、负面或中性，也可以进一步细分为愤怒、喜悦、悲伤等多种情绪类型。情感分析被广泛应用于社交媒体监测、产品评论分析、客户服务和舆情管理等领域，帮助企业和机构理解用户真实想法。与此同时，情感分析也面临语义歧义、讽刺反语和领域差异等挑战，需要结合上下文和更复杂的模型来提升准确性。"
  },
  {
    "instruction": "请用列表形式列出深度学习的6个要点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "列表"
      ]
    ],
    "topic": "深度学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. **多层神经网络结构**：通过多层非线性变换学习复杂特征表示。  \n2. **表示学习能力**：自动从原始数据中学习高层抽象特征，减少人工特征工程。  \n3. **以数据为驱动**：通常依赖大规模数据来提升模型性能。  \n4. **端到端训练**：从输入到输出整体优化模型，而非分阶段设计。  \n5. **反向传播与优化算法**：利用反向传播和梯度下降等方法更新参数。  \n6. **广泛的应用领域**：在计算机视觉、自然语言处理、语音识别等方面表现突出。"
  },
  {
    "instruction": "写一段关于自监督学习的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "非常"
      ]
    ],
    "topic": "自监督学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "自监督学习是一种机器学习方法，模型从数据构造信号并进行训练，结果用于任务。"
  },
  {
    "instruction": "以幽默风趣的方式介绍代码生成。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "代码生成",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "想象一下，代码生成就像一位**不喝咖啡也能通宵写代码的超级程序员**。你只需要告诉它：“我想要一个能点外卖的应用”，它就会点点头，噼里啪啦一顿操作，瞬间甩给你一堆整整齐齐的代码——不抱怨需求改了三次，也不问“这个需求真的合理吗？”\n\n简单来说，**代码生成**就是让机器帮你写代码。你负责动脑子（或者只动嘴），它负责动键盘。过去你要一行一行敲，现在就像点菜：  \n- “来个登录功能，少盐。”  \n- “数据库接口，多点安全。”  \n- “顺便帮我修个 bug，当甜点。”\n\n当然，它也不是魔法师，而是个**记忆力超群、手速惊人的实习生**：用对了提示，效率飞起；说得含糊，它也可能给你煮一锅“能跑但看不懂”的代码汤。  \n\n总之，代码生成的目标只有一个：**让程序员少掉头发，多写需求，早点下班。**"
  },
  {
    "instruction": "请用不少于100个字描述提示工程。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        100
      ]
    ],
    "topic": "提示工程",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "提示工程是指在与大型语言模型或生成式人工智能交互时，通过精心设计、组织和优化提示语，以引导模型产生更准确、更有用、更可控输出的方法与实践。它关注问题表述、上下文提供、角色设定、示例给出、约束条件以及输出格式等多种因素。良好的提示工程可以显著提升模型在写作、编程、分析和决策支持等任务中的表现，减少歧义和错误。同时，它强调通过不断试验和迭代，根据模型反馈持续改进提示，从而在不重新训练模型的前提下，最大化人工智能系统的应用价值。"
  },
  {
    "instruction": "请写5段关于注意力机制的内容，每段以数字编号开头。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "注意力机制",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. 注意力机制（Attention Mechanism）是一种让模型在处理信息时“关注重点”的方法，其核心思想是为不同输入分配不同权重，从而突出与当前任务最相关的部分，减弱无关信息的影响。  \n2. 在序列建模中，注意力机制通过计算查询（Query）、键（Key）和值（Value）之间的相关性，动态决定模型在每一步应重点关注哪些位置，有效缓解了传统模型对长距离依赖建模困难的问题。  \n3. 自注意力机制（Self-Attention）是注意力机制的重要变体，它在同一序列内部计算元素之间的关系，使模型能够同时捕捉全局和局部依赖，是Transformer架构的核心组成部分。  \n4. 多头注意力（Multi-Head Attention）通过并行地学习多个子空间中的注意力分布，使模型可以从不同角度理解数据特征，从而提升表达能力和建模效果。  \n5. 由于其高效性和灵活性，注意力机制已广泛应用于自然语言处理、计算机视觉和多模态学习等领域，成为现代深度学习模型中不可或缺的关键技术之一。"
  },
  {
    "instruction": "用三段式结构（引言、正文、结论）介绍大语言模型。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "大语言模型",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**引言**  \n大语言模型（Large Language Models，LLMs）是近年来人工智能领域的重要突破，它们通过学习海量文本数据，获得对语言结构和语义的深刻理解，能够像人类一样进行阅读、写作和对话，正在深刻影响信息获取与知识生产方式。\n\n**正文**  \n从技术角度看，大语言模型通常基于深度学习中的神经网络架构，尤其是Transformer模型。它们通过预训练学习语言中的统计规律，再通过微调适应具体任务，如问答、翻译、代码生成和内容创作。随着模型规模、数据量和计算能力的提升，大语言模型在理解上下文、生成连贯文本以及处理复杂问题方面表现出越来越强的能力，但也面临着成本高、偏见和可解释性不足等挑战。\n\n**结论**  \n总体而言，大语言模型已成为推动人工智能应用落地的核心技术之一。未来，随着算法优化、算力提升以及规范治理的完善，大语言模型有望在教育、科研、医疗和产业等领域发挥更大价值，同时也需要人类审慎引导其发展方向，以实现技术进步与社会责任的平衡。"
  },
  {
    "instruction": "用正式的学术语言描述推荐系统。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "推荐系统",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "推荐系统（Recommender Systems）是信息过滤与决策支持领域中的重要研究方向，旨在通过分析用户行为数据、内容特征及其交互关系，预测用户对潜在项目的兴趣或偏好，从而为用户提供个性化的信息或服务推荐。该类系统通常基于统计学习、机器学习与信息检索等理论方法，采用协同过滤、基于内容的推荐、混合推荐以及近年来广泛应用的深度学习模型，以刻画用户—项目之间的潜在关联结构。推荐系统的核心目标在于缓解信息过载问题，提高信息获取效率与用户体验，同时兼顾准确性、多样性、新颖性与公平性等评价指标。在实际应用中，推荐系统广泛服务于电子商务、社交媒体、数字内容分发等场景，但仍面临冷启动、数据稀疏性、隐私保护及算法偏见等一系列理论与工程挑战。"
  },
  {
    "instruction": "请写3段关于注意力机制的内容，每段以数字编号开头。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "注意力机制",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. 注意力机制是一种让模型在处理信息时动态分配“关注度”的方法，它通过计算不同输入部分的重要性权重，突出关键信息、弱化无关信息。这一机制最早在机器翻译中被广泛应用，用于解决长序列中信息难以有效传递的问题。\n\n2. 在深度学习中，注意力机制的核心思想是“加权求和”，即根据查询（Query）、键（Key）和值（Value）之间的相似度来分配权重。自注意力机制进一步将这一思想扩展到同一序列内部，使模型能够捕捉长距离依赖关系，显著提升了表示能力。\n\n3. 随着Transformer模型的提出，注意力机制成为现代自然语言处理和多模态模型的基础组件。它不仅提高了模型的并行计算能力，还增强了对上下文语义的理解能力，如今已广泛应用于文本生成、图像理解和语音识别等领域。"
  },
  {
    "instruction": "描述目标检测，开头必须是'在当今时代'。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "目标检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "在当今时代，目标检测是计算机视觉领域中的一项核心技术，旨在从图像或视频中同时识别出感兴趣的目标类别并精确定位其位置。它通常通过在输入数据中预测目标的边界框和对应的类别标签来实现，广泛应用于自动驾驶、智能安防、医学影像分析、工业质检以及人机交互等场景。随着深度学习的发展，基于卷积神经网络和Transformer的目标检测算法在准确率和实时性方面取得了显著进步，使机器能够更高效、更可靠地理解和感知复杂的现实世界。"
  },
  {
    "instruction": "描述边缘计算的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "极其"
      ]
    ],
    "topic": "边缘计算",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "边缘计算的特点包括：\n\n- **低延迟**：计算与数据源距离接近，响应时间短。  \n- **带宽节省**：在本地处理数据，减少向中心云传输的数据量。  \n- **隐私与安全**：数据在本地处理与存储，降低外部暴露风险。  \n- **实时处理**：适合对时延敏感的业务场景。  \n- **分布式架构**：计算节点分散部署，系统具备扩展性与容错性。  \n- **资源受限**：计算、存储与能耗条件存在约束。"
  },
  {
    "instruction": "在描述大语言模型时，请确保'训练'这个词出现至少2次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "训练",
          2
        ]
      ]
    ],
    "topic": "大语言模型",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "大语言模型是一类基于深度学习的人工智能系统，能够理解和生成自然语言文本。它们通过大规模语料的**训练**来学习语言规律，在**训练**过程中不断调整参数以提升预测和生成能力。经过长期**训练**后，这些模型可以应用于问答、翻译、写作等多种任务。"
  },
  {
    "instruction": "请用不少于150个字描述模型量化。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        150
      ]
    ],
    "topic": "模型量化",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "模型量化是指在深度学习中，将原本以高精度浮点数表示的模型参数和中间计算，转换为低比特或低精度数值表示的一种技术。常见做法包括把FP32压缩为FP16、INT8甚至INT4，从而显著减少模型体积、内存占用和计算带宽需求。模型量化可以分为训练后量化和量化感知训练，前者实现简单、部署成本低，后者在训练阶段就考虑量化误差，通常能获得更好的精度表现。量化过程中常结合校准数据、逐通道量化等策略，并依赖硬件对低精度计算的支持，在移动端、边缘设备和高性能推理场景中具有重要价值，但也需要在推理速度、能耗和模型精度之间进行权衡。"
  },
  {
    "instruction": "请写4段关于推荐系统的内容，每段以数字编号开头。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "推荐系统",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. 推荐系统是一类利用数据分析和算法，为用户提供个性化内容或商品的技术，广泛应用于电商、社交媒体、新闻平台和流媒体服务中，其核心目标是提升用户体验并增加平台价值。  \n\n2. 常见的推荐系统方法包括基于内容的推荐、协同过滤以及混合推荐，其中协同过滤又分为基于用户和基于物品两类，能够从大量用户行为中挖掘潜在兴趣关系。  \n\n3. 数据是推荐系统的基础，既包括显式反馈如评分和点赞，也包括隐式反馈如点击、浏览和停留时间，系统通常通过离线指标和在线A/B测试来评估推荐效果。  \n\n4. 当前推荐系统面临冷启动、数据稀疏性和公平性等挑战，未来的发展趋势则是结合深度学习、多模态信息和可解释性设计，使推荐结果更加精准、透明和可信。"
  },
  {
    "instruction": "用恰好50个字概括注意力机制的核心要点。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "exact_length",
        50
      ]
    ],
    "topic": "注意力机制",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "注意力机制通过计算权重聚焦关键信息动态分配资源提升表示能力与建模效果广泛应用于序列任务中学习性能表现"
  },
  {
    "instruction": "请用不少于200个字描述目标检测。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        200
      ]
    ],
    "topic": "目标检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "目标检测是计算机视觉中的重要研究方向，其核心目标是在图像或视频中同时完成目标的定位与类别识别。与图像分类只判断图像中是否存在某一类物体不同，目标检测需要进一步回答“目标在哪里、是什么、有多少”等问题。通常，检测模型会在图像中预测一个或多个边界框，并为每个边界框赋予对应的类别标签和置信度，从而实现对现实世界中多种目标的自动识别与定位。\n\n从技术发展角度来看，目标检测经历了从传统方法到深度学习方法的演进。早期方法主要依赖人工设计的特征、滑动窗口和分类器组合，不仅计算量大，而且对复杂场景的适应能力有限。随着深度学习尤其是卷积神经网络的发展，端到端的目标检测模型逐渐成为主流，在检测精度和运行速度方面都取得了显著进步。\n\n在实际应用中，目标检测被广泛应用于自动驾驶、视频监控、医疗影像分析、工业缺陷检测以及智能机器人等领域。它为机器理解视觉世界提供了关键能力，也为后续的目标跟踪、行为分析和智能决策奠定了重要基础。"
  },
  {
    "instruction": "介绍异常检测，但不要使用'非常'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "非常"
      ]
    ],
    "topic": "异常检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "异常检测是指在数据中识别与大多数样本模式不一致的个体或行为的过程。它关注发现罕见、偏离常态的情况，这些情况往往蕴含关键信息，例如系统故障、欺诈行为或安全威胁。异常检测的核心目标是在复杂数据环境下，尽早、准确地定位这些异常点。\n\n常见的异常检测方法包括基于统计模型、基于距离或密度的方法，以及机器学习与深度学习方法。统计方法依赖数据分布假设，距离与密度方法关注样本间关系，而学习方法则通过训练模型自动提取特征。异常检测广泛应用于金融风控、网络安全、工业监控和医疗健康等领域，其效果依赖于数据质量、特征选择以及对业务背景的理解。"
  },
  {
    "instruction": "请用不少于250个字描述迁移学习。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        250
      ]
    ],
    "topic": "迁移学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "迁移学习是一种重要的机器学习思想，其核心在于将已经在某一领域或任务中获得的知识，迁移并应用到另一个相关但不完全相同的领域或任务中。传统的机器学习往往假设训练数据和测试数据来自同一分布，而迁移学习则打破了这一假设，强调知识的复用与共享，从而减少对大量标注数据和计算资源的依赖。\n\n在实际方法上，迁移学习通常表现为对已有模型参数、特征表示或结构的继承与微调。例如，先在大规模通用数据上训练一个模型，使其学到通用特征，再将该模型应用到目标任务中，通过少量数据进行调整。这种方式能够加快模型收敛速度，提高在小样本场景下的性能，尤其适合数据稀缺或获取成本较高的应用领域。\n\n迁移学习在计算机视觉、自然语言处理、语音识别等领域得到了广泛应用，如图像分类、文本情感分析和医学诊断等场景中，都显著提升了模型效果。然而，迁移学习也面临负迁移等挑战，即不恰当的知识迁移可能反而降低性能。因此，如何选择合适的源任务、设计有效的迁移策略，仍然是当前研究与实践中的重要问题。"
  },
  {
    "instruction": "用恰好50个字概括情感分析的核心要点。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "exact_length",
        50
      ]
    ],
    "topic": "情感分析",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "情感分析通过文本挖掘识别主观态度判断情绪极性强度来源并结合语境数据模型提升准确性与应用价值实际决策中"
  },
  {
    "instruction": "写一段关于异常检测的文字，字数要求在200到300之间。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        200
      ],
      [
        "max_length",
        300
      ]
    ],
    "topic": "异常检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "异常检测是数据分析与人工智能中的重要研究方向，旨在从大量正常样本中识别出偏离规律的异常行为或异常事件。它广泛应用于金融风控、网络安全、工业设备监测和医疗诊断等领域。常见方法包括基于统计模型、距离或密度的方法，以及机器学习和深度学习模型。通过及时发现异常，系统可以提前预警风险、减少损失，并提升整体运行的安全性与可靠性。同时，异常检测面临数据不平衡、噪声干扰和可解释性不足等挑战，需要结合领域知识与持续优化。"
  },
  {
    "instruction": "请用JSON格式输出边缘计算的关键信息。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "边缘计算",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "```json\n{\n  \"名称\": \"边缘计算\",\n  \"定义\": \"一种将数据处理、存储和分析能力从云端下沉到靠近数据源的网络边缘的计算模式。\",\n  \"核心特征\": [\n    \"低时延\",\n    \"本地处理\",\n    \"带宽优化\",\n    \"高可靠性\",\n    \"数据隐私保护\"\n  ],\n  \"主要优势\": [\n    \"减少网络延迟\",\n    \"降低带宽和云端成本\",\n    \"提升实时性和响应速度\",\n    \"增强数据安全与隐私\"\n  ],\n  \"典型应用场景\": [\n    \"物联网（IoT）\",\n    \"工业互联网\",\n    \"自动驾驶\",\n    \"智慧城市\",\n    \"视频监控与分析\"\n  ],\n  \"基本架构\": [\n    \"终端设备层\",\n    \"边缘节点层\",\n    \"云计算中心\"\n  ],\n  \"关键技术\": [\n    \"容器与微服务\",\n    \"人工智能推理\",\n    \"5G/6G通信\",\n    \"分布式计算\",\n    \"边缘安全技术\"\n  ],\n  \"主要挑战\": [\n    \"设备与平台异构性\",\n    \"运维与管理复杂度高\",\n    \"安全风险增加\",\n    \"标准化不足\"\n  ]\n}\n```"
  },
  {
    "instruction": "请用列表形式列出命名实体识别的4个要点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "列表"
      ]
    ],
    "topic": "命名实体识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. **任务目标**：从非结构化文本中识别并分类命名实体。  \n2. **实体类别**：常见包括人名、地名、组织名、时间与数值等。  \n3. **方法与模型**：规则方法、统计模型以及深度学习模型（如 CRF、BiLSTM-CRF、Transformer）。  \n4. **难点与评估**：存在歧义、实体嵌套、未登录词等问题，通常用 Precision、Recall、F1 值评估效果。"
  },
  {
    "instruction": "在描述RAG检索增强时，请确保'反向传播'这个词出现至少2次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "反向传播",
          2
        ]
      ]
    ],
    "topic": "RAG检索增强",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "RAG（Retrieval-Augmented Generation，检索增强生成）通过在生成模型前引入外部知识检索模块，将查询与相关文档进行匹配，再将检索结果作为上下文输入生成模型，从而提升回答的准确性与可解释性。在训练阶段，生成模型通常通过反向传播来优化参数，而检索模块可以是冻结的或与生成器进行联合优化；在联合优化时，系统需要设计可微或近似可微的机制，使得误差信号能够间接影响检索策略，这同样涉及反向传播思想。在推理阶段，RAG先检索再生成，既保持了大模型的语言能力，又借助外部知识减少幻觉问题。"
  },
  {
    "instruction": "请用JSON格式输出神经网络的关键信息。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "神经网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "```json\n{\n  \"名称\": \"神经网络\",\n  \"定义\": \"一种受生物神经系统启发的机器学习模型，通过多层神经元结构对数据进行建模和预测。\",\n  \"核心组成\": {\n    \"神经元\": \"基本计算单元，负责加权求和与激活\",\n    \"权重\": \"表示输入特征重要性的参数\",\n    \"偏置\": \"用于调整模型输出的参数\",\n    \"激活函数\": [\"ReLU\", \"Sigmoid\", \"Tanh\", \"Softmax\"],\n    \"层结构\": [\"输入层\", \"隐藏层\", \"输出层\"]\n  },\n  \"常见类型\": [\n    \"前馈神经网络（FNN）\",\n    \"卷积神经网络（CNN）\",\n    \"循环神经网络（RNN）\",\n    \"长短期记忆网络（LSTM）\",\n    \"Transformer\"\n  ],\n  \"训练方式\": {\n    \"学习方式\": [\"监督学习\", \"无监督学习\", \"半监督学习\", \"强化学习\"],\n    \"核心算法\": \"反向传播算法\",\n    \"优化方法\": [\"SGD\", \"Adam\", \"RMSProp\"]\n  },\n  \"主要应用\": [\n    \"图像识别\",\n    \"语音识别\",\n    \"自然语言处理\",\n    \"推荐系统\",\n    \"自动驾驶\"\n  ],\n  \"优点\": [\n    \"强大的非线性建模能力\",\n    \"可自动学习特征表示\",\n    \"适用于大规模数据\"\n  ],\n  \"缺点\": [\n    \"对数据量和计算资源要求高\",\n    \"可解释性较差\",\n    \"训练过程复杂\"\n  ]\n}\n```"
  },
  {
    "instruction": "撰写目标检测的介绍，每段必须包含'深度学习'。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "深度学习",
          4
        ]
      ]
    ],
    "topic": "目标检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "目标检测是计算机视觉中的核心任务之一，旨在从图像或视频中定位并识别多个目标对象，其应用涵盖自动驾驶、安防监控和医疗影像等领域。随着深度学习的发展，目标检测方法从传统的手工特征与分类器，逐步演进为端到端的学习框架，显著提升了检测的准确性与鲁棒性。\n\n在技术层面，现代目标检测算法通常基于卷积神经网络，通过深度学习自动学习多层次特征表示，从而同时完成目标的分类与位置回归。代表性方法如两阶段检测器和单阶段检测器，都充分利用深度学习在特征提取和表示学习方面的优势，实现了速度与精度之间的有效平衡。\n\n展望未来，目标检测将继续与多模态感知、实时推理和边缘计算相结合，而深度学习仍将是推动该领域进步的关键动力。通过更高效的网络结构和更大规模的数据训练，深度学习有望进一步提升目标检测在复杂场景下的泛化能力与实用价值。"
  },
  {
    "instruction": "请写4段关于模型量化的内容，每段以数字编号开头。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "模型量化",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. 模型量化是指将深度学习模型中的高精度数值（如FP32浮点数）转换为低精度表示（如INT8或INT4）的过程，其核心目标是在尽量保持模型精度的前提下，显著降低模型的存储开销和计算复杂度。\n\n2. 从实现方式上看，模型量化通常分为训练后量化（Post-Training Quantization）和量化感知训练（Quantization-Aware Training），前者实现简单、成本低，后者则通过在训练阶段模拟量化误差来获得更高的推理精度。\n\n3. 模型量化在边缘计算和移动端部署中具有重要价值，它可以减少模型体积、降低内存带宽需求，并充分利用硬件对低精度计算的加速能力，从而实现更低功耗和更快推理速度。\n\n4. 尽管模型量化优势明显，但也存在精度下降、对某些模型结构不友好等挑战，因此在实际应用中需要结合具体任务、数据分布和硬件平台，选择合适的量化策略与精度配置。"
  },
  {
    "instruction": "描述图像分割，开头必须是'众所周知'。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "图像分割",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "众所周知，图像分割是计算机视觉中的一项基础而关键的任务，其目标是将一幅图像划分为若干具有语义或视觉一致性的区域，使得每个区域对应图像中的不同对象或部分。通过分割，可以从复杂背景中提取有意义的信息，为后续的目标检测、识别与理解提供基础。传统方法包括阈值分割、边缘检测和区域生长等，而近年来以卷积神经网络为代表的深度学习方法（如 U-Net、Mask R-CNN）显著提升了分割的精度与鲁棒性，广泛应用于医学影像、自动驾驶和遥感分析等领域。"
  },
  {
    "instruction": "撰写推荐系统的介绍，每段必须包含'训练'。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "训练",
          2
        ]
      ]
    ],
    "topic": "推荐系统",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "推荐系统是一类通过分析用户行为和内容特征来提供个性化建议的技术，广泛应用于电商、视频和新闻平台。其核心在于利用历史数据进行模型训练，从而理解用户偏好并预测其未来可能感兴趣的内容，训练过程直接影响推荐结果的准确性和多样性。\n\n在实际应用中，推荐系统通常结合多种算法，如协同过滤和深度学习模型，并通过持续的在线与离线训练不断优化性能。随着数据规模和复杂度的提升，高效的训练机制和评估方法变得尤为重要，以确保推荐系统能够稳定、及时地为用户提供高质量的个性化服务。"
  },
  {
    "instruction": "写一段关于自监督学习的文字，必须包含以下关键词：推理, 优化, 神经网络。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "参数",
          3
        ]
      ]
    ],
    "topic": "自监督学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "自监督学习是一种不依赖人工标注数据的学习范式，它通过设计预任务从数据自身构造监督信号，使模型在训练过程中学会有效的表示，在实际应用中，这类方法通常基于神经网络来完成特征提取与表示学习，并通过不断的优化目标函数提升模型性能，训练完成后模型能够在下游任务中进行更准确的推理，从而在计算机视觉、自然语言处理等领域展现出强大的泛化能力。"
  },
  {
    "instruction": "写一段关于边缘计算的文字，字数要求在100到300之间。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        100
      ],
      [
        "max_length",
        300
      ]
    ],
    "topic": "边缘计算",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "边缘计算是一种将数据处理和分析能力从集中式云端下沉到网络边缘的计算模式。通过在靠近数据产生源头的设备或节点上进行实时处理，边缘计算可以显著降低网络延迟，减少带宽占用，提高系统响应速度和可靠性。在物联网、智能制造、自动驾驶和智慧城市等场景中，边缘计算能够更好地满足对实时性和安全性的要求，同时与云计算形成协同，构建更加高效、灵活的数字基础设施。"
  },
  {
    "instruction": "写一段关于图像分割的文字，必须包含以下关键词：推理, 特征提取, 神经网络。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "反向传播",
          4
        ]
      ]
    ],
    "topic": "图像分割",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "图像分割是计算机视觉中的重要任务，其目标是将图像划分为具有语义意义的区域，通常依赖于神经网络模型来完成，在这一过程中，模型首先通过多层结构进行特征提取，获取不同尺度和层次的图像信息，然后结合上下文信息进行推理，从而判断每个像素或区域所属的类别，提高分割结果的准确性与鲁棒性。"
  },
  {
    "instruction": "请用列表形式列出文本分类的4个要点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "列表"
      ]
    ],
    "topic": "文本分类",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. **数据预处理**：包括文本清洗、分词、去停用词、规范化等步骤，提高数据质量。  \n2. **特征表示**：将文本转换为模型可处理的数值特征，如词袋模型、TF-IDF、词向量等。  \n3. **模型选择与训练**：选择合适的分类算法（如朴素贝叶斯、SVM、深度学习模型）并进行训练。  \n4. **模型评估与优化**：使用准确率、召回率、F1值等指标评估效果，并进行参数调优。"
  },
  {
    "instruction": "写一段关于联邦学习的文字，必须包含以下关键词：梯度下降, 反向传播, 神经网络。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "梯度下降",
          3
        ]
      ]
    ],
    "topic": "联邦学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "联邦学习是一种在保护数据隐私的前提下进行协同建模的方法，它通过在各个本地设备上训练模型并只上传参数更新来避免原始数据集中化，在训练过程中仍然依赖于神经网络的基本原理，各参与方在本地利用反向传播计算模型误差，并通过梯度下降更新参数，随后将这些更新汇总到服务器进行聚合，从而在保证隐私与安全的同时实现接近集中式训练的性能。"
  },
  {
    "instruction": "写一段关于注意力机制的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "特别"
      ]
    ],
    "topic": "注意力机制",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "注意力机制是一种方法，模型在处理输入序列时计算元素之间的关系，通过函数得到权重，并将权重分配到各个位置，进而形成上下文表示，模型依据表示生成输出。该机制在训练过程中通过梯度更新参数，并在推理过程中依据输入变化调整权重，从而支持序列到序列的计算。"
  },
  {
    "instruction": "描述自然语言处理的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "很"
      ]
    ],
    "topic": "自然语言处理",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "自然语言处理的特点包括：\n\n1. **语义理解**：通过建模词汇、句法与语义关系，实现对文本含义的分析与表达。  \n2. **上下文依赖**：同一词语在不同语境中含义可能变化，模型需要结合上下文进行判断。  \n3. **歧义处理**：自然语言存在多义词和结构歧义，系统需要进行消解与选择。  \n4. **数据驱动**：大量文本数据用于训练模型，从数据中学习语言规律。  \n5. **规则与统计结合**：在部分任务中，语言规则与统计方法共同发挥作用。  \n6. **多语言支持**：相关技术可应用于不同语言，涉及跨语言表示与迁移。  \n7. **应用场景多样**：可用于机器翻译、信息抽取、对话系统、文本分类等任务。"
  },
  {
    "instruction": "以幽默风趣的方式介绍机器学习。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "机器学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "想象一下，**机器学习**就像在教一个超级勤奋、但一开始啥也不懂的实习生。你不给它写死规章制度（那是传统编程），而是不断给它看例子：这是猫，这是狗；这是垃圾邮件，这是老板的邮件（千万别分错）。看得多了，它突然有一天拍着“CPU”说：“我悟了！”\n\n机器学习的日常就是三件事：**吃数据、找规律、犯错误**。而且它犯错的时候还特别自信——把熊猫认成狗、把人脸识别成面包都是家常便饭。但别担心，你只要不断告诉它“错了，再来”，它就会像打游戏升级一样，慢慢从“青铜选手”进化成“王者段位”。\n\n所以，机器学习并不是机器突然有了智慧，而是**人类把耐心、数据和电费，一股脑儿地喂给了电脑**。最终的成果就是：它能推荐你想买但本来不打算买的东西，还能在你妈之前发现你熬夜了。是不是既聪明，又有点欠揍？"
  },
  {
    "instruction": "用通俗易懂的语言向初学者解释深度学习。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "深度学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "深度学习可以把它理解成一种“让电脑自己学会做事”的方法。\n\n**打个比方**：教小孩认猫。你不会给他一大堆数学公式，而是给他看很多猫的照片，告诉他“这是猫”。看得多了，小孩就能自己总结出规律：有耳朵、有胡须、脸型像这样的大多是猫。  \n**深度学习做的事情很像**，只不过学习的不是人，而是电脑。\n\n---\n\n**深度学习是什么？**  \n深度学习是人工智能的一种方法，它通过一种叫做“神经网络”的结构来学习数据里的规律。这个神经网络可以理解成一个由很多层“小判断器”组成的系统，每一层都会从数据中提取一点信息，层数多了（这就是“深度”的意思），就能学到更复杂的东西。\n\n---\n\n**它是怎么学习的？**  \n1. 先给电脑大量数据（比如成千上万张猫和狗的图片）。  \n2. 电脑先“猜”答案（这张是猫还是狗）。  \n3. 如果猜错了，就调整内部参数，让下次猜得更准。  \n4. 重复这个过程很多次，电脑就越来越聪明。\n\n---\n\n**能用来做什么？**  \n- 人脸识别、语音识别  \n- 自动翻译  \n- 推荐系统（比如推荐视频、商品）  \n- 自动驾驶\n\n---\n\n**一句话总结**：  \n深度学习就是让电脑通过大量数据和反复练习，自己学会发现规律，而不是人一步一步把规则写给它。"
  },
  {
    "instruction": "用恰好80个字概括命名实体识别的核心要点。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "exact_length",
        80
      ]
    ],
    "topic": "命名实体识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "命名实体识别旨在从文本中定位并分类人名地名组织机构时间数量等实体依赖语料标注特征表示模型学习与上下文语义以提升准确性与鲁棒性并支持下游信息抽取应用场景扩展效果佳"
  },
  {
    "instruction": "以幽默风趣的方式介绍机器学习。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "机器学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "机器学习是什么？简单说，就是**让电脑学会“自己长脑子”**，而不是我们手把手教它每一步。\n\n以前的程序员像严格的家长：  \n“如果看到A，就做B；如果看到C，就做D。”  \n而机器学习更像是放养式教育：  \n“我给你一堆作业和答案，你自己总结规律，考不好别怪我。”\n\n举个例子：  \n你教电脑分辨猫和狗。  \n- 传统编程：  \n  程序员：耳朵是三角形？体重多少？尾巴长度几厘米？  \n  电脑：……（当场宕机）  \n- 机器学习：  \n  程序员：来，这里有10万张猫狗照片，答案我都标好了。  \n  电脑：我悟了！（其实是算了几亿次）\n\n于是，电脑学会了一种神奇能力：  \n✅ 能认人脸（虽然有时把你认成你爸）  \n✅ 能推荐视频（并精准地浪费你三小时）  \n✅ 能下棋赢世界冠军（但不会帮你点外卖）\n\n总结一句话：  \n**机器学习不是让机器变聪明，而是让它“看起来”比你还聪明。**  \n当然，前提是——数据够多，电费够贵。"
  },
  {
    "instruction": "用三段式结构（引言、正文、结论）介绍模型量化。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "模型量化",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**引言：**  \n模型量化是深度学习模型压缩与加速中的一项重要技术，旨在在尽量保持模型精度的前提下，降低模型的存储大小和计算复杂度。随着深度学习模型在移动端、边缘设备等资源受限场景中的广泛应用，模型量化逐渐成为提升模型部署效率的关键手段。\n\n**正文：**  \n模型量化的核心思想是将模型中的高精度数值（如32位浮点数）转换为低精度表示（如16位浮点数或8位整数）。常见的量化方式包括训练后量化和量化感知训练：前者在模型训练完成后直接进行量化，实施简单但可能带来精度损失；后者在训练过程中模拟量化误差，使模型对低精度表示更具鲁棒性。通过量化，可以显著减少模型参数占用的内存，并加速推理过程中的计算。\n\n**结论：**  \n总体而言，模型量化是一种在性能与效率之间取得平衡的有效方法。它使得复杂的深度学习模型能够在算力和能耗受限的环境中运行，为人工智能技术的落地应用提供了重要支持。随着硬件和算法的不断发展，模型量化技术也将更加成熟和普及。"
  },
  {
    "instruction": "用正式的学术语言描述语音识别。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "语音识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "语音识别（Automatic Speech Recognition，ASR）是指利用计算机系统将人类语音信号自动转换为可处理和理解的文本或符号表示的技术，是语音信息处理与人工智能领域的重要研究方向。该技术以数字信号处理、模式识别和统计建模为理论基础，通常包括语音信号采集、预处理、特征提取、声学建模、语言建模以及解码等关键环节。随着机器学习，尤其是深度学习方法的发展，语音识别系统在建模语音的时序特性、发音变异性和语言上下文依赖方面取得了显著进展，其识别准确率和鲁棒性不断提升。目前，语音识别技术已广泛应用于人机交互、信息检索、智能控制及辅助技术等领域，对推动自然人机交互方式的演进具有重要的理论意义和应用价值。"
  },
  {
    "instruction": "以幽默风趣的方式介绍神经网络。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "神经网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "想象一下，神经网络就像一群特别爱开会的“神经元小员工”。每个小员工都只会做一件很简单的事：接收信息、想一想、再把结果传给下一个同事。单独看，他们可能连“1+1=2”都不敢保证；但一大群凑在一起，就能干出识别人脸、翻译语言、甚至陪你聊天这种“大事业”。\n\n训练神经网络的过程，有点像教一个超级健忘但很听劝的学生做题。一开始它错得离谱，比如把猫当成洗衣机；你告诉它“错了”，它就乖乖调整自己一点点。错、改、再错、再改……经过成千上万次“被批评”，它终于恍然大悟：“哦，原来猫是有胡须的！”这套反复挨骂还能不断进步的能力，我们给它起了个听起来很高深的名字——学习。\n\n所以，神经网络并不是真的有大脑，也不会突然产生自我意识统治世界。它更像一个勤奋的打工人：只要你给数据、给目标、给足算力，它就能不眠不休地干活，而且从不要求涨工资（只要电费你来付）。"
  },
  {
    "instruction": "请写5段关于推荐系统的内容，每段以数字编号开头。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "推荐系统",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. 推荐系统是一类利用数据和算法为用户提供个性化内容或产品建议的技术，广泛应用于电商、视频平台、音乐应用和新闻资讯等场景，其目标是提升用户体验和平台价值。  \n\n2. 常见的推荐系统方法包括协同过滤、基于内容的推荐以及混合推荐，其中协同过滤通过分析用户之间或物品之间的相似性来进行推荐。  \n\n3. 数据是推荐系统的核心基础，通常包括用户行为数据、用户属性、物品特征以及上下文信息，特征工程和数据质量直接影响推荐效果。  \n\n4. 在评估推荐系统时，常用指标有准确率、召回率、点击率和覆盖率，同时还需要关注冷启动、数据稀疏性和实时性等实际挑战。  \n\n5. 随着推荐系统的普及，其带来的隐私保护、公平性和信息茧房问题也受到关注，因此在系统设计中需要兼顾技术效果与社会责任。"
  },
  {
    "instruction": "描述提示工程的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "极其"
      ]
    ],
    "topic": "提示工程",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "提示工程的特点包括：\n\n- **目标导向**：围绕具体任务设定提示内容，引导模型输出符合预期的结果。  \n- **结构化表达**：通过清晰的指令、示例或约束条件，减少理解偏差。  \n- **上下文利用**：在提示中引入必要背景信息，帮助模型把握语境。  \n- **可迭代性**：根据模型输出不断调整提示，逐步接近需求。  \n- **可迁移性**：部分提示模式可应用于不同任务或领域。  \n- **人机协作**：结合人的判断与模型生成能力，提升问题解决效率。"
  },
  {
    "instruction": "请写3段关于图像分割的内容，每段以数字编号开头。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "图像分割",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. 图像分割是计算机视觉中的一项核心任务，目标是将图像划分为若干具有语义意义的区域或对象。通过图像分割，可以将前景与背景区分开来，为后续的目标识别、跟踪和理解提供基础。\n\n2. 常见的图像分割方法包括传统方法和基于深度学习的方法。传统方法如阈值分割、区域生长和边缘检测依赖人工设计特征，而深度学习方法（如U-Net、Mask R-CNN）则通过大规模数据学习复杂特征，在精度和鲁棒性上表现更优。\n\n3. 图像分割在医学影像、自动驾驶和遥感分析等领域具有广泛应用。例如在医学中，分割技术可用于识别肿瘤区域；在自动驾驶中，可用于区分道路、行人和车辆，从而提升系统的安全性和智能化水平。"
  },
  {
    "instruction": "以问答形式介绍强化学习，包含至少4个问题和回答。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "强化学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**问1：什么是强化学习？**  \n**答：**强化学习（Reinforcement Learning, RL）是一种机器学习方法，智能体通过与环境交互，根据获得的奖励或惩罚来学习策略，从而在长期内最大化累计回报。\n\n**问2：强化学习的基本组成要素有哪些？**  \n**答：**强化学习通常包括四个核心要素：智能体（Agent）、环境（Environment）、状态（State）、动作（Action），以及用于反馈的奖励（Reward）。\n\n**问3：强化学习是如何学习的？**  \n**答：**智能体在某个状态下选择一个动作，环境会反馈新的状态和奖励，智能体根据这些信息不断更新策略，使得未来获得的总奖励更高。\n\n**问4：强化学习与监督学习有什么区别？**  \n**答：**监督学习依赖已标注的正确答案进行训练，而强化学习没有直接的“正确答案”，只能通过奖励信号的好坏来判断行为是否合适。\n\n**问5：强化学习有哪些典型应用？**  \n**答：**强化学习常用于游戏AI（如围棋和电子游戏）、机器人控制、自动驾驶、推荐系统以及资源调度等领域。"
  },
  {
    "instruction": "写一篇关于推荐系统的短文，必须以问句结尾。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "推荐系统",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "推荐系统通过分析用户行为、内容特征和上下文信息，为用户提供个性化的内容与商品选择，已广泛应用于电商、媒体和社交平台。它们利用协同过滤、内容理解和深度学习等技术提升匹配效率，同时也面临冷启动、偏见放大和隐私保护等挑战。在追求更高转化率与用户体验的同时，我们该如何在算法效率、透明度与伦理责任之间取得平衡？"
  },
  {
    "instruction": "用通俗易懂的语言向初学者解释代码生成。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "代码生成",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "代码生成，简单来说，就是**让电脑帮你写代码**。\n\n你可以把它想成“自动写作”。比如你想写一封请假邮件，只要告诉工具“我要请假一天，原因是感冒”，它就能帮你把完整的邮件写出来。代码生成也是一样：你告诉电脑**你想要做什么**（比如“做一个计算器”或“从数据库读取数据”），它就会按照一定的规则，自动帮你生成对应的程序代码。\n\n对初学者来说，代码生成的好处是：\n- **省时间**：不用从零开始写很多重复的代码  \n- **少出错**：自动生成的代码通常符合规范  \n- **好学习**：你可以通过看生成的代码，理解“正确的代码应该怎么写”\n\n常见的代码生成方式包括：  \n- **模板生成**：提前准备好“代码模板”，只填关键信息就能生成代码  \n- **工具/框架生成**：比如创建项目时，工具自动生成项目结构  \n- **AI 生成**：像现在的 AI，根据你的描述直接生成代码  \n\n总之，代码生成不是取代你学习编程，而是**帮你更快、更轻松地写代码和学习代码**。"
  },
  {
    "instruction": "用正式的学术语言描述代码生成。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "代码生成",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "代码生成是指在计算机科学与软件工程领域中，依据形式化或半形式化的规格说明、模型或高层抽象表示，通过自动化或半自动化方法生成可执行源代码或中间代码的过程。该过程旨在将人类可理解的设计意图系统地映射为符合特定编程语言语法与语义约束的程序实现，以提高开发效率、降低人为错误率，并促进软件系统的一致性、可维护性与可复用性。"
  },
  {
    "instruction": "用三段式结构（引言、正文、结论）介绍异常检测。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "异常检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**引言**  \n异常检测（Anomaly Detection）是指在大量数据中识别出与大多数数据显著不同、可能代表错误、风险或重要事件的样本或模式。随着数据规模和复杂度的不断增长，异常检测在保障系统安全、提升业务稳定性以及发现潜在问题方面变得尤为重要。\n\n**正文**  \n从方法上看，异常检测主要包括基于统计的方法、基于距离或密度的方法，以及基于机器学习和深度学习的方法。统计方法依赖数据分布假设，适合结构清晰的数据；距离和密度方法通过度量样本间差异来发现离群点；而机器学习方法（如孤立森林、自编码器等）则在高维、复杂数据场景中表现更佳。异常检测被广泛应用于金融欺诈识别、网络入侵检测、工业设备故障预测和医疗健康监测等领域。\n\n**结论**  \n总体而言，异常检测是一项连接数据分析与实际决策的重要技术。尽管不同应用场景对方法选择和效果要求各异，但其核心目标始终是从海量数据中及时、准确地发现“异常”。随着算法和计算能力的进步，异常检测将在更多关键领域发挥更大的价值。"
  },
  {
    "instruction": "介绍计算机视觉，但不要使用'极其'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "极其"
      ]
    ],
    "topic": "计算机视觉",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "计算机视觉是人工智能的重要分支，研究如何让计算机从图像和视频中获取信息、理解场景并作出决策。它结合了图像处理、机器学习、深度学习和几何方法，使机器能够完成目标检测、图像分类、语义分割、人脸识别和三维重建等任务。\n\n在应用层面，计算机视觉被广泛用于自动驾驶、医学影像分析、安防监控、工业质检、零售和增强现实等领域。随着算力提升、数据规模扩大以及算法进步，计算机视觉系统的准确性和实时性不断提高，并逐步走向端侧部署与多模态融合，推动智能系统更好地感知和理解世界。"
  },
  {
    "instruction": "请写3段关于联邦学习的内容，每段以数字编号开头。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "联邦学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. 联邦学习是一种分布式机器学习范式，其核心思想是在数据不出本地的前提下完成模型训练。各参与方仅共享模型参数或梯度，而不直接交换原始数据，从而在保障数据隐私和合规性的同时实现协同建模。\n\n2. 在联邦学习过程中，通常由中心服务器负责协调训练，各客户端在本地数据上训练模型并上传更新结果。服务器对这些更新进行聚合，生成新的全局模型，再下发给客户端，如此反复迭代直至模型收敛。\n\n3. 联邦学习已在金融、医疗、智能终端等领域展现出重要价值，但也面临通信开销大、数据异构性强和安全攻击等挑战。为此，研究者正在探索更高效的聚合算法、个性化联邦学习以及结合差分隐私和安全多方计算的解决方案。"
  },
  {
    "instruction": "用正式的学术语言描述Transformer架构。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "Transformer架构",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "Transformer 架构是一种以自注意力机制（self-attention）为核心的序列建模框架，最初由 Vaswani 等人在 2017 年提出，旨在克服循环神经网络和卷积神经网络在建模长程依赖关系和并行计算方面的局限性。该架构在自然语言处理等序列建模任务中展现了显著的性能优势，并已成为现代深度学习模型的重要基础。\n\n从整体结构上看，标准的 Transformer 由编码器（encoder）和解码器（decoder）两部分组成，二者均由若干结构相同的层堆叠而成。编码器负责将输入序列映射为一组上下文相关的连续表示，解码器则在此基础上以自回归方式生成输出序列。在仅需编码或建模输入序列的任务中，也可使用仅包含编码器或解码器的变体结构。\n\nTransformer 的核心组成模块是多头自注意力机制（multi-head self-attention）。该机制通过对输入序列中不同位置之间的相关性进行加权建模，使模型能够在不依赖序列顺序递归计算的情况下捕获全局依赖关系。多头设计允许模型在不同的表示子空间中并行地关注序列的多种关系模式，从而增强表达能力。注意力计算通常基于缩放点积注意力（scaled dot-product attention），以提高数值稳定性和训练效率。\n\n由于 Transformer 不包含显式的序列递归或卷积结构，为了引入序列顺序信息，模型在输入嵌入中加入位置编码（positional encoding）。位置编码可以采用固定的正弦-余弦函数形式，或通过可学习参数进行建模，从而使模型能够区分不同位置的标记并利用序列结构信息。\n\n在每一层中，自注意力子层之后通常接有逐位置前馈神经网络（position-wise feed-forward network），该网络对序列中每个位置的表示独立地进行非线性变换。为了稳定训练过程并促进梯度传播，Transformer 在各子层之间引入残差连接（residual connection）和层归一化（layer normalization）。\n\n总体而言，Transformer 架构通过完全基于注意力机制的设计，实现了高效的并行计算能力和强大的序列建模性能，为后续大规模预训练语言模型和多模态模型的发展奠定了理论与工程基础。"
  },
  {
    "instruction": "请用不少于200个字描述模型量化。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        200
      ]
    ],
    "topic": "模型量化",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "模型量化是指在不显著降低模型精度的前提下，将神经网络中参数、激活值或中间计算结果从高精度表示转换为低精度表示的一种技术。常见做法是把原本使用的浮点数表示，如FP32，转换为INT8甚至更低比特宽度的整数表示。通过这种方式，可以显著减少模型的存储空间，降低内存带宽占用，并加快推理速度。模型量化在资源受限的场景中尤为重要，例如移动端设备、嵌入式系统和边缘计算设备。根据实施阶段的不同，量化方法通常分为训练后量化和量化感知训练。训练后量化实现简单，对原模型改动小，但在某些任务上可能带来精度下降。量化感知训练则在训练过程中模拟低精度计算，使模型提前适应量化误差，通常能获得更好的性能表现。尽管模型量化带来了效率提升，但也面临数值精度损失、量化误差累积以及对不同算子支持不一致等挑战。因此，在实际应用中需要结合具体任务、硬件平台和性能需求，选择合适的量化策略，以在效率与精度之间取得平衡。"
  },
  {
    "instruction": "以问答形式介绍注意力机制，包含至少6个问题和回答。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "注意力机制",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**问1：什么是注意力机制（Attention Mechanism）？**  \n答：注意力机制是一种让模型在处理信息时，能够“关注”更重要部分的方法。它通过为不同输入分配不同权重，使模型把计算资源集中在与当前任务最相关的信息上。\n\n**问2：注意力机制最早应用在哪些领域？**  \n答：注意力机制最早广泛应用于自然语言处理领域，尤其是机器翻译任务中，用来解决长句子中信息丢失或对齐困难的问题。\n\n**问3：注意力机制的核心思想是什么？**  \n答：核心思想是“加权求和”。模型会根据当前目标，计算输入中每个元素的重要性权重，然后对输入进行加权组合，得到更有针对性的表示。\n\n**问4：注意力机制中的 Query、Key 和 Value 分别是什么？**  \n答：Query 表示当前需要关注什么，Key 表示可供匹配的特征，Value 表示实际携带的信息。通过计算 Query 和 Key 的相似度，来决定 Value 的权重。\n\n**问5：自注意力（Self-Attention）与传统注意力有什么区别？**  \n答：自注意力是指 Query、Key 和 Value 都来自同一序列，它可以建模序列内部元素之间的关系，而传统注意力通常用于两个不同序列之间的信息对齐。\n\n**问6：注意力机制有哪些实际应用？**  \n答：注意力机制广泛应用于机器翻译、文本摘要、语音识别、图像识别以及大语言模型（如 Transformer）中，是现代深度学习模型的重要组成部分。"
  },
  {
    "instruction": "写一段关于模型压缩的文字，必须包含以下关键词：神经网络, 参数, 模型。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "算法",
          3
        ]
      ]
    ],
    "topic": "模型压缩",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "模型压缩是提升人工智能系统效率的重要技术，它通过剪枝、量化和知识蒸馏等方法，减少神经网络中的冗余参数，在尽量不损失性能的前提下降低模型的存储和计算开销，从而使模型更适合在资源受限的设备上部署与运行。"
  },
  {
    "instruction": "用简洁的技术文档风格说明RAG检索增强。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "RAG检索增强",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**RAG（Retrieval-Augmented Generation，检索增强生成）**\n\n**概述**  \nRAG是一种将信息检索与生成式模型结合的技术，用外部知识库增强大模型的回答准确性与时效性。\n\n**架构**  \n- **检索器（Retriever）**：基于向量检索/关键词检索从知识库召回相关文档。  \n- **生成器（Generator）**：在检索结果的上下文中生成答案（如LLM）。\n\n**流程**  \n1. 用户提问  \n2. 将问题向量化并检索相关文档  \n3. 组装上下文（问题 + 文档）  \n4. 大模型生成回答\n\n**优势**  \n- 降低幻觉，提升事实准确性  \n- 支持私有/实时数据  \n- 无需频繁微调模型\n\n**适用场景**  \n- 企业知识问答  \n- 技术文档检索  \n- 客服与搜索增强系统"
  },
  {
    "instruction": "在描述神经网络时，请确保'深度学习'这个词出现至少2次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "深度学习",
          2
        ]
      ]
    ],
    "topic": "神经网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "神经网络是一种受人脑神经元结构启发的计算模型，通过多层神经元的加权连接来学习数据中的复杂模式。在现代人工智能领域，神经网络是**深度学习**的核心基础，能够通过反向传播算法不断调整参数，从而提升预测和表示能力。随着计算能力和数据规模的增长，**深度学习**推动了卷积神经网络、循环神经网络和Transformer等结构的发展，使神经网络在计算机视觉、自然语言处理和语音识别等任务中取得突破性成果，因此**深度学习**已成为神经网络研究与应用中不可或缺的关键方向。"
  },
  {
    "instruction": "用三段式结构（引言、正文、结论）介绍推荐系统。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "推荐系统",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**引言**  \n推荐系统是一种通过分析用户行为和偏好，为用户提供个性化信息或产品建议的技术，广泛应用于电商、社交媒体、新闻平台和流媒体服务中。随着信息爆炸和用户需求多样化，推荐系统成为提升用户体验和平台价值的重要工具。\n\n**正文**  \n从技术角度看，推荐系统主要包括基于内容的推荐、协同过滤推荐以及混合推荐等方法。基于内容的推荐关注物品本身的特征与用户兴趣的匹配；协同过滤则通过分析用户之间或物品之间的相似性来进行推荐；混合推荐结合多种方法，以弥补单一算法的不足。近年来，随着大数据和深度学习的发展，推荐系统在准确性、实时性和多样性方面得到了显著提升。\n\n**结论**  \n总体而言，推荐系统在缓解信息过载、提升用户满意度和促进商业转化方面发挥着关键作用。未来，随着算法不断进化和对隐私保护、伦理问题的重视，推荐系统将朝着更加智能、可信和以用户为中心的方向发展。"
  },
  {
    "instruction": "用恰好50个字概括情感分析的核心要点。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "exact_length",
        50
      ]
    ],
    "topic": "情感分析",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "情感分析旨在识别文本情绪极性强度与主观性结合语境领域与讽刺提升模型准确性并评估数据质量与泛化能力表现"
  },
  {
    "instruction": "写一段关于大语言模型的文字，必须包含以下关键词：训练, 深度学习, 推理。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "梯度下降",
          3
        ]
      ]
    ],
    "topic": "大语言模型",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "大语言模型是一类以深度学习为核心的方法构建的智能系统，通过在海量数据上的训练来学习语言的统计规律和语义结构，从而在面对新问题时进行高效的推理与生成，它们被广泛应用于问答、写作辅助和知识检索等场景，正在不断推动人工智能技术的发展。"
  },
  {
    "instruction": "用恰好80个字概括计算机视觉的核心要点。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "exact_length",
        80
      ]
    ],
    "topic": "计算机视觉",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "计算机视觉研究如何让机器理解图像与视频通过感知建模特征学习目标检测分割跟踪与识别结合深度学习数据与算法实现自动分析与决策提升理解泛化效率可靠性安全应用落地能力强"
  },
  {
    "instruction": "用恰好120个字概括情感分析的核心要点。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "exact_length",
        120
      ]
    ],
    "topic": "情感分析",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "情感分析旨在从文本语音或图像中识别主观情绪与态度核心在于情感词语特征上下文语义语用线索结合需处理否定程度讽刺多义性并借助监督或预训练模型提升准确性与泛化并关注领域差异与伦理风险并评估数据偏差可解释性应用场景与实时性稳健性与隐私保护合规性审计"
  },
  {
    "instruction": "在描述注意力机制时，请确保'深度学习'这个词出现至少2次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "深度学习",
          2
        ]
      ]
    ],
    "topic": "注意力机制",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "注意力机制是一种在模型中动态分配权重的方法，最早在序列到序列任务中得到广泛应用。它的核心思想是让模型在处理信息时，能够“关注”对当前任务最重要的部分，从而提升表示和决策的质量。在**深度学习**领域，注意力机制显著改善了模型对长距离依赖的建模能力，使模型不再受限于固定长度的表示。\n\n随着研究的发展，注意力机制逐渐成为**深度学习**中的关键组成部分，尤其是在Transformer结构中发挥了核心作用。通过自注意力机制，模型可以并行地建模序列内部各元素之间的关系，这不仅提高了计算效率，也推动了自然语言处理、计算机视觉等多个**深度学习**应用方向的突破。"
  },
  {
    "instruction": "介绍机器翻译，但不要使用'非常'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "非常"
      ]
    ],
    "topic": "机器翻译",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "机器翻译是利用计算机技术将一种语言自动转换为另一种语言的过程。早期方法主要依赖人工编写的规则，后来发展到基于大规模语料的统计模型，如今主流是以深度学习为核心的神经机器翻译。该技术通过学习上下文和语义关系，能够生成更自然、连贯的译文。机器翻译已广泛应用于跨语言交流、信息检索、国际商务和内容本地化等场景，同时仍面临歧义处理、文化差异和专业领域准确性等挑战。随着算力和数据的提升，机器翻译的质量和适用范围仍在不断拓展。"
  },
  {
    "instruction": "用Markdown格式撰写关于深度学习的说明，必须包含标题和分点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "深度学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "# 深度学习简介\n\n- **定义**：深度学习是机器学习的一个分支，基于多层神经网络对数据进行表示和学习。  \n- **核心思想**：通过多层非线性变换，自动从原始数据中提取高层次特征。  \n- **常见模型**：包括卷积神经网络（CNN）、循环神经网络（RNN）和Transformer等。  \n- **应用领域**：广泛应用于计算机视觉、自然语言处理、语音识别和推荐系统。  \n- **优势与挑战**：具有强大的特征学习能力，但对数据规模、计算资源和模型可解释性要求较高。"
  },
  {
    "instruction": "写一段关于神经网络的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "极其"
      ]
    ],
    "topic": "神经网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "神经网络是一种模型。模型由节点和连接组成。节点接收输入并进行计算，然后产生输出。训练过程使用数据和算法，通过调整参数来减少误差。方法用于识别、预测和控制等任务。"
  },
  {
    "instruction": "用正式的学术语言描述机器学习。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "机器学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "机器学习（Machine Learning）是人工智能领域的重要分支，旨在研究和构建能够从数据中自动获取规律并据此进行预测或决策的算法与模型。其核心思想在于通过对样本数据的统计分析与优化学习，使系统在无需显式编程的情况下不断改进性能，并具备对未知数据的泛化能力。根据学习方式的不同，机器学习通常分为监督学习、无监督学习和强化学习等范式，分别对应于在有标签数据、无标签数据以及与环境交互反馈条件下的学习机制。在方法层面，机器学习涵盖线性模型、概率模型、核方法、神经网络及深度学习等多种技术体系。随着计算能力的提升和大规模数据的广泛可得，机器学习已在模式识别、自然语言处理、计算机视觉和科学研究等诸多领域中展现出重要的理论价值与应用前景。"
  },
  {
    "instruction": "以幽默风趣的方式介绍图像分割。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "图像分割",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "想象一下，你有一张“超级大杂烩”照片：天空、草地、猫、外卖盒子全挤在一起，像周一早高峰的地铁。**图像分割**要做的事，就是当一位有强迫症的整理大师——把“谁是谁”分清楚：天空归天空，草地归草地，猫坚决不和外卖混坐。它不是简单地“看图说话”，而是拿着放大镜逐像素地问：“你是哪一派的？”\n\n更形象点说，图像分割就像给披萨切块：不是随便一刀下去，而是精准地沿着配料边缘切——香肠是香肠，芝士是芝士。切得好，机器就能安心地数猫、找肿瘤、认路牌；切得不好，恭喜你，猫可能会被当成地毯。总之，图像分割的目标很简单：**让世界不再糊成一团，让像素各回各家。**"
  },
  {
    "instruction": "撰写计算机视觉的介绍，每段必须包含'深度学习'。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "深度学习",
          4
        ]
      ]
    ],
    "topic": "计算机视觉",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "计算机视觉是人工智能的重要分支，致力于让计算机理解和分析图像与视频中的信息。随着传感器和算力的发展，计算机视觉在识别、检测和理解方面取得了显著进展，而深度学习的引入使模型能够从大量数据中自动学习特征，显著提升了系统的准确性和鲁棒性。\n\n在实际应用中，计算机视觉已广泛服务于自动驾驶、医疗影像、安防监控和工业质检等领域。依托深度学习构建的卷积神经网络和视觉Transformer等方法，系统可以更好地处理复杂场景，实现从感知到决策的智能化升级。"
  },
  {
    "instruction": "写一段关于图像分割的文字，字数要求在150到250之间。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        150
      ],
      [
        "max_length",
        250
      ]
    ],
    "topic": "图像分割",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "图像分割是计算机视觉中的关键任务，其目标是将图像划分为具有语义或属性一致的区域。通过分割，系统可以从复杂背景中提取目标，为目标检测、医学影像分析和自动驾驶提供基础。常见方法包括阈值分割、区域生长、边缘检测以及基于深度学习的语义分割与实例分割。随着算力和数据的发展，分割精度和鲁棒性不断提升。未来应用前景广阔。"
  },
  {
    "instruction": "用正式的学术语言描述图像分割。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "图像分割",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "图像分割是计算机视觉与图像处理领域中的一项基础性研究任务，其目标是依据像素在灰度、颜色、纹理、形状或空间关系等方面的相似性与差异性，将图像划分为若干具有语义一致性和区域连通性的子区域。通过图像分割，可以实现对目标与背景或不同语义对象的有效区分，从而为后续的目标检测、特征提取、图像理解与高层语义分析提供结构化表示和重要先验信息。"
  },
  {
    "instruction": "用Markdown格式撰写关于语音识别的说明，必须包含标题和分点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "语音识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "```markdown\n# 语音识别说明\n\n- **定义**：语音识别（Speech Recognition）是将人类语音信号转换为可处理的文本或指令的技术。\n- **基本流程**：\n  - 语音采集：通过麦克风获取音频信号\n  - 特征提取：从音频中提取关键声学特征\n  - 模型识别：使用算法或深度学习模型进行解码\n  - 文本输出：生成对应的文字或命令\n- **常见应用**：\n  - 智能助手（如语音助手、智能音箱）\n  - 语音输入法与实时转写\n  - 客服机器人与语音导航系统\n- **主要挑战**：\n  - 噪声干扰与口音差异\n  - 多人同时说话的识别\n  - 专业术语与低资源语言识别\n```"
  },
  {
    "instruction": "以问答形式介绍模型量化，包含至少6个问题和回答。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "模型量化",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**问1：什么是模型量化？**  \n答：模型量化是一种模型压缩与加速技术，通过将模型中的高精度数值（如32位浮点数）转换为低精度表示（如16位浮点数或8位整数），在尽量保持模型精度的前提下，减少模型体积和计算开销。\n\n**问2：为什么要进行模型量化？**  \n答：模型量化可以显著降低模型的存储需求、减少内存占用、提升推理速度，并降低功耗，特别适合在移动端、嵌入式设备和边缘计算场景中部署深度学习模型。\n\n**问3：模型量化常见的类型有哪些？**  \n答：常见的模型量化类型包括：  \n1）后训练量化（Post-Training Quantization，PTQ）；  \n2）量化感知训练（Quantization-Aware Training，QAT）；  \n3）静态量化和动态量化；  \n4）按位宽划分的量化，如INT8、INT4等。\n\n**问4：后训练量化和量化感知训练有什么区别？**  \n答：后训练量化是在模型训练完成后直接进行量化，实施简单但可能带来一定精度损失；量化感知训练则在训练过程中模拟量化误差，使模型提前适应低精度计算，通常能获得更高的量化后精度。\n\n**问5：模型量化会带来哪些影响或风险？**  \n答：模型量化可能导致精度下降，尤其是在对数值精度敏感的模型或任务中；此外，并非所有硬件都对低精度计算有良好支持，若硬件不匹配，可能无法获得预期的性能提升。\n\n**问6：哪些场景特别适合使用模型量化？**  \n答：模型量化特别适合对延迟、功耗和存储敏感的场景，如手机端AI应用、物联网设备、实时语音和图像识别系统，以及大规模模型的高并发推理服务。\n\n**问7：常见的模型量化工具或框架有哪些？**  \n答：常见的模型量化工具和框架包括TensorFlow Lite、PyTorch Quantization、ONNX Runtime、TensorRT等，它们都提供了较为成熟的量化接口和部署支持。"
  },
  {
    "instruction": "用恰好150个字概括深度学习的核心要点。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "exact_length",
        150
      ]
    ],
    "topic": "深度学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "深度学习是以多层神经网络为核心的方法通过数据驱动自动学习特征表示依赖反向传播与梯度下降优化参数强调端到端训练在感知与决策任务中表现突出需要大量数据计算资源与合理正则化以提升泛化"
  },
  {
    "instruction": "用正式的学术语言描述推荐系统。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "推荐系统",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "推荐系统（Recommender Systems）是信息检索与数据挖掘领域中的重要研究方向，旨在通过分析用户行为数据、内容特征及其交互关系，自动为用户提供个性化的信息或决策支持。其核心目标是在信息过载的环境下，准确建模用户兴趣偏好，并从大规模候选集合中筛选出与用户需求高度相关的项目，从而提升用户体验与系统效能。\n\n从方法论角度看，推荐系统通常依赖于统计学习、机器学习及深度学习等理论与技术，典型范式包括基于内容的推荐、协同过滤以及混合推荐方法。近年来，随着大规模数据与计算资源的增长，推荐系统在表示学习、序列建模和因果推断等方面取得了显著进展。然而，其研究与应用仍面临诸如数据稀疏性、冷启动问题、可解释性、公平性与隐私保护等挑战。因此，推荐系统不仅是工程实践中的关键技术，也是人工智能与信息科学领域持续关注的重要学术问题。"
  },
  {
    "instruction": "以问答形式介绍情感分析，包含至少6个问题和回答。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "情感分析",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**问：什么是情感分析？**  \n答：情感分析是一种自然语言处理技术，用于判断文本中所表达的情绪倾向，如正面、负面或中性。\n\n**问：情感分析主要应用在哪些领域？**  \n答：情感分析常用于产品评论分析、社交媒体舆情监测、客户满意度调查、品牌口碑管理等领域。\n\n**问：情感分析是如何工作的？**  \n答：它通常通过机器学习或深度学习模型，对文本中的词语、句子结构和上下文进行分析，从而判断情感倾向。\n\n**问：情感分析可以分为哪些类型？**  \n答：常见类型包括情绪极性分析（正面/负面/中性）、情绪分类分析（如愤怒、喜悦、悲伤等）以及基于方面的情感分析。\n\n**问：进行情感分析需要哪些数据？**  \n答：需要大量带有情感标签的文本数据，如评论、微博、论坛帖子等，用于训练和测试模型。\n\n**问：情感分析面临哪些挑战？**  \n答：主要挑战包括讽刺和反讽的识别、语境依赖、多义词处理以及不同文化和语言差异带来的影响。\n\n**问：情感分析的未来发展趋势是什么？**  \n答：未来情感分析将更加注重细粒度情绪理解、多模态分析（结合文本、语音、图像）以及更强的上下文感知能力。"
  },
  {
    "instruction": "请用JSON格式输出语音识别的关键信息。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "语音识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "```json\n{\n  \"language\": \"zh-CN\",\n  \"transcript\": \"这是语音识别得到的完整文本内容。\",\n  \"confidence\": 0.95,\n  \"duration_seconds\": 12.34,\n  \"speakers\": [\n    {\n      \"speaker_id\": \"S1\",\n      \"segments\": [\n        {\n          \"start_time\": 0.00,\n          \"end_time\": 5.60,\n          \"text\": \"第一位说话人的内容。\"\n        }\n      ]\n    }\n  ],\n  \"keywords\": [\"语音识别\", \"关键信息\", \"示例\"],\n  \"timestamps\": [\n    {\n      \"start_time\": 0.00,\n      \"end_time\": 2.10,\n      \"text\": \"这是\"\n    },\n    {\n      \"start_time\": 2.10,\n      \"end_time\": 4.50,\n      \"text\": \"语音识别\"\n    }\n  ],\n  \"noise_level\": \"low\",\n  \"model\": \"ASR-Model-Name\",\n  \"created_at\": \"2026-02-02T10:00:00Z\"\n}\n```"
  },
  {
    "instruction": "描述神经网络，开头必须是'随着技术发展'。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "神经网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "随着技术发展，神经网络作为人工智能的重要基础模型逐渐成熟，它受生物神经系统启发，由输入层、若干隐藏层和输出层组成，通过加权连接和非线性激活函数对数据进行建模。神经网络通过反向传播算法不断调整参数，以最小化误差并提升预测或决策能力，已广泛应用于图像识别、语音处理、自然语言理解和智能控制等领域，成为推动现代智能技术进步的核心工具。"
  },
  {
    "instruction": "写一段关于机器翻译的文字，必须包含以下关键词：推理, 模型, 深度学习。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "参数",
          4
        ]
      ]
    ],
    "topic": "机器翻译",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "机器翻译是自然语言处理中的重要应用，它依托深度学习方法，通过大规模语料训练模型，使系统能够在不同语言之间进行高质量转换。在这一过程中，模型不仅学习词汇和语法对应关系，还需要在翻译时进行有效的推理，以理解上下文语义并生成自然流畅的目标语言文本。"
  },
  {
    "instruction": "写一篇关于知识蒸馏的短文，必须以问句结尾。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "知识蒸馏",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "知识蒸馏是一种将复杂模型中的知识迁移到轻量模型中的机器学习方法，通常通过让学生模型学习教师模型的软标签或中间表示来实现。它能够在尽量保持性能的同时显著降低模型的参数量和计算成本，因此在模型压缩、边缘部署和实时推理等场景中具有重要价值。随着深度学习模型规模不断扩大，知识蒸馏是否会成为平衡性能与效率的关键技术之一？"
  },
  {
    "instruction": "用通俗易懂的语言向初学者解释Transformer架构。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "Transformer架构",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "可以把 **Transformer** 想成一种“特别会读书和写作的模型”，它的目标是：**理解一段文字中各个词之间的关系，然后生成新的文字**。下面用尽量通俗的方式来解释。\n\n---\n\n## 一、Transformer在做什么？\n\n假设有一句话：\n\n> 我 今天 吃 了 苹果\n\nTransformer要做的事情是：  \n**让每个词都能“看见”句子里其他所有词，并判断哪些词对自己最重要。**\n\n比如理解“吃”的时候，“我”和“苹果”就很重要，“今天”次要一些。\n\n---\n\n## 二、核心思想：注意力（Attention）\n\n### 1️⃣ 什么是注意力？\n\n注意力可以理解为：  \n> **在一堆信息里，挑重点来看**\n\n就像你读一句话时，不是每个字都同样重要，大脑会自动关注关键部分。\n\n### 2️⃣ 自注意力（Self-Attention）\n\n“自注意力”意思是：  \n> **一句话里的每个词，都会和这句话里的其他所有词做一次比较**\n\n每个词都会问自己三个问题（这是比喻）：\n- 我是谁？（Query）\n- 我有什么信息？（Key）\n- 我能提供什么内容？（Value）\n\n通过比较，模型会算出：  \n👉 “我应该多关注哪个词？少关注哪个词？”\n\n---\n\n## 三、Transformer的结构\n\nTransformer主要由两大部分组成：\n\n### 1️⃣ 编码器（Encoder）——负责“理解”\n\n可以把编码器想成**读书的人**。\n\n- 输入一句话\n- 通过多层自注意力\n- 输出“每个词都已经理解上下文后的表示”\n\n而且编码器不是只有一层，而是**一层一层叠加**，理解得越来越深入。\n\n---\n\n### 2️⃣ 解码器（Decoder）——负责“生成”\n\n解码器像是**写作的人**。\n\n- 一边看编码器的理解结果\n- 一边根据已经写出的词\n- 预测下一个最可能的词\n\n比如写句子：\n> 我今天吃了 ___\n\n模型会预测下一个词可能是“苹果”“米饭”等。\n\n---\n\n## 四、那顺序怎么办？（位置编码）\n\n你可能会问：  \n> Transformer一次看所有词，那它怎么知道顺序？\n\n答案是：**位置编码（Positional Encoding）**\n\n可以理解为：\n- 给每个词偷偷贴一个“位置标签”\n- 比如：第1个词、第2个词……\n\n这样模型就知道：\n> “苹果在句子后面，不是在开头”\n\n---\n\n## 五、为什么Transformer这么厉害？\n\n✅ 所有词可以**同时处理**（不像老模型要一个一个看）  \n✅ 能很好地抓住**远距离关系**  \n✅ 非常适合大规模训练  \n\n这也是为什么：\n- GPT\n- BERT\n- ChatGPT  \n\n都基于Transformer。\n\n---\n\n## 六、一句话总结\n\n> **Transformer就是一个靠“注意力机制”来理解和生成文字的模型，它让每个词都能关注句子中最重要的其他词，从而更聪明地读和写。**\n\n如果你愿意，我也可以用**图示思路**、**代码直觉版**，或者**和人类大脑对比**的方式再讲一遍。"
  },
  {
    "instruction": "写一段关于知识蒸馏的文字，字数要求在150到300之间。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        150
      ],
      [
        "max_length",
        300
      ]
    ],
    "topic": "知识蒸馏",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "知识蒸馏是一种模型压缩与迁移学习方法，通过让性能更强的教师模型向学生模型传递知识，从而在保持精度的同时降低计算成本。其核心思想是利用软标签、特征表示或中间层信息，引导学生模型学习教师模型的判别能力。知识蒸馏常用于将大模型能力迁移到轻量模型，适合边缘设备与实时应用，也能提升泛化性与训练稳定性。在工业实践中，它已成为深度学习部署的重要技术之一。"
  },
  {
    "instruction": "用通俗易懂的语言向初学者解释RAG检索增强。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "RAG检索增强",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "RAG（Retrieval-Augmented Generation，检索增强生成）可以理解为：**让大模型在回答问题前，先“翻资料”，再“组织语言回答”**。\n\n**为什么需要RAG？**  \n大模型就像一个记忆力很强但不一定“最新、最全”的学生。它主要靠训练时学到的知识回答问题，但遇到**最新信息、公司内部资料、专业文档**时，可能不知道或答不准。RAG就是给它配了一本“随时可查的参考书”。\n\n**RAG是怎么工作的？（三步走）**  \n1. **检索（Retrieval）**：当你提问时，系统先从资料库里（比如文档、网页、数据库）找出和问题最相关的内容。  \n2. **增强（Augmented）**：把这些找到的内容当作“参考材料”，一起交给大模型。  \n3. **生成（Generation）**：大模型根据这些真实资料，用通俗的话整理出答案。\n\n**举个例子**  \n你问：“我们公司去年请假制度是怎样的？”  \n- 没有RAG：模型可能只能泛泛而谈，甚至猜。  \n- 有RAG：系统先查到公司内部的请假制度文件，再根据文件内容，准确地告诉你规则。\n\n**一句话总结**  \n👉 **RAG就是：让大模型先查资料，再用自己的话回答，从而更准确、更可靠。**"
  },
  {
    "instruction": "用简洁的技术文档风格说明知识蒸馏。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "知识蒸馏",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**知识蒸馏（Knowledge Distillation）说明**\n\n**定义**  \n知识蒸馏是一种模型压缩技术，通过让小模型（Student）学习大模型（Teacher）的输出分布或中间知识，在降低模型规模与计算成本的同时保持性能。\n\n**核心思想**  \n- Teacher 提供“软目标”（Soft Targets），包含类别间的相对信息。  \n- Student 同时学习真实标签与 Teacher 的知识。\n\n**典型流程**  \n1. 训练或选取高性能 Teacher。  \n2. 使用温度系数 *T* 软化 Teacher 的输出概率。  \n3. 以蒸馏损失训练 Student。\n\n**常用损失函数**  \n- 蒸馏损失：KL 散度（Teacher vs. Student 的软输出）。  \n- 监督损失：交叉熵（Student vs. 真实标签）。  \n- 总损失：加权和。\n\n**优势**  \n- 模型更小、推理更快。  \n- 在资源受限场景保持较高精度。\n\n**应用场景**  \n- 移动/边缘设备部署  \n- 大模型压缩  \n- 多模型集成知识迁移"
  },
  {
    "instruction": "用简洁的技术文档风格说明自然语言处理。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "自然语言处理",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**自然语言处理（NLP）概述**\n\n**定义**  \n自然语言处理（NLP）是人工智能与计算语言学的交叉领域，研究计算机对人类自然语言的理解、生成与交互。\n\n**目标**  \n- 将非结构化文本/语音转化为可计算表示  \n- 实现语言理解、推理与生成\n\n**关键技术**  \n- 文本表示：词袋、TF-IDF、词向量、预训练语言模型  \n- 模型方法：统计方法、深度学习、Transformer  \n- 训练策略：监督学习、无监督学习、迁移学习\n\n**典型任务**  \n- 分词、词性标注、命名实体识别  \n- 文本分类、情感分析、信息抽取  \n- 机器翻译、问答系统、文本生成\n\n**应用场景**  \n- 搜索与推荐  \n- 智能客服与对话系统  \n- 文档分析与知识管理\n\n**主要挑战**  \n- 语义歧义与上下文依赖  \n- 多语言与低资源问题  \n- 数据偏差与可解释性"
  },
  {
    "instruction": "撰写代码生成的介绍，每段必须包含'参数'。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "参数",
          3
        ]
      ]
    ],
    "topic": "代码生成",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "代码生成是指通过工具或模型自动产出源代码的过程，它能够根据需求描述、模板或配置快速生成可用程序。在这一过程中，开发者通常需要提供清晰的规则和参数，以确保生成的代码符合预期的功能和风格。\n\n在实际应用中，代码生成可以显著提升开发效率，减少重复劳动，并降低人为错误的概率。无论是前端页面、后端接口还是测试脚本，合理设置参数都能让生成结果更加贴合具体业务场景。\n\n随着人工智能技术的发展，智能化代码生成逐渐普及，系统可以根据上下文自动推断结构和逻辑。即便如此，用户仍需通过调整参数来控制生成范围、语言特性和性能要求，从而获得高质量的代码输出。"
  },
  {
    "instruction": "以问答形式介绍自监督学习，包含至少3个问题和回答。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "自监督学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**问1：什么是自监督学习？**  \n**答：**自监督学习是一种介于监督学习和无监督学习之间的机器学习方法。它不依赖人工标注的数据，而是从数据本身自动构造“伪标签”，通过解决预设任务来学习有用的表示。\n\n**问2：自监督学习是如何生成标签的？**  \n**答：**自监督学习通过设计预任务（Pretext Task）来生成标签，例如在图像中预测被遮挡的区域、在文本中预测被掩盖的词（如Mask Language Modeling），这些标签都直接来自原始数据本身。\n\n**问3：自监督学习有哪些常见应用？**  \n**答：**自监督学习广泛应用于计算机视觉、自然语言处理和语音领域，例如图像表征学习、语言模型预训练（如BERT、GPT）以及语音识别中的特征学习。\n\n**问4：自监督学习相比监督学习有什么优势？**  \n**答：**自监督学习可以大幅减少对人工标注数据的依赖，降低成本，并能利用海量未标注数据学习通用特征，从而在下游任务中获得更好的性能。"
  },
  {
    "instruction": "请写4段关于提示工程的内容，每段以数字编号开头。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "提示工程",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. 提示工程（Prompt Engineering）是指通过精心设计和优化提示语，使大语言模型更准确、高效地完成任务的一种方法。它关注如何用清晰、具体、可控的语言与模型交互，从而减少歧义并提升输出质量。\n\n2. 在提示工程中，常见技巧包括明确角色设定、给出背景信息、指定输出格式以及提供示例。这些方法可以帮助模型更好地理解用户意图，尤其在复杂任务中，如写作、编程或数据分析。\n\n3. 提示工程并非一次性完成，而是一个不断迭代的过程。用户需要根据模型的输出结果调整提示内容，通过反复测试和优化，逐步找到最适合当前任务的提示结构。\n\n4. 随着大模型能力的提升，提示工程正在成为一项重要技能。无论是开发者、研究人员还是普通用户，掌握基本的提示工程思路，都能更高效地利用人工智能工具解决实际问题。"
  },
  {
    "instruction": "以幽默风趣的方式介绍强化学习。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "强化学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "想象一下你在教一只“程序猿”学本事——它一开始啥也不会，只会到处乱按按钮。你不给它讲大道理，也不写说明书，只做一件事：**干得好就给糖，干得差就扣分**。这，就是强化学习的精髓。\n\n在强化学习的世界里，有三个主角：  \n- **智能体（Agent）**：比如这只“程序猿”，负责行动；  \n- **环境（Environment）**：比如游戏地图或现实世界，负责出难题；  \n- **奖励（Reward）**：比如糖果或电击（别担心，只是数学意义上的），负责告诉它“刚才那步行不行”。\n\n智能体的日常就是：**试一试 → 被表扬或挨骂 → 记住教训 → 下次再试**。一开始它可能笨到撞墙，但在无数次“被生活毒打”后，终于悟了：哦，原来这样走能通关！于是它从“人工智障”进化成了“别人家的AI”。这，就是强化学习——用奖励把机器“哄”成高手。"
  },
  {
    "instruction": "在描述模型压缩时，请确保'反向传播'这个词出现至少4次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "反向传播",
          4
        ]
      ]
    ],
    "topic": "模型压缩",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "模型压缩旨在在尽量保持精度的前提下降低模型规模和计算量，常见方法包括剪枝、量化和知识蒸馏。在这些方法中，反向传播起着核心作用：通过反向传播可以评估参数对损失的敏感性，从而指导结构化或非结构化剪枝。在量化训练中，反向传播用于在低比特表示下更新权重，使模型在受限表示中仍能收敛。对于知识蒸馏，学生模型同样依赖反向传播来最小化与教师模型输出之间的差异。即便在联合多种压缩策略的流程里，反向传播依然是连接优化目标与参数更新的关键机制。总体而言，合理设计包含反向传播的训练策略，能显著提升模型压缩的效果与稳定性。"
  },
  {
    "instruction": "用三段式结构（引言、正文、结论）介绍自监督学习。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "自监督学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**引言**  \n自监督学习是一种介于监督学习与无监督学习之间的机器学习范式，旨在充分利用大量未标注数据。它通过从数据自身构造“伪标签”，让模型在无需人工标注的情况下学习有用的表示，从而降低标注成本并提升泛化能力。\n\n**正文**  \n自监督学习的核心思想是设计预任务（Pretext Task），让模型从数据内部的结构或关系中学习。例如，在计算机视觉中常见的任务包括图像遮挡预测、旋转角度预测和对比学习；在自然语言处理中，则有掩码语言模型、下一句预测等。模型先在这些预任务上进行预训练，学到通用特征表示，再将其迁移到下游任务（如分类、检测或问答）中进行微调。近年来，对比学习方法（如SimCLR、MoCo）和掩码建模方法（如BERT、MAE）显著推动了自监督学习的发展。\n\n**结论**  \n总体而言，自监督学习通过挖掘数据自身的监督信号，有效缓解了对大规模人工标注的依赖，并在多个领域取得了接近甚至超越监督学习的性能。随着模型规模和数据规模的不断增长，自监督学习正成为构建通用人工智能系统的重要基础方法之一。"
  },
  {
    "instruction": "请写5段关于深度学习的内容，每段以数字编号开头。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "深度学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. 深度学习是机器学习的一个重要分支，核心思想是通过多层神经网络自动学习数据中的高层特征表示，从而减少人工特征工程的需求。\n\n2. 在计算机视觉领域，深度学习通过卷积神经网络在图像分类、目标检测和人脸识别等任务中取得了突破性进展，显著提升了准确率和鲁棒性。\n\n3. 在自然语言处理方面，深度学习模型如循环神经网络和Transformer结构，使机器在翻译、问答和文本生成等任务上具备了更强的语义理解能力。\n\n4. 深度学习的成功离不开大规模数据、强大的计算资源以及优化算法的发展，例如反向传播、随机梯度下降和GPU加速训练。\n\n5. 尽管深度学习取得了巨大成就，但仍面临可解释性不足、训练成本高和数据依赖性强等挑战，这些问题也是未来研究的重要方向。"
  },
  {
    "instruction": "写一篇关于模型压缩的短文，必须以问句结尾。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "模型压缩",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "模型压缩旨在在尽量保持模型性能的前提下，减少参数规模、计算量和存储开销，从而提升推理效率并降低部署成本。常见方法包括剪枝、量化、知识蒸馏和低秩分解，它们通过不同机制压缩冗余信息，使模型更适合在移动端和边缘设备上运行。然而，压缩往往伴随精度损失与工程复杂度的权衡，那么如何在效率、精度与可部署性之间找到最佳平衡点呢？"
  },
  {
    "instruction": "请用JSON格式输出Transformer架构的关键信息。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "Transformer架构",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "```json\n{\n  \"名称\": \"Transformer\",\n  \"提出时间\": \"2017\",\n  \"核心思想\": \"基于自注意力机制，完全摒弃循环和卷积结构\",\n  \"主要组成\": {\n    \"编码器\": [\n      \"多头自注意力层\",\n      \"前馈全连接网络\",\n      \"残差连接\",\n      \"层归一化\"\n    ],\n    \"解码器\": [\n      \"掩码多头自注意力层\",\n      \"编码器-解码器注意力层\",\n      \"前馈全连接网络\",\n      \"残差连接\",\n      \"层归一化\"\n    ]\n  },\n  \"关键机制\": {\n    \"自注意力\": \"建模序列中任意位置之间的依赖关系\",\n    \"多头注意力\": \"并行关注不同子空间的信息\",\n    \"位置编码\": \"引入序列中位置信息\"\n  },\n  \"优势\": [\n    \"高度并行化\",\n    \"长距离依赖建模能力强\",\n    \"训练效率高\"\n  ],\n  \"典型应用\": [\n    \"机器翻译\",\n    \"文本生成\",\n    \"语音识别\",\n    \"计算机视觉\"\n  ],\n  \"代表模型\": [\n    \"BERT\",\n    \"GPT\",\n    \"T5\"\n  ]\n}\n```"
  },
  {
    "instruction": "在描述分布式训练时，请确保'算法'这个词出现至少4次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "算法",
          4
        ]
      ]
    ],
    "topic": "分布式训练",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "分布式训练通过将模型和数据拆分到多台计算节点上并行计算，以提升效率和可扩展性。在这一过程中，核心是并行算法与通信算法的设计，算法需要在保证收敛性的同时减少通信开销。常见做法包括数据并行算法、模型并行算法，以及基于参数服务器或全量同步的算法。为了稳定训练，还需要在算法中处理一致性、容错和负载均衡问题。"
  },
  {
    "instruction": "写一段关于语音识别的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "非常"
      ]
    ],
    "topic": "语音识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "语音识别是将语音信号转换为文本的技术。该技术接收音频输入，经过特征提取、模型训练和解码，输出文字结果。语音识别用于人机交互、信息录入和服务流程。研究关注数据收集、算法设计和评估方法。"
  },
  {
    "instruction": "写一段关于强化学习的文字，字数要求在250到450之间。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        250
      ],
      [
        "max_length",
        450
      ]
    ],
    "topic": "强化学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "强化学习是机器学习的重要分支，其核心思想是智能体在环境中通过不断试错来学习最优策略。智能体根据当前状态选择动作，环境反馈奖励或惩罚，并转移到新状态，学习过程的目标是最大化长期累计回报。与监督学习依赖标注数据不同，强化学习更强调交互和决策，具有较强的自主性。常见方法包括基于价值函数的Q学习、基于策略梯度的方法，以及二者结合的Actor-Critic框架。近年来，深度神经网络与强化学习的结合推动了深度强化学习的发展，使其在游戏对战、机器人控制、自动驾驶和资源调度等领域取得显著成果。但强化学习仍面临样本效率低、训练不稳定和安全性等挑战，如何在复杂真实环境中可靠应用，仍是研究的重点方向。"
  },
  {
    "instruction": "写一篇关于分布式训练的短文，必须以问句结尾。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "分布式训练",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "分布式训练是指将模型训练任务拆分到多台计算节点上并行执行，通过数据并行、模型并行或混合并行来提升训练效率和可扩展性，它能够显著缩短大规模模型的训练时间，但同时也带来了通信开销、参数同步和系统稳定性等挑战，在算力不断增长和算法持续演进的背景下，你认为分布式训练将如何塑造未来人工智能的发展方向？"
  },
  {
    "instruction": "描述Transformer架构的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "非常"
      ]
    ],
    "topic": "Transformer架构",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "Transformer架构的主要特点包括：\n\n- **自注意力机制**：通过计算序列中各位置之间的关联，捕捉上下文信息，支持建模长距离依赖关系。  \n- **并行计算**：不依赖时间步递归结构，序列中各位置可同时计算，有利于训练与推理阶段的效率。  \n- **多头注意力**：在多个子空间中并行执行注意力计算，整合不同视角的信息表示。  \n- **位置编码**：引入位置信息以弥补模型本身不包含顺序结构的特点，常见形式包括正弦位置编码与可学习位置向量。  \n- **编码器—解码器结构**：编码器负责提取输入序列表示，解码器基于编码结果生成输出序列，适用于序列到序列任务。  \n- **模块化设计**：由注意力层、前馈网络、残差连接与层归一化组成，结构清晰，便于扩展与改造。  \n- **可扩展性**：参数规模与计算方式适合在大规模数据与算力条件下训练，常用于预训练与下游任务微调。"
  },
  {
    "instruction": "写一段关于生成对抗网络的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "非常"
      ]
    ],
    "topic": "生成对抗网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "生成对抗网络是一种模型，由生成器和判别器组成。生成器接收向量并产生样本，判别器接收样本并给出判断。两者在训练过程中形成博弈，生成器尝试使判别器给出通过，判别器尝试区分数据来源。通过该机制，模型学习数据分布，并用于图像、文本等任务。"
  },
  {
    "instruction": "用通俗易懂的语言向初学者解释提示工程。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "提示工程",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**提示工程（Prompt Engineering）**，简单来说，就是**教会你如何把话说清楚，让 AI 更好地帮你干活**。\n\n你可以把 AI 想象成一个很聪明、但有点“听话不听懂”的助手。  \n你问得越清楚、越具体，它给你的答案就越好；你问得模糊，它就只能猜。\n\n---\n\n### 什么是“提示”（Prompt）？\n**提示**就是你对 AI 说的话，比如一个问题、一段说明或一条指令。\n\n例如：  \n- 不好的提示：  \n  > 帮我写一篇文章  \n- 更好的提示：  \n  > 请用通俗的语言，写一篇 300 字的文章，介绍什么是人工智能，读者是中学生。\n\n第二个提示更清楚，所以 AI 更容易给出你想要的结果。\n\n---\n\n### 提示工程在做什么？\n提示工程不是编程，也不复杂，它主要做三件事：\n1. **说清楚你要什么**（目标）\n2. **告诉 AI 背景和要求**（对象、风格、长度等）\n3. **引导 AI 按你想要的方式回答**\n\n---\n\n### 给初学者的几个简单技巧\n- ✅ 说明“你是谁 + 要干什么”  \n  > 假设你是一名老师，帮我解释……\n- ✅ 说清楚用途和对象  \n  > 给完全没有基础的人看\n- ✅ 提出格式要求  \n  > 用列表说明 / 分步骤回答\n- ✅ 不满意就继续补充  \n  > 再简单一点 / 举个例子\n\n---\n\n### 一句话总结\n**提示工程就是：用更好的提问方式，换来更好的 AI 回答。**  \n不是 AI 变聪明了，而是你更会“指挥”它了。"
  },
  {
    "instruction": "用三段式结构（引言、正文、结论）介绍语音识别。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "语音识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**引言**  \n语音识别是一种将人类语音转换为可理解文本或指令的技术，是人工智能与人机交互领域的重要组成部分。随着计算能力的提升和数据规模的扩大，语音识别逐渐从实验室走向日常生活，成为智能设备的基础功能之一。\n\n**正文**  \n语音识别的核心过程包括语音信号采集、特征提取、模型匹配和结果输出。早期方法主要依赖规则和统计模型，而现代语音识别更多基于深度学习，通过大量语音数据训练神经网络，提高对不同口音、语速和噪声环境的适应能力。该技术已广泛应用于智能助手、语音输入、自动字幕、客服系统等场景，显著提升了信息获取和操作效率。\n\n**结论**  \n总体来看，语音识别正在不断向更高准确率、更自然交互的方向发展。尽管在复杂环境理解、多语言支持和隐私保护方面仍面临挑战，但随着算法和硬件的持续进步，语音识别有望在未来的人机交互中发挥更加关键的作用。"
  },
  {
    "instruction": "描述分布式训练的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "极其"
      ]
    ],
    "topic": "分布式训练",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "分布式训练具有以下特点：  \n- 利用多台计算节点并行执行计算任务，缩短模型训练时间。  \n- 通过数据并行、模型并行或混合并行方式分担计算与存储压力。  \n- 依赖通信机制进行参数同步，如参数服务器架构或集体通信方式。  \n- 对网络带宽、通信延迟和系统稳定性提出要求。  \n- 需要处理节点失效、负载不均以及参数一致性等问题。  \n- 训练流程和调试复杂度增加，通常需要调度、监控与容错工具配合使用。"
  },
  {
    "instruction": "写一篇关于大语言模型的短文，必须以问句结尾。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "大语言模型",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "大语言模型通过海量数据与深度学习掌握语言规律，能够进行对话、写作、翻译与推理，正在改变教育、科研与产业形态，但也带来偏见、安全与责任等挑战，那么我们应如何在发挥其价值的同时建立可靠的治理与伦理框架？"
  },
  {
    "instruction": "以问答形式介绍目标检测，包含至少3个问题和回答。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "目标检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**问：什么是目标检测？**  \n答：目标检测是计算机视觉中的一项重要任务，目的是在图像或视频中找出感兴趣的目标，并同时给出它们的位置（通常用边界框表示）和类别信息。\n\n**问：目标检测和图像分类有什么区别？**  \n答：图像分类只判断整张图像属于哪个类别，而目标检测不仅要判断“是什么”，还要回答“在哪里”，可以在一张图像中检测并定位多个不同目标。\n\n**问：常见的目标检测算法有哪些？**  \n答：常见的目标检测算法包括两阶段方法（如 R-CNN、Fast R-CNN、Faster R-CNN）和单阶段方法（如 YOLO、SSD、RetinaNet），其中单阶段方法通常速度更快。\n\n**问：目标检测有哪些实际应用？**  \n答：目标检测广泛应用于自动驾驶（行人和车辆检测）、安防监控、人脸识别、医学影像分析以及工业质检等领域。"
  },
  {
    "instruction": "描述语音识别的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "极其"
      ]
    ],
    "topic": "语音识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "语音识别的主要特点包括：  \n- 将语音信号转换为文本信息，支持人机交互。  \n- 依赖声学模型、语言模型与算法协同工作。  \n- 对语音清晰度、噪声环境与说话人发音存在要求。  \n- 支持多语言与多口音的识别扩展。  \n- 可应用于输入法、语音助手、客服系统等场景。"
  },
  {
    "instruction": "请写3段关于深度学习的内容，每段以数字编号开头。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "深度学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. 深度学习是机器学习的重要分支，主要通过构建多层神经网络来自动学习数据中的高层次特征。与传统方法相比，深度学习能够减少人工特征工程，在图像、语音和自然语言处理等领域表现尤为突出。\n\n2. 深度学习模型通常包括卷积神经网络、循环神经网络和变换器等结构，它们分别擅长处理图像、序列数据和长距离依赖问题。随着计算能力的提升和大规模数据的出现，这些模型的性能得到了显著提升。\n\n3. 尽管深度学习取得了巨大成功，但仍面临可解释性不足、训练成本高和对数据依赖强等挑战。未来研究将更加关注模型效率、鲁棒性以及与人类认知相结合的发展方向。"
  },
  {
    "instruction": "请用不少于300个字描述机器翻译。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        300
      ]
    ],
    "topic": "机器翻译",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "机器翻译是指利用计算机技术和人工智能方法，将一种自然语言自动转换为另一种自然语言的过程。它的目标是在尽量保持原文意义、语气和风格的前提下，实现跨语言的信息交流。随着全球化的发展和互联网的普及，不同语言之间的沟通需求迅速增长，机器翻译因此成为语言技术领域中最重要的研究方向之一。\n\n从发展历程来看，机器翻译经历了多个阶段。早期方法主要依赖人工编写的规则和词典，通过语法分析和句法转换来完成翻译，但这种方式成本高、灵活性差，难以应对真实语言的复杂性。后来，统计机器翻译利用大量双语语料，通过概率模型寻找最可能的译文，大幅提升了实用性。近年来，基于神经网络的机器翻译成为主流，它通过深度学习模型自动学习语言之间的对应关系，在流畅度和准确性方面取得了显著进步。\n\n在实际应用中，机器翻译已经广泛服务于日常生活和专业领域，例如网页浏览、跨国商务、学术研究和文化传播等。用户可以借助翻译系统快速理解外语内容，降低语言门槛，提高信息获取效率。不过，机器翻译仍然面临诸多挑战，如对语境、隐含意义、文化差异和专业术语的理解仍不够完善。\n\n总体而言，机器翻译并非要完全取代人工翻译，而是作为一种高效的辅助工具。随着算法、算力和语料资源的不断进步，机器翻译的质量将持续提升，在促进全球交流和知识共享方面发挥越来越重要的作用。"
  },
  {
    "instruction": "写一段关于自然语言处理的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "非常"
      ]
    ],
    "topic": "自然语言处理",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "NLP 是一个研究领域，关注文本与语音的表达与理解。该领域使用统计方法与模型，对语料进行处理，实现分词、标注、解析、生成等任务。应用包括搜索、翻译、问答与对话系统，服务用户与系统交互。"
  },
  {
    "instruction": "请写5段关于自然语言处理的内容，每段以数字编号开头。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "自然语言处理",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. 自然语言处理（NLP）是人工智能的重要分支，研究如何让计算机理解、生成和处理人类语言，其目标是实现人机之间更加自然、高效的交流。\n\n2. 在技术层面，自然语言处理涵盖分词、词性标注、句法分析、语义理解等基础任务，这些任务为更复杂的语言应用奠定了基础。\n\n3. 随着深度学习的发展，基于神经网络的模型（如Transformer）显著提升了机器翻译、文本摘要和问答系统的性能，推动了NLP的快速进步。\n\n4. 自然语言处理已广泛应用于搜索引擎、智能客服、语音助手和舆情分析等场景，帮助企业和个人更高效地获取和利用信息。\n\n5. 未来，自然语言处理将更加注重跨语言、跨模态和可解释性研究，同时也需要关注数据隐私、公平性和伦理等重要问题。"
  },
  {
    "instruction": "写一段关于边缘计算的文字，必须包含以下关键词：神经网络, 人工智能, 推理。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "优化",
          4
        ]
      ]
    ],
    "topic": "边缘计算",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "边缘计算是一种将数据处理能力下沉到设备近端的技术，它通过在终端或本地节点部署神经网络模型，使人工智能能力不再完全依赖云端，从而在本地完成高效的推理任务，这不仅降低了时延和带宽消耗，还提升了数据隐私与系统可靠性，特别适用于物联网、智能制造和实时感知等场景。"
  },
  {
    "instruction": "用Markdown格式撰写关于目标检测的说明，必须包含标题和分点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "目标检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "# 目标检测（Object Detection）简介\n\n目标检测是计算机视觉中的重要研究方向，旨在从图像或视频中识别并定位特定目标。\n\n## 基本概念\n- **目标识别**：判断图像中是否存在某类目标  \n- **目标定位**：用边界框（Bounding Box）标出目标的位置  \n- **分类与回归结合**：同时完成类别判断和位置预测  \n\n## 常见方法\n- **传统方法**：  \n  - 基于手工特征（如 HOG、SIFT）  \n  - 结合滑动窗口与分类器  \n- **深度学习方法**：  \n  - 两阶段检测器（如 R-CNN、Faster R-CNN）  \n  - 单阶段检测器（如 YOLO、SSD）  \n\n## 应用场景\n- **自动驾驶**：行人、车辆、交通标志检测  \n- **安防监控**：异常行为与目标跟踪  \n- **医疗影像**：病灶区域检测  \n- **工业检测**：缺陷识别与质量控制  \n\n## 主要挑战\n- **尺度变化大**：目标大小差异明显  \n- **遮挡问题**：目标被部分遮挡  \n- **实时性要求高**：对速度和精度的平衡要求严格  \n\n## 总结\n- 目标检测是视觉智能系统的核心技术之一  \n- 随着深度学习发展，其精度和应用范围不断提升"
  },
  {
    "instruction": "介绍RAG检索增强，但不要使用'特别'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "特别"
      ]
    ],
    "topic": "RAG检索增强",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "RAG（Retrieval-Augmented Generation，检索增强生成）是一种将信息检索与大语言模型生成能力结合的方法，用于提升回答的准确性与时效性。它通过在生成答案前先从外部知识库中检索相关内容，再将检索结果作为上下文输入模型，从而减少模型“凭空编造”的风险，并让回答更贴近真实数据。\n\nRAG的基本流程包括：首先对用户问题进行向量化；然后在文档库或数据库中进行相似度检索；接着把检索到的内容与原始问题一起输入到生成模型；最后由模型基于这些信息生成答案。这种方式使模型能够利用最新、私有或领域内的数据，而不局限于训练时学到的知识。\n\n在实际应用中，RAG常用于企业知识问答、客服系统、文档搜索、代码辅助以及科研信息检索等场景。它的优势在于可扩展性强、知识更新成本低，并且能够在保持自然语言生成质量的同时，提高答案的可信度与可解释性。"
  },
  {
    "instruction": "请用列表形式列出目标检测的6个要点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "列表"
      ]
    ],
    "topic": "目标检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. 数据集与标注质量（类别定义、边界框准确性）  \n2. 特征提取（骨干网络与多尺度特征）  \n3. 候选区域/检测头设计（两阶段或单阶段）  \n4. 分类与边界框回归（类别判别与位置精度）  \n5. 损失函数与训练策略（正负样本分配、优化方法）  \n6. 评估与部署（mAP、速度、模型大小与推理效率）"
  },
  {
    "instruction": "写一篇关于迁移学习的短文，必须以问句结尾。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "迁移学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "迁移学习是一种利用已有知识来解决新问题的机器学习方法，它通过将预训练模型中的特征和经验迁移到相关任务上，显著减少数据需求和训练成本。在计算机视觉、自然语言处理等领域，迁移学习已经成为提升模型性能和效率的重要手段。随着预训练模型规模和质量的不断提升，迁移学习是否会成为未来人工智能发展的核心范式之一？"
  },
  {
    "instruction": "以问答形式介绍代码生成，包含至少4个问题和回答。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "代码生成",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**问：什么是代码生成？**  \n**答：**代码生成是指通过工具或程序，根据一定的规则、模板或模型，自动生成源代码的过程，目的是提高开发效率并减少人为错误。\n\n**问：代码生成通常应用在哪些场景？**  \n**答：**常见场景包括数据库实体类生成、接口和API代码生成、前端页面模板生成、配置文件生成，以及基于模型的自动化开发（如低代码/无代码平台）。\n\n**问：代码生成有哪些优点？**  \n**答：**主要优点包括提高开发速度、减少重复劳动、保持代码风格一致性、降低出错概率，并便于团队协作和维护。\n\n**问：代码生成有哪些常见方式或工具？**  \n**答：**常见方式包括基于模板的代码生成（如使用模板引擎）、基于模型的代码生成，以及利用AI进行代码生成；常见工具有Yeoman、Swagger Codegen、OpenAPI Generator等。"
  },
  {
    "instruction": "用三段式结构（引言、正文、结论）介绍自然语言处理。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "自然语言处理",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**引言**  \n自然语言处理（Natural Language Processing，NLP）是人工智能的重要分支，研究如何让计算机理解、生成和处理人类语言。随着互联网和大数据的发展，海量文本、语音和对话数据不断涌现，自然语言处理在信息获取和人机交互中的价值日益凸显。\n\n**正文**  \n自然语言处理融合了语言学、计算机科学和统计学等多学科方法，核心任务包括分词、句法分析、语义理解和文本生成等。典型应用有机器翻译、语音助手、情感分析、信息检索和智能客服。近年来，深度学习和大规模预训练模型的出现，使得自然语言处理在准确性和泛化能力上取得了显著突破。\n\n**结论**  \n总体而言，自然语言处理正在不断缩小人类语言与机器理解之间的差距，并深刻影响着人们获取信息和交流的方式。未来，随着算法和算力的持续进步，自然语言处理有望在教育、医疗、科研等更多领域发挥更大的作用。"
  },
  {
    "instruction": "用Markdown格式撰写关于知识蒸馏的说明，必须包含标题和分点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "知识蒸馏",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "# 知识蒸馏（Knowledge Distillation）说明\n\n- **概念定义**\n  - 知识蒸馏是一种模型压缩与迁移技术，通过让**教师模型（Teacher）**指导**学生模型（Student）**学习，从而在保持性能的同时降低模型规模与计算成本。\n\n- **核心思想**\n  - 学生模型不仅学习真实标签（Hard Labels），还学习教师模型输出的**软标签（Soft Labels）**。\n  - 软标签包含类别之间的相对关系信息，有助于提升学生模型的泛化能力。\n\n- **常见方法**\n  - **基于输出蒸馏**：对齐教师与学生的预测分布（如使用 KL 散度）。\n  - **基于特征蒸馏**：让学生学习教师的中间层特征表示。\n  - **基于关系蒸馏**：学习样本间或特征间的关系结构。\n\n- **优势**\n  - 减小模型体积，便于部署到边缘设备。\n  - 在较低计算资源下保持接近教师模型的性能。\n  - 提升小模型的训练效果与稳定性。\n\n- **应用场景**\n  - 移动端与嵌入式设备部署\n  - 大模型向小模型的能力迁移\n  - 实时推理与低延迟系统"
  },
  {
    "instruction": "以幽默风趣的方式介绍图像分割。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "图像分割",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "想象一下，图像分割就像给一张照片“分家产”。\n\n一张照片里，天空、草地、猫、咖啡杯全都挤在一起，图像分割站出来说：“别挤别挤，按区域排队！”于是它拿着一把“像素手术刀”，把属于天空的像素分到“天空区”，把猫完整地圈出来，郑重其事地告诉电脑：“这是一只猫，不是一团会喵喵叫的马赛克。”\n\n简单说，**图像分割就是让计算机学会‘哪里到哪里算一回事’**。它不再只看整张图，而是像强迫症晚期一样，把每个像素都安排得明明白白：这是前景，那是背景；这是人，那不是垃圾桶。分得越准，电脑就越像人；分得越乱，电脑就越像没戴眼镜。\n\n所以，下次你看到自动抠图、无人驾驶识别行人、医学影像里精准圈出病灶时，别忘了——幕后那位认真分家产的“图像分割”，正在默默加班。"
  },
  {
    "instruction": "用简洁的技术文档风格说明联邦学习。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "联邦学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**联邦学习（Federated Learning）技术概述**\n\n**1. 定义**  \n联邦学习是一种分布式机器学习范式，在不集中原始数据的前提下，通过多方协作训练共享模型。\n\n**2. 架构**  \n- **客户端（Client）**：本地持有数据，执行本地训练  \n- **服务器（Server）**：协调训练流程，聚合模型参数\n\n**3. 工作流程**  \n1. 服务器初始化并下发模型  \n2. 客户端基于本地数据训练  \n3. 客户端上传模型更新（如梯度/权重）  \n4. 服务器进行安全聚合并更新全局模型  \n5. 重复迭代直至收敛\n\n**4. 核心优势**  \n- 数据不出本地，提升隐私保护  \n- 减少数据传输成本  \n- 支持跨机构协作\n\n**5. 关键挑战**  \n- 系统与数据异构性  \n- 通信效率与稳定性  \n- 隐私与安全攻击防护\n\n**6. 典型应用**  \n- 移动设备个性化  \n- 医疗与金融跨机构建模  \n- 边缘计算场景"
  },
  {
    "instruction": "用Markdown格式撰写关于目标检测的说明，必须包含标题和分点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "目标检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "# 目标检测（Object Detection）简介\n\n目标检测是计算机视觉中的一项重要任务，旨在在图像或视频中**定位并识别**特定目标。\n\n## 核心概念\n- **定位（Localization）**：通过边界框（Bounding Box）确定目标在图像中的位置  \n- **分类（Classification）**：判断检测到的目标属于哪一类别  \n- **多目标处理**：同一图像中可同时检测多个不同目标  \n\n## 常见应用\n- **自动驾驶**：检测行人、车辆、交通标志  \n- **安防监控**：识别人、异常行为或危险物体  \n- **医疗影像**：定位病灶或异常区域  \n- **工业质检**：检测产品缺陷  \n\n## 主流方法\n- **两阶段方法**：如 R-CNN、Faster R-CNN，精度高但速度较慢  \n- **单阶段方法**：如 YOLO、SSD，速度快，适合实时场景  \n\n## 评估指标\n- **IoU（交并比）**：衡量预测框与真实框的重叠程度  \n- **Precision / Recall**：评估检测准确性与完整性  \n- **mAP（平均精度均值）**：综合评价模型性能  \n\n## 面临挑战\n- **小目标检测困难**  \n- **复杂背景干扰**  \n- **实时性与精度的平衡**"
  },
  {
    "instruction": "用简洁的技术文档风格说明情感分析。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "情感分析",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**情感分析（Sentiment Analysis）**\n\n**定义**  \n情感分析是自然语言处理（NLP）任务之一，用于自动识别文本中表达的情绪倾向或主观态度。\n\n**输入/输出**  \n- 输入：文本数据（句子、评论、文档等）  \n- 输出：情感标签（如正面/中性/负面）或情感分值（连续数值）\n\n**常见方法**  \n- 基于规则：情感词典与规则匹配  \n- 传统机器学习：特征工程 + 分类器（如SVM、朴素贝叶斯）  \n- 深度学习：RNN、CNN、Transformer（如BERT）\n\n**评估指标**  \n- 准确率（Accuracy）  \n- 精确率/召回率（Precision/Recall）  \n- F1值\n\n**应用场景**  \n- 用户评论分析  \n- 舆情监测  \n- 客户满意度评估  \n- 市场与品牌分析"
  },
  {
    "instruction": "以问答形式介绍自然语言处理，包含至少4个问题和回答。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "自然语言处理",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**问1：什么是自然语言处理（NLP）？**  \n**答：**自然语言处理是人工智能的一个分支，研究如何让计算机理解、生成和处理人类语言，如中文、英文等。\n\n**问2：自然语言处理能做哪些事情？**  \n**答：**NLP可以用于机器翻译、文本分类、情感分析、语音识别、智能问答、文本摘要等应用。\n\n**问3：自然语言处理的核心技术有哪些？**  \n**答：**常见技术包括分词、词性标注、句法分析、语义理解、语言模型以及基于深度学习的神经网络方法。\n\n**问4：自然语言处理在生活中有哪些实际应用？**  \n**答：**常见应用有搜索引擎、智能客服、语音助手（如Siri、智能音箱）、垃圾邮件过滤和推荐系统等。\n\n**问5：学习自然语言处理需要哪些基础？**  \n**答：**通常需要一定的编程能力（如Python）、数学基础（概率与线性代数）以及对语言学和机器学习的基本理解。"
  },
  {
    "instruction": "介绍语音识别，但不要使用'很'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "很"
      ]
    ],
    "topic": "语音识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "语音识别是人工智能领域的重要技术之一，目标是让计算机理解和转写人类语音。系统通常通过麦克风采集声音信号，经过降噪、特征提取和声学建模，将连续语音转换为文字或指令。\n\n随着深度学习和大规模数据的发展，语音识别在准确率、实时性和适应性方面取得显著提升，已应用于智能助手、车载系统、客服机器人和无障碍辅助等场景，为人机交互提供更加自然、高效的方式。"
  },
  {
    "instruction": "描述提示工程，开头必须是'在当今时代'。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "提示工程",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "在当今时代，提示工程是指通过精心设计和优化对人工智能模型的输入提示，以引导其产生更准确、更有用和更可控输出的方法与实践。它融合了对模型能力与局限的理解、语言表达技巧以及任务目标的拆解，通过明确角色、约束条件、示例和步骤等要素，提升模型在写作、编程、分析、决策支持等场景中的表现，从而降低试错成本并放大人机协作的效率与价值。"
  },
  {
    "instruction": "撰写异常检测的介绍，每段必须包含'参数'。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "参数",
          2
        ]
      ]
    ],
    "topic": "异常检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "异常检测是一种用于识别数据中偏离正常模式行为的方法，广泛应用于金融风控、网络安全和工业监测等领域。通过分析历史数据分布，模型可以学习正常状态下的特征，并利用关键**参数**来刻画系统的常规运行范围。\n\n在具体实现中，异常检测方法可以分为基于统计、基于机器学习以及基于深度学习的不同类型。不同方法对数据规模、维度和实时性的要求不同，需要根据业务场景合理选择模型及其**参数**配置，以平衡检测准确率和计算成本。\n\n随着数据复杂度的提升，异常检测也面临着可解释性和鲁棒性的挑战。研究人员通常通过调整模型**参数**、引入领域知识以及结合多模型策略，来提升检测效果并降低误报率，从而更好地服务实际应用需求。"
  },
  {
    "instruction": "用通俗易懂的语言向初学者解释代码生成。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "代码生成",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "代码生成，简单来说，就是**让电脑帮你写代码**。\n\n你可以把它想象成：  \n你对电脑说一句“人话”（比如需求或描述），电脑就把这句话翻译成程序员能用的代码。\n\n**举个生活中的例子：**  \n- 你说：“帮我做一张购物清单。”  \n- 电脑根据你的要求，自动写好一份清单。  \n\n代码生成也是一样：  \n- 你告诉电脑：“我需要一个能计算两个数之和的程序。”  \n- 代码生成工具就会自动写出相应的代码，比如用 Python、Java 或其他语言。\n\n**它是怎么做到的？**  \n- 电脑提前学习了大量代码和规则  \n- 知道“这种需求通常要用那样的代码”  \n- 然后按照常见写法帮你生成一份可用的代码\n\n**对初学者有什么好处？**  \n- 不用从零开始写代码  \n- 可以快速看到示例，学习代码是怎么写的  \n- 减少重复、枯燥的工作\n\n一句话总结：  \n👉 **代码生成就是“你说需求，电脑写代码”，帮助人更快、更轻松地编程。**"
  },
  {
    "instruction": "请写5段关于大语言模型的内容，每段以数字编号开头。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "大语言模型",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. 大语言模型（LLM）是一类基于深度学习的人工智能模型，通常采用变换器架构，通过对海量文本数据的学习来理解和生成自然语言。它们能够捕捉语言中的语义、语法和上下文关系，从而实现高质量的文本生成。\n\n2. 在训练过程中，大语言模型使用无监督或自监督学习方法，从互联网、书籍和文档等多源数据中学习统计规律。模型参数规模往往达到数十亿甚至更多，这使其具备强大的表达能力，但也带来了计算和能耗方面的挑战。\n\n3. 大语言模型在实际应用中表现出广泛价值，例如智能客服、机器翻译、内容创作和代码辅助等领域。它们能够提升工作效率，并在一定程度上降低专业技能的使用门槛。\n\n4. 尽管能力强大，大语言模型仍存在局限性，包括可能生成不准确或带有偏见的内容，以及对事实和推理的可靠性不足。这些问题促使研究者不断改进数据治理、模型结构和对齐方法。\n\n5. 展望未来，大语言模型的发展方向包括更高效的训练方式、更强的多模态理解能力以及更安全可控的行为机制。随着技术进步，它们有望在教育、科研和产业创新中发挥更加重要的作用。"
  },
  {
    "instruction": "请用不少于300个字描述推荐系统。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        300
      ]
    ],
    "topic": "推荐系统",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "推荐系统是一类通过分析用户行为、兴趣偏好以及环境信息，为用户主动提供个性化内容或服务的重要技术系统。它广泛应用于电子商务、新闻资讯、短视频、音乐、社交网络和在线教育等领域，其核心目标是在信息过载的环境中，帮助用户更高效地发现有价值的内容，同时提升平台的转化率和用户黏性。推荐系统通常以数据为基础，数据来源包括用户的显式反馈（如评分、点赞、收藏）和隐式反馈（如点击、浏览时长、购买记录），并结合用户属性、内容特征以及上下文信息进行综合建模。\n\n从方法上看，传统推荐系统主要包括基于内容的推荐和协同过滤推荐。基于内容的推荐通过分析物品本身的特征，向用户推荐与其历史兴趣相似的内容；协同过滤则通过挖掘用户与用户、物品与物品之间的相似性来进行推荐。随着数据规模和计算能力的提升，混合推荐方法和基于机器学习、深度学习的推荐模型逐渐成为主流，如矩阵分解、因子分解机、深度神经网络以及大模型驱动的推荐架构。这些方法能够更好地捕捉复杂的非线性关系，提高推荐的准确性和多样性。\n\n在实际应用中，推荐系统还需要面对冷启动、数据稀疏、推荐偏置、隐私保护和公平性等挑战，因此需要在算法设计、系统架构和业务策略之间不断权衡。未来，推荐系统将更加注重可解释性、实时性以及对用户长期价值的关注，从“猜你喜欢”逐步演进为“真正理解你”的智能信息服务系统。"
  },
  {
    "instruction": "写一段关于强化学习的文字，必须包含以下关键词：训练, 参数, 损失函数。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "推理",
          4
        ]
      ]
    ],
    "topic": "强化学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "强化学习是一种通过与环境交互来学习最优策略的方法，在训练过程中，智能体根据获得的奖励不断调整自身的参数，以提升决策效果，通常会借助损失函数来衡量当前策略与理想目标之间的差距，并通过最小化损失函数来引导模型逐步收敛，从而实现性能的持续改进。"
  },
  {
    "instruction": "以幽默风趣的方式介绍机器学习。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "机器学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "想象一下，你在教一只非常“勤奋但有点笨”的小狗认东西——这只小狗就是**机器学习**。\n\n一开始你说：“这是猫。”  \n小狗点头。  \n你又说：“这也是猫。”  \n小狗继续点头。  \n当你拿出一只老虎，小狗兴奋地喊：“猫！”  \n恭喜你，小狗已经学会了**模式识别**，虽然有点过拟合。\n\n机器学习的核心思想其实很朴素：**不手写规则，而是让机器自己从数据里总结规律**。就像你不会给孩子写一本《如何识别所有自行车的500条规则》，而是带他多看、多骑、多摔几次——机器学习也是这么“摔”出来的。\n\n举个常见例子：  \n- 垃圾邮件过滤器，看了几百万封邮件后，悟了：“这类标题一般不靠谱。”  \n- 推荐系统，偷偷记住你凌晨三点还在看搞笑视频，然后假装“贴心”地继续推荐。  \n- 人脸识别，通过大量照片学会：这张脸是你，那张脸是你戴墨镜的你。\n\n当然，机器学习也有烦恼：  \n数据太少，它会胡思乱想；  \n数据太脏，它会学坏；  \n你问它“为什么这么判断”，它往往回答：“我感觉是这样。”  \n\n总之，**机器学习就是：用数据喂机器，用算法调教它，然后祈祷它别在关键时刻犯傻**。听起来不太科学，但 surprisingly，好用。"
  },
  {
    "instruction": "介绍神经网络，但不要使用'极其'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "极其"
      ]
    ],
    "topic": "神经网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "神经网络是一类受生物神经系统启发的机器学习模型，旨在让计算机从数据中自动学习规律。它由多个“神经元”组成，这些神经元按层次结构连接，常见包括输入层、若干隐藏层和输出层。每个神经元会对输入进行加权求和，并通过激活函数产生输出，从而实现对复杂关系的表达。\n\n在训练过程中，神经网络通过损失函数衡量预测结果与真实结果的差距，并利用反向传播算法不断调整权重，使模型性能逐步提升。由于具备强大的非线性建模能力，神经网络被广泛应用于图像识别、语音处理、自然语言理解、推荐系统等领域，是现代人工智能的重要基础之一。"
  },
  {
    "instruction": "请用JSON格式输出计算机视觉的关键信息。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "计算机视觉",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "```json\n{\n  \"定义\": \"计算机视觉是研究如何使计算机从图像或视频中获取、理解和分析信息的人工智能领域。\",\n  \"核心任务\": [\n    \"图像分类\",\n    \"目标检测\",\n    \"语义分割\",\n    \"实例分割\",\n    \"人脸识别\",\n    \"姿态估计\",\n    \"视频理解\"\n  ],\n  \"关键技术\": [\n    \"卷积神经网络（CNN）\",\n    \"视觉Transformer（ViT）\",\n    \"深度学习\",\n    \"特征提取与表示学习\",\n    \"多模态学习\"\n  ],\n  \"常用数据集\": [\n    \"ImageNet\",\n    \"COCO\",\n    \"MNIST\",\n    \"CIFAR-10/100\",\n    \"Open Images\"\n  ],\n  \"应用领域\": [\n    \"自动驾驶\",\n    \"医学影像分析\",\n    \"安防监控\",\n    \"工业质检\",\n    \"增强现实与虚拟现实\"\n  ],\n  \"主要挑战\": [\n    \"复杂场景理解\",\n    \"小样本与零样本学习\",\n    \"模型泛化能力\",\n    \"计算资源与效率\",\n    \"数据隐私与伦理问题\"\n  ],\n  \"常用工具与框架\": [\n    \"OpenCV\",\n    \"PyTorch\",\n    \"TensorFlow\",\n    \"Detectron2\",\n    \"MMCV\"\n  ]\n}\n```"
  },
  {
    "instruction": "用简洁的技术文档风格说明注意力机制。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "注意力机制",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**注意力机制（Attention Mechanism）**\n\n**定义**  \n注意力机制是一种在序列建模中动态分配权重的方法，用于让模型在处理当前元素时重点关注与其最相关的上下文信息。\n\n**核心思想**  \n通过计算相关性权重，对输入信息进行加权汇聚，提升关键信息的表达能力。\n\n**基本组成**  \n- **Query（Q）**：当前查询向量  \n- **Key（K）**：用于匹配的特征向量  \n- **Value（V）**：被加权汇聚的信息向量\n\n**计算流程**  \n1. 计算相关性分数：`score = Q · K^T`  \n2. 归一化权重：`α = softmax(score)`  \n3. 加权求和输出：`output = α · V`\n\n**常见类型**  \n- 自注意力（Self-Attention）  \n- 多头注意力（Multi-Head Attention）  \n- 交叉注意力（Cross-Attention）\n\n**优点**  \n- 捕获长距离依赖  \n- 支持并行计算  \n- 提升表示能力与模型性能"
  },
  {
    "instruction": "在描述命名实体识别时，请确保'特征提取'这个词出现至少4次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "特征提取",
          4
        ]
      ]
    ],
    "topic": "命名实体识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "命名实体识别（Named Entity Recognition，NER）是自然语言处理中的一项核心任务，目标是从文本中识别并分类具有特定意义的实体，如人名、地名、组织机构名和时间等。在这一过程中，**特征提取**起着至关重要的作用，因为模型需要通过**特征提取**从原始文本中捕捉有判别力的信息，例如词形、词性、上下文窗口以及语义表示。传统方法通常依赖人工设计的规则和统计特征，其性能在很大程度上取决于**特征提取**的质量和覆盖面。随着深度学习的发展，神经网络模型能够自动完成多层次的**特征提取**，减少人工干预，并通过词向量、上下文编码等方式进一步提升识别效果。因此，无论是在传统方法还是现代深度学习方法中，**特征提取**始终是命名实体识别系统成功的关键环节。"
  },
  {
    "instruction": "在描述深度学习时，请确保'深度学习'这个词出现至少4次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "深度学习",
          4
        ]
      ]
    ],
    "topic": "深度学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "深度学习是机器学习的重要分支。通过多层神经网络，深度学习能够自动学习特征表示，使深度学习在图像、语音和自然语言处理中表现突出。随着算力和数据增长，深度学习的效果持续提升，深度学习也推动了人工智能的应用落地。"
  },
  {
    "instruction": "撰写模型量化的介绍，每段必须包含'训练'。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "训练",
          2
        ]
      ]
    ],
    "topic": "模型量化",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "模型量化是一种在保持模型性能的同时降低计算与存储成本的技术，通常通过将高精度参数转换为低精度表示来实现。在这一过程中，量化既可以发生在训练后，也可以融入训练阶段，以更好地适配目标硬件并减少精度损失，训练数据的分布对效果有重要影响。\n\n常见的模型量化方法包括后训练量化和量化感知训练，其中后训练量化实现简单、部署快速，而量化感知训练在训练时模拟低精度运算，从而让模型在训练阶段就适应量化误差，通常能获得更稳定的推理精度。\n\n模型量化的优势在于显著降低模型大小和推理延迟，适合在边缘设备和移动端部署，但也面临精度下降和调参复杂等挑战，因此需要结合具体任务与训练策略进行权衡与优化。"
  },
  {
    "instruction": "在描述神经网络时，请确保'损失函数'这个词出现至少4次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "损失函数",
          4
        ]
      ]
    ],
    "topic": "神经网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "神经网络是一种由多层神经元组成的模型，通过前向传播计算输出，并通过损失函数衡量预测与真实值之间的差距。在训练过程中，损失函数的设计直接影响模型学习的目标和效果。反向传播算法会根据损失函数对参数的梯度来更新权重，从而逐步降低损失函数的数值。不同任务会选择不同的损失函数，例如分类问题常用交叉熵，回归问题常用均方误差。"
  },
  {
    "instruction": "在描述机器学习时，请确保'推理'这个词出现至少4次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "推理",
          4
        ]
      ]
    ],
    "topic": "机器学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "机器学习是让计算机从数据中学习规律的技术。在训练阶段，模型通过优化参数来提高预测能力；在应用阶段，模型进行推理，根据输入数据给出结果。推理不仅包括分类和回归，还涉及序列预测与决策。为了保证可靠性，推理需要高质量数据与合理假设。现代系统还强调可解释性，使推理过程透明。通过持续学习与评估，推理性能不断提升。"
  },
  {
    "instruction": "用通俗易懂的语言向初学者解释推荐系统。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "推荐系统",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "推荐系统，简单说，就是**帮你“猜”你可能喜欢什么的程序**。\n\n比如你在淘宝看过运动鞋，之后首页就总给你推荐鞋子；你在抖音多看了几条美食视频，后面就全是吃的。这不是巧合，而是推荐系统在工作。\n\n它是怎么做到的呢？主要做三件事：  \n1️⃣ **收集信息**：记录你点过什么、看过什么、买过什么（比如浏览记录、点赞、购买）。  \n2️⃣ **找规律**：分析你的行为，看看你和哪些人“口味相似”，或者你过去喜欢什么类型的内容。  \n3️⃣ **做推荐**：根据这些规律，把你“可能感兴趣”的东西排在前面给你看。\n\n你可以把推荐系统想成一个**很懂你的店员**：它记住了你的喜好，再根据你的习惯帮你挑东西。你用得越多，它了解你越多，推荐也就越“准”。\n\n总的来说，推荐系统的目的就是：**帮用户更快找到想要的内容，也帮平台把合适的东西推荐给合适的人**。"
  },
  {
    "instruction": "用正式的学术语言描述分布式训练。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "分布式训练",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "分布式训练（distributed training）是指在多计算节点或多加速设备（如 GPU、TPU）组成的计算环境中，通过协同计算完成机器学习或深度学习模型参数优化的训练范式。其核心目标在于通过并行化计算与存储，缓解单节点在计算能力、内存容量与训练时间方面的瓶颈，从而提升模型训练的效率与可扩展性。根据并行粒度与任务划分方式的不同，分布式训练通常包括数据并行、模型并行以及流水线并行等基本形式；其中，数据并行通过在不同节点上复制模型并分配不同数据子集进行计算，并借助参数同步机制（如同步或异步梯度聚合）实现一致性更新。分布式训练系统还涉及通信拓扑设计、同步策略、一致性模型以及容错机制等关键问题，其性能与稳定性在很大程度上依赖于底层通信效率、算法收敛特性与系统调度策略的协同优化。因此，分布式训练已成为支撑大规模模型学习与高性能人工智能计算的重要基础技术。"
  },
  {
    "instruction": "写一段关于迁移学习的文字，字数要求在100到250之间。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        100
      ],
      [
        "max_length",
        250
      ]
    ],
    "topic": "迁移学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "迁移学习是一种利用已有知识解决新任务的机器学习方法。它通过将源领域中学到的模型参数、特征表示或结构迁移到目标领域，减少对大量标注数据的依赖。迁移学习能够显著提升训练效率和模型性能，尤其适用于数据稀缺或训练成本高的场景，在计算机视觉、自然语言处理等领域得到广泛应用。"
  },
  {
    "instruction": "请写4段关于RAG检索增强的内容，每段以数字编号开头。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "RAG检索增强",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. RAG（Retrieval-Augmented Generation，检索增强生成）是一种将信息检索与大语言模型生成能力相结合的技术，其核心思想是在生成回答前，先从外部知识库中检索相关内容，从而弥补模型自身参数知识的局限性。\n\n2. 在实际流程中，RAG通常包括向量化、相似度检索和文本生成三个阶段，系统会先将用户问题转化为向量，再从向量数据库中找出最相关的文档片段，最后将这些内容作为上下文输入给大模型生成答案。\n\n3. 相比纯生成模型，RAG在准确性、可解释性和时效性方面具有明显优势，它可以基于最新或私有的数据进行回答，减少“幻觉”问题，并支持对答案来源进行追溯和验证。\n\n4. RAG已广泛应用于智能问答、企业知识库、客服系统和专业领域助手等场景，随着向量数据库和大模型能力的不断提升，RAG正逐渐成为构建可靠AI应用的关键架构之一。"
  },
  {
    "instruction": "用简洁的技术文档风格说明自然语言处理。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "自然语言处理",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**自然语言处理（NLP）概述**\n\n**定义**  \n自然语言处理是人工智能与语言学的交叉领域，研究计算机对人类自然语言的理解、生成与交互。\n\n**目标**  \n- 将非结构化文本/语音转化为可计算表示  \n- 实现语言理解、推理与生成\n\n**核心任务**  \n- 文本处理：分词、词性标注、句法/语义分析  \n- 语义建模：命名实体识别、关系抽取、情感分析  \n- 生成与交互：机器翻译、文本生成、对话系统\n\n**技术方法**  \n- 规则与统计方法  \n- 机器学习与深度学习（如Transformer、预训练语言模型）\n\n**应用场景**  \n- 搜索与推荐、智能客服、信息抽取、内容审核\n\n**挑战**  \n- 语义歧义、领域迁移、多语言与低资源问题"
  },
  {
    "instruction": "描述异常检测的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "特别"
      ]
    ],
    "topic": "异常检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "异常检测的主要特点包括：\n\n- 关注少数偏离常态的数据，用于发现潜在问题或新现象。  \n- 对数据分布假设依赖有限，适用于多种数据类型与场景。  \n- 正常样本与异常样本数量不平衡，标注数据获取存在难度。  \n- 对噪声与数据质量敏感，需要配合数据清洗与特征工程。  \n- 方法选择多样，涵盖统计方法、距离方法、密度方法与模型方法。  \n- 结果解释性在应用中占据重要位置，便于定位原因与采取措施。"
  },
  {
    "instruction": "请用不少于150个字描述注意力机制。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        150
      ]
    ],
    "topic": "注意力机制",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "注意力机制是一种模拟人类在处理信息时选择性关注重要部分的计算方法，广泛应用于自然语言处理、计算机视觉和多模态任务中。其核心思想是在大量输入信息中"
  },
  {
    "instruction": "以幽默风趣的方式介绍模型压缩。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "模型压缩",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "想象一下，模型压缩就像给一个“吃太多的AI”做减肥。\n\n原本的模型：参数多到像春节后的你，走两步就喘，占内存、费算力、跑得还慢。  \n模型压缩：营养师 + 健身教练 + 断舍离大师联手出击！\n\n常见“减肥秘籍”包括——  \n- **剪枝（Pruning）**：把“几乎不用但还占座位”的参数统统裁掉，相当于清理衣柜里三年没穿的衣服。模型：轻松多了！  \n- **量化（Quantization）**：把原本精致的“64位高档西餐”换成“8位营养快餐”，味道差不多，速度却快多了。  \n- **知识蒸馏（Distillation）**：让学霸大模型当家教，把知识教给小模型——小模型体型小，但脑子不小。\n\n最终效果？  \n模型成功瘦身：**更小、更快、更省钱**，还能在手机、边缘设备上健步如飞。  \n一句话总结：模型压缩不是把AI饿瘦，而是让它**减脂不减智商**。"
  },
  {
    "instruction": "请用不少于100个字描述神经网络。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        100
      ]
    ],
    "topic": "神经网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "神经网络是一种受人类大脑神经元结构启发的计算模型，由输入层、隐藏层和输出层组成。网络通过大量带权连接进行信息传递，利用非线性激活函数表达复杂关系。通过反向传播和梯度下降等算法，模型能够从数据中自动学习特征，在图像识别、语音理解、自然语言处理和决策预测等领域取得了显著成效，同时也面临可解释性和能耗等挑战。"
  },
  {
    "instruction": "在描述自然语言处理时，请确保'训练'这个词出现至少3次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "训练",
          3
        ]
      ]
    ],
    "topic": "自然语言处理",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "自然语言处理是人工智能的重要分支，旨在让计算机理解、生成和分析人类语言。通过大规模语料的训练，模型可以学习词汇、语法和语义关系；在监督或无监督的训练过程中，算法不断调整参数以提升性能。随着深度学习的发展，更复杂的训练方法使系统在翻译、问答和对话等任务上表现更好，而持续的训练也有助于模型适应新的领域和语言。"
  },
  {
    "instruction": "用通俗易懂的语言向初学者解释RAG检索增强。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "RAG检索增强",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "RAG（检索增强生成，Retrieval-Augmented Generation）可以理解为：**让大模型“先查资料，再回答问题”**。\n\n我们一步一步用通俗的话说清楚：\n\n**1️⃣ 普通大模型的问题**\n普通的大模型就像一个“记忆力很强但不查书”的学生。  \n它只靠自己以前学过的内容来回答问题：\n- 如果问题很新（比如公司内部文档）\n- 或者很专业、很具体  \n它可能会**答不准，甚至胡编**。\n\n**2️⃣ RAG 是怎么改进的？**\nRAG 给大模型加了一个“查资料”的能力。\n\n流程很简单：\n1. **你提问**\n2. 系统先去**知识库里检索相关资料**（比如文档、PDF、数据库）\n3. 把找到的内容**一起交给大模型**\n4. 大模型**基于这些真实资料来生成答案**\n\n所以不是“凭空想”，而是“**有依据地回答**”。\n\n**3️⃣ 打个比方**\n- 不用 RAG：  \n  👉 像考试时**不让翻书**，全靠记忆\n- 用了 RAG：  \n  👉 像考试时**先翻教材，再组织答案**\n\n答案自然更准、更可靠。\n\n**4️⃣ RAG 常用来做什么？**\n- 企业内部知识问答\n- 客服机器人\n- 法律 / 医疗 / 技术文档查询\n- 让 AI 回答“你自己的资料”\n\n**一句话总结：**  \n👉 **RAG = 检索（查资料） + 生成（写答案）**  \n它让 AI 不只会“说话”，还会“查证后再说”。"
  },
  {
    "instruction": "用通俗易懂的语言向初学者解释机器翻译。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "机器翻译",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "机器翻译，简单来说，就是**让电脑把一种语言自动变成另一种语言**。\n\n你可以把它想象成一个“会多国语言的机器人”。当你输入一句中文，比如“你好，世界”，机器翻译会分析这句话的意思，然后用它学到的知识，把意思用另一种语言表达出来，比如英文的“Hello, world”。\n\n那电脑是怎么做到的呢？  \n电脑本身不懂语言，它是**通过大量学习**实现的。研究人员把海量的中英文对照句子喂给电脑，让它学会：这个词通常对应哪个词，这种句子结构该怎么变。现在常见的机器翻译大多使用**人工智能和神经网络**，有点像在模仿人类大脑学习语言的方式。\n\n机器翻译很方便，比如出国旅行、看外文资料时都能用。但它也不是完美的，有时会翻译得不够自然，或者在双关语、成语、文化背景上出错。所以，它**适合快速理解大意**，但重要、正式的内容，最好还是让人来校对。"
  },
  {
    "instruction": "在描述提示工程时，请确保'反向传播'这个词出现至少2次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "反向传播",
          2
        ]
      ]
    ],
    "topic": "提示工程",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "提示工程是一种通过精心设计输入提示来引导模型产生更高质量输出的方法，它强调对任务目标、上下文信息和约束条件的清晰表达。虽然提示工程不同于传统模型训练中的参数更新，但在概念上可以类比学习过程中的反馈机制，例如在深度学习中通过反向传播来不断修正模型参数。理解模型如何根据提示进行推理，有助于像反向传播那样迭代优化提示内容，从而逐步提升结果的准确性和稳定性。在一些高级应用中，人们甚至会将提示的改进过程视为一种“无梯度的反向传播”思维方式。"
  },
  {
    "instruction": "用正式的学术语言描述分布式训练。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "分布式训练",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "分布式训练（distributed training）是指在多计算节点或多处理单元（如多核 CPU、GPU 或专用加速器）上协同执行模型训练任务的一种计算范式。其核心思想是通过将模型参数、训练数据或计算过程在不同节点之间进行划分与并行化处理，以提升计算效率、缩短训练时间，并突破单一计算资源在存储容量和算力方面的限制。在实现方式上，分布式训练通常包括数据并行、模型并行以及混合并行等策略，并依赖高效的通信机制（如参数服务器架构或基于集体通信的同步机制）来保证各节点之间参数更新的一致性与收敛性。\n\n从理论与实践角度看，分布式训练在大规模机器学习与深度学习任务中具有重要意义，尤其适用于超大规模数据集和高复杂度模型。然而，其性能和稳定性受到通信开销、负载均衡、同步策略以及系统异构性等因素的显著影响。因此，分布式训练的研究不仅涉及算法设计与优化问题，也涵盖并行计算体系结构、分布式系统以及数值优化等多个学科领域。"
  },
  {
    "instruction": "用正式的学术语言描述知识蒸馏。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "知识蒸馏",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "知识蒸馏（Knowledge Distillation）是一种模型压缩与迁移学习技术，其核心思想是将性能较强、结构复杂的教师模型（Teacher Model）中所蕴含的知识迁移到结构更为简洁、计算效率更高的学生模型（Student Model）中。在训练过程中，学生模型不仅学习来自真实标签的监督信号，还通过拟合教师模型输出的软目标（soft targets）来获取更丰富的类别间关系信息。这些软目标通常由教师模型在引入温度参数的软化输出分布中获得，能够揭示不同类别之间的相对相似性，从而为学生模型提供比硬标签更具信息量的学习信号。通过联合优化蒸馏损失与传统监督损失，知识蒸馏在显著降低模型参数规模和计算复杂度的同时，能够有效保持甚至在某些情况下提升模型的泛化性能，因此在资源受限场景和模型部署中具有重要的理论价值与实际意义。"
  },
  {
    "instruction": "描述强化学习的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "很"
      ]
    ],
    "topic": "强化学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "强化学习的主要特点包括：\n\n- 通过与环境交互进行学习，智能体根据当前状态选择动作并获得反馈。  \n- 以奖励信号作为学习依据，目标是累计回报最大化。  \n- 不依赖明确的监督标签，学习过程由试错驱动。  \n- 同时涉及探索与利用，在未知行为与已知策略之间进行权衡。  \n- 决策具有序列性，当前动作会影响后续状态与长期结果。  \n- 可建模为马尔可夫决策过程，包含状态、动作、奖励和策略等要素。"
  },
  {
    "instruction": "用通俗易懂的语言向初学者解释注意力机制。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "注意力机制",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "注意力机制可以把它想成**人看东西或读文章时“把注意力放在重点上”**的能力。\n\n比如你读一句话：“我昨天在公园里看到一只可爱的猫。”如果别人问你“可爱的是什么？”，你会自然把注意力放在“猫”这个词上，而不是“昨天”或“公园”。**注意力机制就是让计算机学会做同样的事：在一堆信息里，自动找出当前最重要的部分。**\n\n在人工智能里，模型一次会看到很多信息（比如一句话里的所有词，或一张图片里的所有区域）。注意力机制会做三件事：  \n1️⃣ 看所有信息  \n2️⃣ 判断每一部分“有多重要”  \n3️⃣ 把重要的信息多看一点，不重要的少看一点  \n\n举个更直观的例子：  \n- **没有注意力机制**：像平均地看每个词，所有词都一样重要  \n- **有注意力机制**：像打手电筒，光照得最亮的地方就是最关键的信息  \n\n正因为有了注意力机制，模型在**翻译、聊天、看图说话**等任务上表现大大提升。简单来说，**注意力机制就是让机器学会“抓重点”**。"
  },
  {
    "instruction": "请用不少于150个字描述文本分类。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        150
      ]
    ],
    "topic": "文本分类",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "文本分类是自然语言处理领域中的一项基础而重要的任务，主要指将大量无结构或半结构的文本数据，按照预先设定的类别或标签体系进行自动归类。其核心在于从文本中提取能够反映语义和主题的信息特征，使计算机具备对文本内容进行理解和判断的能力。早期的文本分类方法多依赖人工设计特征，例如词频、词频逆文档频率以及主题分布等，再结合朴素贝叶斯、支持向量机等传统机器学习算法完成分类。随着深度学习技术的发展，神经网络模型逐渐成为主流，能够自动学习高层语义特征，显著提升分类的准确性和泛化能力。文本分类广泛应用于垃圾邮件过滤、情感分析、新闻主题划分、"
  },
  {
    "instruction": "用简洁的技术文档风格说明代码生成。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "代码生成",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**代码生成（Code Generation）**\n\n**概述**  \n代码生成是将高层描述（如需求、模型或配置）自动转换为可执行源代码的过程。\n\n**输入**  \n- 需求说明、设计模型（UML/DSL）  \n- 模板与规则  \n- 配置参数\n\n**过程**  \n1. 解析输入描述  \n2. 应用生成规则与模板  \n3. 输出目标语言代码  \n4. 可选：格式化与校验\n\n**输出**  \n- 源代码文件  \n- 相关配置或构建脚本\n\n**优势**  \n- 提高开发效率  \n- 减少重复与人为错误  \n- 保持一致性\n\n**注意事项**  \n- 模板与规则需版本管理  \n- 生成代码可维护性与可读性  \n- 与手写代码的边界定义"
  },
  {
    "instruction": "用Markdown格式撰写关于联邦学习的说明，必须包含标题和分点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "联邦学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "# 联邦学习（Federated Learning）说明\n\n## 什么是联邦学习\n- 联邦学习是一种**分布式机器学习**方法  \n- 数据**不出本地**，仅共享模型参数或梯度  \n- 由多个参与方在保护隐私的前提下**协同训练模型**\n\n## 基本流程\n- 各客户端在本地使用私有数据训练模型  \n- 将本地更新的模型参数上传至服务器  \n- 服务器对参数进行**聚合（如加权平均）**  \n- 将聚合后的全局模型下发给各客户端继续训练\n\n## 主要优势\n- **隐私保护**：原始数据不集中存储，降低泄露风险  \n- **合规性强**：有助于满足数据保护法规要求  \n- **资源利用高**：充分利用边缘设备的计算能力\n\n## 面临的挑战\n- **通信成本高**：频繁的模型参数传输  \n- **数据异质性**：不同客户端数据分布不一致  \n- **系统可靠性**：设备掉线或性能差异影响训练效果\n\n## 典型应用场景\n- **移动设备输入法与推荐系统**  \n- **医疗健康数据建模**  \n- **金融风控与反欺诈系统**"
  },
  {
    "instruction": "撰写模型量化的介绍，每段必须包含'人工智能'。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "人工智能",
          4
        ]
      ]
    ],
    "topic": "模型量化",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "模型量化是指在人工智能模型的训练或部署阶段，将高精度的浮点数参数与计算转换为低精度表示（如 INT8），以减少模型体积和计算开销。通过量化，人工智能系统能够在保持可接受精度的前提下，更高效地运行在资源受限的设备上。\n\n常见的模型量化方法包括后训练量化、量化感知训练以及动态量化等，这些方法在人工智能应用中各有适用场景。后训练量化实现简单、部署成本低，而量化感知训练通过在训练阶段引入量化误差建模，通常能获得更高的精度表现。\n\n在实际应用中，模型量化显著提升了人工智能在边缘计算、移动设备和嵌入式系统中的可用性。通过降低功耗和推理延迟，量化技术使人工智能模型更易于大规模部署，同时也推动了低成本、高效率智能应用的发展。"
  },
  {
    "instruction": "写一段关于提示工程的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "特别"
      ]
    ],
    "topic": "提示工程",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "提示工程是一种围绕模型交互的实践，它通过设计提示来引导模型生成内容。该实践涉及任务目标的表达、输入结构的安排、限制条件的设置、示例的加入以及输出格式的规定。使用者会通过测试、记录结果、调整提示来影响模型行为，并在应用中形成流程，用于写作、编程、分析等场景。"
  },
  {
    "instruction": "介绍命名实体识别，但不要使用'极其'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "极其"
      ]
    ],
    "topic": "命名实体识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "命名实体识别（Named Entity Recognition，简称 NER）是自然语言处理中的一项基础任务，目标是从文本中自动识别并标注具有特定意义的实体，例如人名、地名、组织机构名、时间、日期、金额等。通过对文本进行结构化处理，命名实体识别能够帮助计算机更好地理解语言内容，为后续分析打下基础。\n\n在实际应用中，命名实体识别被广泛用于信息抽取、搜索引擎、智能问答、舆情分析和知识图谱构建等场景。常见的方法包括基于规则的方式、传统机器学习模型以及近年来广泛应用的深度学习模型，如基于神经网络和预训练语言模型的方法。这些技术不断提升了实体识别的准确性和适应能力。"
  },
  {
    "instruction": "用Markdown格式撰写关于强化学习的说明，必须包含标题和分点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "强化学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "```markdown\n# 强化学习（Reinforcement Learning）简介\n\n## 什么是强化学习\n- 强化学习是机器学习的一个重要分支\n- 智能体（Agent）通过与环境（Environment）交互来学习策略\n- 目标是最大化长期累积奖励（Reward）\n\n## 核心要素\n- **智能体（Agent）**：做出决策的学习者\n- **环境（Environment）**：智能体所处并与之交互的外部世界\n- **状态（State）**：对当前环境的描述\n- **动作（Action）**：智能体在某一状态下采取的行为\n- **奖励（Reward）**：环境对动作的反馈信号\n\n## 基本流程\n- 智能体观察当前状态\n- 根据策略选择一个动作\n- 环境反馈奖励并转移到新状态\n- 智能体根据反馈更新策略\n\n## 常见算法\n- Q-learning\n- SARSA\n- 深度Q网络（DQN）\n- 策略梯度方法（Policy Gradient）\n\n## 应用领域\n- 游戏（如围棋、电子游戏）\n- 机器人控制\n- 自动驾驶\n- 资源调度与推荐系统\n```"
  },
  {
    "instruction": "在描述模型量化时，请确保'训练'这个词出现至少3次。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "训练",
          3
        ]
      ]
    ],
    "topic": "模型量化",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "模型量化是将高精度参数与计算转换为低比特表示的技术，用于降低模型的计算量和存储开销。在量化感知训练中，训练过程会显式地模拟低精度算子，使模型在训练阶段就能适应量化带来的数值误差。相比之下，后训练量化是在完成训练之后再进行权重量化，虽然实现简单，但有时会导致精度下降。通过合理设计训练策略、延长训练时间以及使用代表性数据进行校准，训练可以进一步提升量化模型在实际部署中的性能与稳定性。"
  },
  {
    "instruction": "用通俗易懂的语言向初学者解释模型压缩。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "模型压缩",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "模型压缩，简单来说，就是**让原本“又大又重”的模型变得“更小更轻”，但尽量不影响它的效果**。\n\n你可以把模型想象成一本很厚的字典：功能很强，但携带不方便、查起来也慢。模型压缩就像是**把这本厚字典精简成一本口袋版**——删掉不太重要的内容，用更聪明的方式存储信息，这样在手机、嵌入式设备等算力有限的地方也能跑得动。\n\n常见的模型压缩方法包括：\n- **剪枝**：把模型中“作用不大”的部分删掉，就像修剪多余的树枝。\n- **量化**：用更简单的数字表示模型参数，好比把小数改成整数，占用更少空间。\n- **知识蒸馏**：让一个大模型（老师）教一个小模型（学生），把关键知识教给它。\n- **低秩分解**：用更少的参数来近似原来的复杂计算。\n\n总的来说，**模型压缩的目标是在“性能”和“效率”之间找到平衡**：模型尽量小、快、省资源，同时还能保持不错的准确率。这也是为什么它在手机 AI、边缘计算等场景中非常重要。"
  },
  {
    "instruction": "写一段关于迁移学习的文字，必须包含以下关键词：反向传播, 模型, 神经网络。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "损失函数",
          3
        ]
      ]
    ],
    "topic": "迁移学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "迁移学习是一种利用已有知识加速新任务训练的方法，通常通过在相关领域预训练的模型作为起点，将其参数迁移到新的任务中，在此过程中依然依赖反向传播来微调权重，使模型能够快速适应数据分布的变化；这种方法在神经网络中尤为常见，因为深层结构可以复用底层特征表示，从而减少训练成本并提升整体性能。"
  },
  {
    "instruction": "用简洁的技术文档风格说明代码生成。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "代码生成",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**代码生成（Code Generation）说明**\n\n**概述**  \n代码生成是将高层描述（如需求、模型或模板）自动转换为可执行源代码的过程。\n\n**输入**  \n- 规格说明（需求文档、API 定义）  \n- 模型/配置（DSL、UML、Schema）  \n- 模板与规则（代码模板、映射规则）\n\n**过程**  \n1. 解析输入（语法/语义分析）  \n2. 映射与转换（模型到代码结构）  \n3. 渲染输出（模板填充、格式化）  \n4. 校验与优化（编译检查、静态分析）\n\n**输出**  \n- 源代码（单/多语言）  \n- 构建脚本与配置文件\n\n**优势**  \n- 提高开发效率与一致性  \n- 减少人为错误  \n- 便于规模化维护\n\n**风险与注意事项**  \n- 模板质量决定代码质量  \n- 需与手写代码的边界管理  \n- 版本与生成结果可追溯性"
  },
  {
    "instruction": "请用不少于250个字描述生成对抗网络。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        250
      ]
    ],
    "topic": "生成对抗网络",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "生成对抗网络是一类重要的深度学习模型，由生成器和判别器两个部分组成。生成器的目标是从随机噪声中生成尽可能逼真的数据样本，例如图像、语音或文本；判别器的任务则是判断输入的数据是真实数据还是由生成器伪造的数据。二者在训练过程中形成一种“对抗”关系：生成器努力欺骗判别器，而判别器不断提升识别真假的能力。随着训练的反复进行，生成器生成的数据质量逐步提高，最终能够在统计分布上逼近真实数据。\n\n生成对抗网络的训练过程通常被描述为一个极小极大的博弈问题，其核心思想来源于博弈论。尽管这种结构在理论和实践中都表现出强大的建模能力，但训练过程也存在不稳定、模式崩溃等问题，需要通过改进网络结构、损失函数或训练策略来缓解。生成对抗网络在图像生成、图像修复、风格迁移、数据增强以及医学影像等领域具有广泛应用，对人工智能的发展产生了深远影响。"
  },
  {
    "instruction": "以问答形式介绍自然语言处理，包含至少6个问题和回答。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "list"
      ]
    ],
    "topic": "自然语言处理",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**问：什么是自然语言处理（NLP）？**  \n答：自然语言处理是人工智能的一个分支，研究如何让计算机理解、生成和处理人类语言，如中文、英文等。\n\n**问：自然语言处理可以做哪些事情？**  \n答：它可以用于机器翻译、文本分类、情感分析、语音识别、信息抽取、智能问答和聊天机器人等任务。\n\n**问：自然语言处理的核心技术有哪些？**  \n答：常见技术包括分词、词性标注、句法分析、语义理解、向量表示（如词向量）、深度学习模型和大语言模型等。\n\n**问：自然语言处理与机器学习有什么关系？**  \n答：自然语言处理大量依赖机器学习，尤其是深度学习，通过从大量文本数据中学习语言规律来提升模型效果。\n\n**问：自然语言处理在日常生活中的应用有哪些？**  \n答：常见应用包括搜索引擎自动补全、手机语音助手、垃圾邮件过滤、智能客服和社交媒体内容分析等。\n\n**问：学习自然语言处理需要哪些基础？**  \n答：通常需要一定的编程基础（如Python）、数学基础（概率、线性代数）、机器学习知识，以及对语言学的基本理解。\n\n**问：自然语言处理未来的发展趋势是什么？**  \n答：未来将更加注重多语言、多模态理解，更强的推理能力，以及在医疗、教育和科研等领域的深入应用。"
  },
  {
    "instruction": "以幽默风趣的方式介绍文本分类。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "文本分类",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "想象一下，文本分类就像给文字开了一家“分拣中心”。每天成千上万的句子排队进门，有的穿着“新闻”的西装，有的背着“垃圾邮件”的小包，还有的顶着“情感分析”的表情包。文本分类模型呢？它就是那位眼疾手快的分拣员，扫一眼内容，啪！贴个标签：“这是体育”“那是吐槽”“这个……放垃圾桶”。\n\n再专业一点说，文本分类是让计算机学会“看懂话里话外”，把文本按主题、情绪或用途分门别类。再不专业一点说，它就是教电脑学会整理房间：把袜子扔进袜子抽屉，把外卖小票丢掉，把情书小心翼翼地藏好——只不过电脑不嫌累，还特别听话。"
  },
  {
    "instruction": "请用列表形式列出边缘计算的5个要点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "列表"
      ]
    ],
    "topic": "边缘计算",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. **低时延**：在数据源附近处理数据，显著降低响应时间。  \n2. **节省带宽**：减少向云端传输的原始数据量，降低网络负载。  \n3. **数据隐私与安全**：本地处理敏感数据，降低数据外泄风险。  \n4. **高可靠性**：在网络不稳定或断连情况下仍可本地运行。  \n5. **支持实时与物联网应用**：适合大规模设备接入和实时决策场景。"
  },
  {
    "instruction": "描述文本分类的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "非常"
      ]
    ],
    "topic": "文本分类",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "文本分类具有以下特点：\n\n- 基于文本内容与特征进行类别判定。  \n- 需要对文本进行特征表示，如词袋模型、TF-IDF、向量表示。  \n- 可采用规则方法或机器学习方法实现分类。  \n- 支持多类别任务与多标签任务。  \n- 依赖标注数据进行训练，并通过准确率、召回率、F1值进行评估。  \n- 应用于信息检索、情感分析、垃圾信息识别等场景。"
  },
  {
    "instruction": "介绍异常检测，但不要使用'特别'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "特别"
      ]
    ],
    "topic": "异常检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "异常检测是一类用于识别数据中偏离常态行为的技术，目标是在大量正常样本中发现少量异常点或异常模式。它广泛应用于金融风控、网络安全、设备运维、医疗监测等领域，用于预警风险、发现故障并提升系统可靠性。\n\n常见方法包括基于统计假设的模型、基于距离或密度的算法、基于机器学习的分类与聚类方法，以及利用深度学习进行表示学习的方案。不同方法在对数据分布假设、可解释性、实时性和计算成本方面各有侧重。\n\n在实践中，异常检测面临样本不平衡、噪声干扰、概念漂移和评估困难等问题。合理选择特征、结合业务规则，并持续监控模型表现，是取得良好效果的关键。"
  },
  {
    "instruction": "描述联邦学习，开头必须是'在当今时代'。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "联邦学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "在当今时代，联邦学习是一种分布式机器学习范式，它通过在数据本地进行模型训练、仅共享模型参数或梯度而非原始数据，实现多方协同建模。这种方法在保障数据隐私与合规性的同时，能够利用分散在不同设备或机构中的数据提升模型性能，常见于医疗、金融、智能终端等场景，并结合加密、差分隐私等技术进一步增强安全性与可靠性。"
  },
  {
    "instruction": "用简洁的技术文档风格说明代码生成。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "代码生成",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**代码生成（Code Generation）**\n\n**概述**  \n代码生成是将高层描述（需求、模型或规范）自动转换为可执行源代码的过程。\n\n**输入**  \n- 自然语言需求  \n- 结构化规范（API、DSL、模型）  \n- 配置与约束\n\n**输出**  \n- 源代码（如 Java、Python、C++）  \n- 相关配置与模板文件\n\n**流程**  \n1. 解析输入（语义/语法）  \n2. 映射到代码模板或规则  \n3. 生成并格式化代码  \n4. 校验与测试\n\n**应用场景**  \n- CRUD/样板代码生成  \n- API/SDK 生成  \n- 跨平台代码同步  \n- 低代码/无代码平台\n\n**注意事项**  \n- 保证可读性与可维护性  \n- 加强安全与依赖校验  \n- 支持定制与版本管理"
  },
  {
    "instruction": "撰写语音识别的介绍，每段必须包含'反向传播'。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "反向传播",
          3
        ]
      ]
    ],
    "topic": "语音识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "语音识别是将人类语音信号转化为可处理文本的技术，广泛应用于智能助手、客服系统和无障碍交互中。现代语音识别通常基于深度学习模型，通过大量语音与文本数据进行训练，其中模型参数的更新依赖于反向传播来最小化识别误差并提升准确率。\n\n在系统流程上，语音识别会经历特征提取、声学建模、语言建模和解码等阶段，端到端模型进一步简化了这些步骤。无论采用传统的混合模型还是端到端神经网络，训练过程中都需要使用反向传播在多层网络中传递梯度，从而不断优化模型对语音时序与语义的理解能力。"
  },
  {
    "instruction": "介绍分布式训练，但不要使用'非常'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "非常"
      ]
    ],
    "topic": "分布式训练",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "分布式训练是指在多台计算设备上协同训练同一个模型的技术。通过将数据或模型参数划分到不同的计算节点，并借助高效的通信机制进行同步，可以缩短训练时间、提升资源利用率，同时支持更大规模的数据集和模型。\n\n常见的分布式训练方式包括数据并行、模型并行以及混合并行。数据并行将不同的数据批次分配给各个设备独立计算梯度，再进行聚合；模型并行则把模型拆分到多个设备上，适合参数规模较大的模型；混合并行结合两者的特点。在实际应用中，需要重点考虑通信开销、同步策略、容错机制和结果一致性，常用实现工具包括 Parameter Server、AllReduce，以及 PyTorch Distributed、Horovod、DeepSpeed 等框架。"
  },
  {
    "instruction": "请用列表形式列出模型压缩的4个要点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "列表"
      ]
    ],
    "topic": "模型压缩",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. **剪枝（Pruning）**：移除对模型贡献较小的权重或神经元，减少参数量和计算量。  \n2. **量化（Quantization）**：将高精度参数和计算（如 FP32）转换为低精度表示（如 INT8），降低存储和推理成本。  \n3. **知识蒸馏（Knowledge Distillation）**：用大模型（教师）指导小模型（学生）学习，在较小规模下保持性能。  \n4. **低秩分解与权重共享**：通过矩阵低秩分解或共享参数，减少冗余表示，提升模型效率。"
  },
  {
    "instruction": "描述目标检测的特点，不要使用'非常'、'很'等程度副词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "特别"
      ]
    ],
    "topic": "目标检测",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "目标检测的特点包括：  \n- **定位与分类并行**：在同一任务中识别目标类别并给出空间位置。  \n- **边界框表示**：使用矩形框或其他几何形式描述目标范围。  \n- **密集预测机制**：在图像多个位置与尺度上进行候选目标判断。  \n- **多尺度处理**：应对目标尺寸差异与视角变化。  \n- **上下文建模**：结合局部特征与全局信息提升识别可靠性。  \n- **效率与精度权衡**：在推理速度与检测效果之间进行设计取舍。  \n- **标准化评估**：采用精度、召回率、mAP等指标衡量性能。"
  },
  {
    "instruction": "写一篇关于自监督学习的短文，必须以问句结尾。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "自监督学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "自监督学习是一种利用数据自身结构生成监督信号的学习范式，它通过预设任务从无标注数据中学习表示，如对比学习、掩码预测等方法，显著降低了对人工标注的依赖，并在视觉、语言和多模态领域展现出强大的迁移能力与泛化性能，那么自监督学习是否会成为通用智能发展的关键路径？"
  },
  {
    "instruction": "撰写自监督学习的介绍，每段必须包含'特征提取'。",
    "input": "",
    "instruction_type": "keyword_inclusion",
    "verifiable_rules": [
      [
        "keyword_count",
        [
          "特征提取",
          4
        ]
      ]
    ],
    "topic": "自监督学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "自监督学习是一种利用数据自身结构来构建学习信号的方法，不依赖人工标注，通过预设任务让模型从未标注数据中学习有用表示，其核心目标之一就是提升模型在下游任务中的特征提取能力，从而降低对大规模标注数据的需求。\n\n在技术层面，自监督学习通常通过对比学习、掩码预测或生成式任务等方式，引导模型理解数据的内在规律，这些方法强调在不同视角或上下文中保持语义一致性，使模型能够进行稳健的特征提取，并在图像、语音和自然语言等领域取得显著效果。\n\n随着深度学习的发展，自监督学习已成为表示学习的重要方向，它不仅能够提高模型的泛化性能，还能在资源受限的场景中发挥优势，通过高质量的特征提取为分类、检索和生成等多种应用奠定基础。"
  },
  {
    "instruction": "写一段关于RAG检索增强的文字，字数要求在200到350之间。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        200
      ],
      [
        "max_length",
        350
      ]
    ],
    "topic": "RAG检索增强",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "RAG检索增强生成是一种将信息检索与大模型生成能力结合的技术。其核心思路是在回答问题前，先从外部知识库、文档或向量数据库中检索相关内容，再将检索结果作为上下文输入模型生成答案。这样可以有效缓解模型知识过时、幻觉和事实不准确的问题。RAG适用于企业知识问答、智能客服、科研辅助和代码搜索等场景，同时支持私有数据接入，提升答案的可控性与可追溯性。通过合理的切分、向量化和重排策略，RAG系统还能在保证效率的同时显著提升回答质量。"
  },
  {
    "instruction": "描述知识蒸馏，开头必须是'在当今时代'。",
    "input": "",
    "instruction_type": "structure_constraint",
    "verifiable_rules": [
      [
        "structure",
        "structure_constraint"
      ]
    ],
    "topic": "知识蒸馏",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "在当今时代，知识蒸馏是一种将大型、复杂模型（教师模型）的知识迁移到小型、高效模型（学生模型）中的重要技术。其核心思想是让学生模型不仅学习真实标签，还学习教师模型输出的“软标签”或中间表示，从而捕捉更丰富的类别关系与隐含信息。通过这种方式，学生模型在参数更少、计算成本更低的情况下，依然能够接近甚至达到教师模型的性能，因此被广泛应用于模型压缩、边缘计算和实际工业部署中。"
  },
  {
    "instruction": "介绍语音识别，但不要使用'很'这个词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "很"
      ]
    ],
    "topic": "语音识别",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "语音识别是一种将人类语音转换为文本或指令的技术。它通过麦克风采集声音信号，利用声学模型、语言模型和算法进行分析与匹配，从而理解说话内容。该技术已广泛应用于智能助手、车载系统、客服机器人和无障碍服务，提升了交互效率与使用体验。随着算力、数据规模和深度学习的发展，语音识别在准确率、实时性和多语种支持方面持续进步，但在噪声环境、方言差异和隐私保护上仍需进一步优化。"
  },
  {
    "instruction": "请用列表形式列出Transformer架构的5个要点。",
    "input": "",
    "instruction_type": "format_constraint",
    "verifiable_rules": [
      [
        "format",
        "列表"
      ]
    ],
    "topic": "Transformer架构",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "1. **自注意力机制（Self-Attention）**：建模序列中各位置之间的依赖关系，捕获全局上下文信息。  \n2. **多头注意力（Multi-Head Attention）**：并行关注不同子空间的特征，提高表示能力。  \n3. **位置编码（Positional Encoding）**：为序列引入位置信息，弥补模型本身不具备顺序感知的不足。  \n4. **编码器–解码器堆叠结构（Encoder–Decoder）**：通过多层堆叠提升特征抽象能力，适用于序列到序列任务。  \n5. **前馈全连接网络与残差连接、层归一化**：增强非线性表达能力并稳定训练过程。"
  },
  {
    "instruction": "写一段关于机器学习的文字，字数要求在200到400之间。",
    "input": "",
    "instruction_type": "word_count",
    "verifiable_rules": [
      [
        "min_length",
        200
      ],
      [
        "max_length",
        400
      ]
    ],
    "topic": "机器学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "机器学习是人工智能的重要分支，它通过从数据中自动学习规律，帮助计算机在没有明确规则的情况下完成预测与决策。常见的方法包括监督学习、无监督学习和强化学习，广泛应用于图像识别、语音处理、推荐系统和金融风控等领域。随着大数据和算力的发展，机器学习模型不断提升，但也面临数据偏见、可解释"
  },
  {
    "instruction": "以幽默风趣的方式介绍推荐系统。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "推荐系统",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "推荐系统，其实就是互联网世界里的“贴心损友”。你刚说一句“我最近有点无聊”，它立刻端上一桌短视频；你只是随手点了个猫咪，它就认定你是“资深铲屎官”，从此满屏都是猫，连狗都不让你看一眼。\n\n它的工作原理也不复杂：一边偷偷记小本本——你点了什么、看了多久、几点还不睡；一边假装很懂你——“你既然喜欢这个，那一定也会喜欢那个！”于是它像个热情过度的朋友，不断给你安利：“这个你一定要看！”“相信我，你会爱上的！”\n\n所以，推荐系统不是在控制你，而是在用数据谈恋爱：你给它一点点击，它回你一堆内容。至于准不准嘛……有时候是灵魂知己，有时候就是大型社死现场。"
  },
  {
    "instruction": "写一段关于机器学习的文字，不要使用任何形容词。",
    "input": "",
    "instruction_type": "exclusion_constraint",
    "verifiable_rules": [
      [
        "exclude_word",
        "特别"
      ]
    ],
    "topic": "机器学习",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "机器学习 是 一个 研究 领域。它 使用 数据 和 算法 建立 模型。模型 通过 训练 调整 参数。系统 根据 输入 产生 输出。研究 包括 分类 回归 聚类。应用 覆盖 语音 视觉 推荐。过程 涉及 数据 收集 清洗 评估。"
  },
  {
    "instruction": "用简洁的技术文档风格说明机器翻译。",
    "input": "",
    "instruction_type": "language_style",
    "verifiable_rules": [
      [
        "style",
        "language_style"
      ]
    ],
    "topic": "机器翻译",
    "task_type": "instruction_following",
    "source": "chinese_generated",
    "language": "zh",
    "output": "**机器翻译（Machine Translation, MT）**\n\n**1. 定义**  \n机器翻译是利用计算机算法将文本或语音从一种自然语言自动转换为另一种自然语言的技术。\n\n**2. 工作原理**  \n通过对源语言进行分析、建模与解码，生成在目标语言中语义等价且语法合理的输出。\n\n**3. 主要方法**  \n- **基于规则（RBMT）**：依赖人工编写的语法和词典规则。  \n- **统计机器翻译（SMT）**：基于大规模双语语料的概率模型。  \n- **神经机器翻译（NMT）**：基于深度学习（如Transformer）的端到端模型。\n\n**4. 基本流程**  \n输入文本 → 预处理 → 翻译模型推理 → 后处理 → 输出译文\n\n**5. 评估指标**  \n- BLEU、TER、COMET 等自动评测指标  \n- 人工评测（流畅度、准确性）\n\n**6. 应用与局限**  \n- **应用**：跨语言交流、本地化、信息检索。  \n- **局限**：歧义处理、领域适应、低资源语言支持不足。"
  }
]